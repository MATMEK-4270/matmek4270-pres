[
  {
    "objectID": "vibration.html#a-simple-vibration-problem",
    "href": "vibration.html#a-simple-vibration-problem",
    "title": "A simple vibration problem",
    "section": "A simple vibration problem",
    "text": "A simple vibration problem\n\n\n\nThe vibration equation is given as\n\\[\nu^{\\prime\\prime}(t) + \\omega^2u(t) = 0,\\quad u(0)=I,\\ u^{\\prime}(0)=0,\\ t\\in (0,T]\n\\]\nand the exact solution is:\n\\[\nu(t) = I\\cos (\\omega t)\n\\]\n\n\\(u(t)\\) oscillates with constant amplitude \\(I\\) and (angular) frequency \\(\\omega\\).\nPeriod: \\(P=2\\pi/\\omega\\). The period is the time between two neighboring peaks in the cosine function."
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-1-and-2",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-1-and-2",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 1 and 2",
    "text": "A centered finite difference scheme; step 1 and 2\n\nStrategy: follow the four steps of the finite difference method.\nStep 1: Introduce a time mesh, here uniform on \\([0,T]\\): \\[\nt_n=n\\Delta t, \\quad n=0, 1, \\ldots, N_t\n\\]\n\n\n\nStep 2: Let the ODE be satisfied at each mesh point minus 2 boundary conditions:\n\n\\[\nu^{\\prime\\prime}(t_n) + \\omega^2u(t_n) = 0,\\quad n=2,\\ldots,N_t\n\\]"
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-3",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-3",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 3",
    "text": "A centered finite difference scheme; step 3\nStep 3: Approximate derivative(s) by finite difference approximation(s). Very common (standard!) formula for \\(u^{\\prime\\prime}\\):\n\\[\nu^{\\prime\\prime}(t_n) \\approx \\frac{u^{n+1}-2u^n + u^{n-1}}{\\Delta t^2}\n\\]\n\nInsert into vibration ODE:\n\\[\n\\frac{u^{n+1}-2u^n + u^{n-1}}{\\Delta t^2} = -\\omega^2 u^n\n\\]"
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-4",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-4",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 4",
    "text": "A centered finite difference scheme; step 4\nStep 4: Formulate the computational algorithm. Assume \\(u^{n-1}\\) and \\(u^n\\) are known, solve for unknown \\(u^{n+1}\\):\n\\[\nu^{n+1} = 2u^n - u^{n-1} - \\Delta t^2\\omega^2 u^n\n\\]\nNick names for this scheme: Störmer’s method or Verlet integration.\n\nThe scheme is a recurrence relation. That is, \\(u^{n+1}\\) is an explicit function of one or more of the solutions at previous time steps \\(u^n, u^{n-1}, \\ldots\\). We will later see implicit schemes where the equation for \\(u^{n+1}\\) depends also on \\(u^{n+2}, u^{n+3}\\) etc."
  },
  {
    "objectID": "vibration.html#computing-the-first-step---option-1",
    "href": "vibration.html#computing-the-first-step---option-1",
    "title": "A simple vibration problem",
    "section": "Computing the first step - option 1",
    "text": "Computing the first step - option 1\n\nThe two initial conditions require that we fix both \\(u^0\\) and \\(u^1\\). How to fix \\(u^1\\)?\n\n\n\nWe cannot use the difference equation \\(u^{1} = 2u^0 - u^{-1} - \\Delta t^2\\omega^2 u^0\\) because \\(u^{-1}\\) is unknown and outside the mesh!\nAnd: we have not used the initial condition \\(u^{\\prime}(0)=0\\)!\n\n\n\nOption 1: Use a forward difference\n\\[\n  \\begin{align*}\n  u^{\\prime}(0) &=  \\frac{u^1-u^0}{\\Delta t}=0  &\\longrightarrow u^1=u^0=I \\\\\n  u^{\\prime}(0) &= \\frac{-u^2+4u^1-3u^0}{2 \\Delta t}=0 \\quad &\\longrightarrow u^1=\\frac{u^2+3u^0}{4}\n  \\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\nFirst is merely first order accurate, second is second order, but implicit (depends on the unknown \\(u^2\\).)"
  },
  {
    "objectID": "vibration.html#computing-the-first-step---option-2",
    "href": "vibration.html#computing-the-first-step---option-2",
    "title": "A simple vibration problem",
    "section": "Computing the first step - option 2",
    "text": "Computing the first step - option 2\nUse the discrete ODE at \\(t=0\\) together with a central difference at \\(t=0\\) and a ghost cell \\(u^{-1}\\). The central difference is\n\\[\n\\frac{u^1-u^{-1}}{2\\Delta t} = 0\\quad\\Rightarrow\\quad u^{-1} = u^1\n\\]\nThe vibration scheme for \\(n=0\\) is\n\\[\nu^{1} = 2u^0 - u^{-1} - \\Delta t^2\\omega^2 u^0\n\\]\nUse \\(u^{-1}=u^1\\) to get\n\\[\nu^1 = u^0 - \\frac{1}{2} \\Delta t^2 \\omega^2 u^0\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit (does not depend on unknown \\(u^2\\))."
  },
  {
    "objectID": "vibration.html#the-computational-algorithm",
    "href": "vibration.html#the-computational-algorithm",
    "title": "A simple vibration problem",
    "section": "The computational algorithm",
    "text": "The computational algorithm\n\n\\(u^0=I\\)\ncompute \\(u^1 = u^0 - \\frac{1}{2} \\Delta t^2 \\omega^2 u^0\\)\nfor \\(n=1, 2, \\ldots, N_t-1\\):\n\ncompute \\(u^{n+1}\\)\n\n\n\nMore precisely expressed in Python:\n\nT = 1\nNt = 10\nI = 1\nw = 4\nt = np.linspace(0, T, Nt+1)  # mesh points in time\ndt = t[1] - t[0]          # constant time step.\nu = np.zeros(Nt+1)           # solution\n\nu[0] = I\nu[1] = u[0] - 0.5*dt**2*w**2*u[0]\nfor n in range(1, Nt):\n    u[n+1] = 2*u[n] - u[n-1] - dt**2*w**2*u[n]\n\nThe code is difficult to vectorize, so we should use Numba or Cython for speed."
  },
  {
    "objectID": "vibration.html#computing-uprime",
    "href": "vibration.html#computing-uprime",
    "title": "A simple vibration problem",
    "section": "Computing \\(u^{\\prime}\\)",
    "text": "Computing \\(u^{\\prime}\\)\n\\(u\\) is often displacement/position, \\(u^{\\prime}\\) is velocity and can be computed by\n\\[\nu^{\\prime}(t_n) \\approx \\frac{u^{n+1}-u^{n-1}}{2\\Delta t}\n\\]\n\n\n\n\n\n\nNote\n\n\nFor \\(u^{\\prime}(t_0)\\) and \\(u^{\\prime}(t_{N_t})\\) it is possible to use forward or backwards differences, respectively. However, we already know from initial conditions that \\(u^{\\prime}(t_0) = 0\\) so no need to use finite difference there.\n\n\n\n\nWith vectorization:\n\ndu = np.zeros(Nt+1)\ndu[1:-1] = (u[2:] - u[:-2]) / (2*dt)  # second order accurate\ndu[0] = 0                             # exact from initial condition\ndu[-1] = (u[-1]-u[-2]) / dt           # first order accurate"
  },
  {
    "objectID": "vibration.html#implementation",
    "href": "vibration.html#implementation",
    "title": "A simple vibration problem",
    "section": "Implementation",
    "text": "Implementation\n\ndef solver(I, w, dt, T):\n    \"\"\"\n    Solve u'' + w**2*u = 0 for t in (0,T], u(0)=I and u'(0)=0,\n    by a central finite difference method with time step dt.\n    \"\"\"\n    Nt = int(round(T/dt))\n    u = np.zeros(Nt+1)\n    t = np.linspace(0, Nt*dt, Nt+1)\n\n    u[0] = I\n    u[1] = u[0] - 0.5*dt**2*w**2*u[0]\n    for n in range(1, Nt):\n        u[n+1] = 2*u[n] - u[n-1] - dt**2*w**2*u[n]\n    return u, t\n \ndef u_exact(t, I, w):\n    return I*np.cos(w*t)"
  },
  {
    "objectID": "vibration.html#visualization",
    "href": "vibration.html#visualization",
    "title": "A simple vibration problem",
    "section": "Visualization",
    "text": "Visualization\n\ndef visualize(u, t, I, w):\n    plt.plot(t, u, 'b-o')\n    t_fine = np.linspace(0, t[-1], 1001)  # very fine mesh for u_e\n    u_e = u_exact(t_fine, I, w)\n    plt.plot(t_fine, u_e, 'r--')\n    plt.legend(['numerical', 'exact'], loc='upper left')\n    plt.xlabel('t')\n    plt.ylabel('u(t)')\n    dt = t[1] - t[0]\n    plt.title('dt=%g' % dt)\n    umin = 1.2*u.min();  umax = -umin\n    plt.axis([t[0], t[-1], umin, umax])"
  },
  {
    "objectID": "vibration.html#main-program",
    "href": "vibration.html#main-program",
    "title": "A simple vibration problem",
    "section": "Main program",
    "text": "Main program\n\nI = 1\nw = 2*np.pi\nnum_periods = 6\nP = 2*np.pi/w    #  one period\nT = P*num_periods\ndt = 1/w\nu, t = solver(I, w, dt, T)\nvisualize(u, t, I, w)"
  },
  {
    "objectID": "vibration.html#various-timestep",
    "href": "vibration.html#various-timestep",
    "title": "A simple vibration problem",
    "section": "Various timestep",
    "text": "Various timestep\n\n\n\n\n\n\n\n\n\nWe see that \\(\\Delta t = 2/\\omega\\) is a limit. Using longer \\(\\Delta t\\) leads to growth. Why?"
  },
  {
    "objectID": "vibration.html#mathematical-analysis",
    "href": "vibration.html#mathematical-analysis",
    "title": "A simple vibration problem",
    "section": "Mathematical analysis",
    "text": "Mathematical analysis\nThe exact solution to the continuous vibration equation is \\(u_e(t) = I \\cos (\\omega t)\\)\nThe key to study the numerical solution is knowing that linear difference equations like\n\\[\nu^{n+1} = (2-\\Delta t^2\\omega^2) u^n - u^{n-1}\n\\]\nadmit discrete (numerical) solutions of the form\n\\[\nu^{n+1} = A u^n  \\quad \\text{or} \\quad u^n = A^n I\n\\]\nwhere \\(I\\) is an initial condition.\n\nSo the recurrence relation (the difference equation) have solutions \\(u^n\\) that may be computed recursively\n\ndef u(n, I=2., A=0.5):\n  if n &gt; 0:\n    return A * u(n-1, I, A)\n  return I\nprint(u(0), u(1), u(2), u(3))\n\n2.0 1.0 0.5 0.25"
  },
  {
    "objectID": "vibration.html#exact-discrete-solution",
    "href": "vibration.html#exact-discrete-solution",
    "title": "A simple vibration problem",
    "section": "Exact discrete solution",
    "text": "Exact discrete solution\nWe now have (at least) two possibilities\n\nAssume that \\(A=e^{i \\tilde{\\omega} \\Delta t}\\) and solve for the numerical frequency \\(\\tilde{\\omega}\\)\nAssume nothing and compute with a function A(\\(\\Delta t, \\omega\\)) (like for the exponential decay)\n\n\nWe follow Langtangen’s approach (1) first. Note that since\n\\[\ne^{i \\tilde{\\omega} \\Delta t} = \\cos (\\tilde{\\omega} \\Delta t ) + i \\sin(\\tilde{\\omega} \\Delta t)\n\\]\nwe can work with a complex A and let the real part represent the physical solution.\nThe exact discrete solution is then\n\\[\nu(t_n) = I \\cos (\\tilde{\\omega} t_n)\n\\]\nand we can study the error in \\(\\tilde{\\omega}\\) compared to the true \\(\\omega\\)."
  },
  {
    "objectID": "vibration.html#find-the-error-in-tildeomega",
    "href": "vibration.html#find-the-error-in-tildeomega",
    "title": "A simple vibration problem",
    "section": "Find the error in \\(\\tilde\\omega\\)",
    "text": "Find the error in \\(\\tilde\\omega\\)\nInsert the numerical solution \\(u^n = I \\cos (\\tilde{\\omega} t_n)\\) into the discrete equation\n\\[\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0\n\\]\n\nQuite messy, but Wolfram Alpha (or another long derivation in the FD for PDEs book) will give you\n\\[\n\\begin{align}\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} &= \\frac{I}{\\Delta t^2} (\\cos (\\tilde{\\omega} t_{n+1}) - 2 \\cos (\\tilde{\\omega} t_n) + \\cos (\\tilde{\\omega} t_{n-1})) \\\\\n&= \\frac{2 I}{\\Delta t^2} (\\cos (\\tilde{\\omega} \\Delta t) - 1) \\cos (\\tilde{\\omega} n \\Delta t) \\\\\n&= -\\frac{4}{\\Delta t^2} \\sin^2 (\\tilde{\\omega} \\Delta t) \\cos (\\tilde{\\omega} n \\Delta t)\n\\end{align}\n\\]"
  },
  {
    "objectID": "vibration.html#insert-into-discrete-equation",
    "href": "vibration.html#insert-into-discrete-equation",
    "title": "A simple vibration problem",
    "section": "Insert into discrete equation",
    "text": "Insert into discrete equation\n\\[\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0\n\\]\nWe get\n\\[\n-\\frac{4}{\\Delta t^2} \\sin^2 (\\tilde{\\omega} \\Delta t) { \\cos (\\tilde{\\omega} n \\Delta t)} + { \\omega^2 { \\cos (\\tilde{\\omega} n \\Delta t)}} = 0\n\\]\nand thus\n\\[\n\\omega^2 = \\frac{4}{\\Delta t^2} \\sin^2 \\left( \\frac{\\tilde{\\omega} \\Delta t}{2} \\right)\n\\]\nSolve for \\(\\tilde{\\omega}\\) by taking the root and using \\(\\sin^{-1}\\)"
  },
  {
    "objectID": "vibration.html#numerical-frequency",
    "href": "vibration.html#numerical-frequency",
    "title": "A simple vibration problem",
    "section": "Numerical frequency",
    "text": "Numerical frequency\n\\[\n\\tilde{\\omega} = \\pm \\frac{2}{\\Delta t} \\sin^{-1} \\left( \\frac{\\omega  \\Delta t }{2} \\right)\n\\]\n\n\nThere is always a frequency error because \\(\\tilde{\\omega} \\neq \\omega\\).\nThe dimensionless number \\(p=\\omega\\Delta t\\) is the key parameter (i.e., no of time intervals per period is important, not \\(\\Delta t\\) itself. Remember \\(P=2\\pi/w\\) and thus \\(p=2\\pi / P\\))\nHow good is the approximation \\(\\tilde{\\omega}\\) to \\(\\omega\\)?\nDoes it possibly lead to growth? \\(|A|&gt;1\\)."
  },
  {
    "objectID": "vibration.html#polynomial-approximation-of-the-frequency-error",
    "href": "vibration.html#polynomial-approximation-of-the-frequency-error",
    "title": "A simple vibration problem",
    "section": "Polynomial approximation of the frequency error",
    "text": "Polynomial approximation of the frequency error\nHow good is the approximation \\(\\tilde{\\omega} = \\pm \\frac{2}{\\Delta t} \\sin^{-1} \\left( \\frac{\\omega  \\Delta t }{2} \\right)\\) ?\n\nWe can easily use a Taylor series expansion for small \\(h=\\Delta t\\)\n\nimport sympy as sp \nh, w = sp.symbols('h,w')\nw_tilde = sp.asin(w*h/2).series(h, 0, 4)*2/h\nsp.simplify(w_tilde)\n\n\\(\\displaystyle w + \\frac{h^{2} w^{3}}{24} + O\\left(h^{3}\\right)\\)\n\n\n\n\nSo the numerical frequency i always too large (to fast oscillations): \\[\n\\tilde\\omega = \\omega\\left( 1 + \\frac{1}{24}\\omega^2\\Delta t^2\\right) + {\\cal O}(\\Delta t^3)\n\\]"
  },
  {
    "objectID": "vibration.html#simple-improvement-of-previous-solver",
    "href": "vibration.html#simple-improvement-of-previous-solver",
    "title": "A simple vibration problem",
    "section": "Simple improvement of previous solver",
    "text": "Simple improvement of previous solver\n\n\n\n\n\n\nNote\n\n\nWhat happens if we simply use the frequency \\(\\omega(1-\\omega^2 \\Delta t^2 /24)\\) instead of just \\(\\omega\\)?\n\n\n\n\nThe leading order numerical error disappears and we get\n\\[\n\\tilde\\omega = \\omega\\left( 1 - \\left(\\frac{1}{24}\\omega^2\\Delta t^2\\right)^2\\right) + \\cdots\n\\]\n\n\n\n\n\n\nNote\n\n\nDirty trick, and only usable when you can compute the numerical error exactly. But fourth order…"
  },
  {
    "objectID": "vibration.html#how-about-the-global-error",
    "href": "vibration.html#how-about-the-global-error",
    "title": "A simple vibration problem",
    "section": "How about the global error?",
    "text": "How about the global error?\n\\[\nu^n = I\\cos\\left(\\tilde\\omega n\\Delta t\\right),\\quad\n\\tilde\\omega = \\frac{2}{\\Delta t}\\sin^{-1}\\left(\\frac{\\omega\\Delta t}{2}\\right)\n\\]\nThe error mesh function,\n\\[\ne^n = u_{e}(t_n) - u^n =\nI\\cos\\left(\\omega n\\Delta t\\right)\n- I\\cos\\left(\\tilde\\omega n\\Delta t\\right)\n\\]\nis ideal for verification and further analysis!\n\n\\[\n\\begin{align*}\ne^n &= I\\cos\\left(\\omega n\\Delta t\\right)\n- I\\cos\\left(\\tilde\\omega n\\Delta t\\right) \\\\\n&= -2I\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega - \\tilde\\omega\\right)\\right)\n\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega + \\tilde\\omega\\right)\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "vibration.html#convergence-of-the-numerical-scheme",
    "href": "vibration.html#convergence-of-the-numerical-scheme",
    "title": "A simple vibration problem",
    "section": "Convergence of the numerical scheme",
    "text": "Convergence of the numerical scheme\nWe can easily show convergence (i.e., \\(e^n\\rightarrow 0 \\hbox{ as }\\Delta t\\rightarrow 0\\)) from what we know about sines in the error\n\\[\ne^n = -2I\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega - \\tilde\\omega\\right)\\right)\n\\sin\\left(n \\Delta t \\frac{1}{2}\\left( \\omega + \\tilde\\omega\\right)\\right)\n\\] and the following limit \\[\n\\lim_{\\Delta t\\rightarrow 0}\n\\tilde\\omega = \\lim_{\\Delta t\\rightarrow 0}\n\\frac{2}{\\Delta t}\\sin^{-1}\\left(\\frac{\\omega\\Delta t}{2}\\right)\n= \\omega\n\\]\n\nThe limit can be computed using L’Hopital’s rule or simply by asking sympy or WolframAlpha. Sympy is easier:\n\nsp.limit((2/h)*sp.asin(w*h/2), h, 0, dir='+')\n\n\\(\\displaystyle w\\)"
  },
  {
    "objectID": "vibration.html#how-about-stability",
    "href": "vibration.html#how-about-stability",
    "title": "A simple vibration problem",
    "section": "How about stability?",
    "text": "How about stability?\n\n\nSolutions are oscillatory, so not a problem that \\(A&lt;0\\), like for the exponential decay\nSolutions should have constant amplitudes (constant \\(I\\)), but we will have growth if \\[\n|A| &gt; 1\n\\]\nConstant amplitude requires \\[\n|A| = |e^{i \\tilde{\\omega} \\Delta t}| = 1\n\\] Is this always satisfied?"
  },
  {
    "objectID": "vibration.html#stability-in-growth",
    "href": "vibration.html#stability-in-growth",
    "title": "A simple vibration problem",
    "section": "Stability in growth?",
    "text": "Stability in growth?\nConsider \\[\n|A| = |e^{i y}| \\quad \\text{where} \\quad y= \\pm 2 \\sin^{-1}\\left(\\frac{\\omega \\Delta t}{2}\\right)\n\\]\nIs \\(|e^{iy}|=1\\) for all \\(y\\)?\n\n\n\nNo! For \\(\\Im(y) &lt; 0\\) we have that \\(|e^{iy}| &gt; 1\\).\n\n\ny = -0.001j\nabs(sp.exp(1j*y)) \n\n\\(\\displaystyle 1.00100050016671\\)\n\n\n\n\n\n\nHow can we get negative \\(\\Im(y)\\)? Can \\(\\Im(\\sin^{-1}(x)) &lt; 0\\) for some \\(x\\)?\n\n\nYes! We can easily check that if \\(|x|&gt;1\\) then \\(\\sin^{-1}(x)\\) has a negative imaginary part:\n\nsp.asin(1.01)\n\n\\(\\displaystyle 1.5707963267949 - 0.141303769485649 i\\)\n\n\n\n\nHence if \\(\\left| \\frac{\\omega \\Delta t}{2} \\right| &gt; 1\\) then we will have growth! For stability \\(\\longrightarrow \\Delta t \\le 2 / w\\) !"
  },
  {
    "objectID": "vibration.html#stability-limit",
    "href": "vibration.html#stability-limit",
    "title": "A simple vibration problem",
    "section": "Stability limit",
    "text": "Stability limit\nSummary: we get\n\\[\n\\left|\\sin^{-1}\\left(\\frac{\\omega \\Delta t}{2} \\right) \\right| &gt; 1\n\\]\nif\n\\[\n\\left| \\frac{\\omega \\Delta t}{2} \\right| &gt; 1\n\\]\nThis happens for\n\\[\n\\Delta t &gt; \\frac{2}{\\omega}\n\\]"
  },
  {
    "objectID": "vibration.html#remember-the-initial-plots",
    "href": "vibration.html#remember-the-initial-plots",
    "title": "A simple vibration problem",
    "section": "Remember the initial plots",
    "text": "Remember the initial plots\n\n\n\n\n\n\n\n\n\nWe have growth for \\(\\Delta t &gt; 2/\\omega\\)."
  },
  {
    "objectID": "vibration.html#about-the-stability-limit",
    "href": "vibration.html#about-the-stability-limit",
    "title": "A simple vibration problem",
    "section": "About the stability limit",
    "text": "About the stability limit\n\n\n\n\n\n\n\n\n\nFor \\(\\Delta t = 2/\\omega\\) there is exactly one timestep between a minimum and a maximum point for the numerical simulation (zigzag pattern). This is absolutely the smallest number of points that can possibly resolve (poorly) a wave of this frequency! So it really does not make sense physically to use larger time steps!"
  },
  {
    "objectID": "vibration.html#alternative-derivation-of-stability-limit",
    "href": "vibration.html#alternative-derivation-of-stability-limit",
    "title": "A simple vibration problem",
    "section": "Alternative derivation of stability limit",
    "text": "Alternative derivation of stability limit\nWe have the difference equation\n\\[\nu^{n+1} = (2-\\Delta t^2\\omega^2) u^n - u^{n-1}\n\\]\nand a numerical solution of the form\n\\[\nu^{n} = A^n I\n\\]\n\nInsert for the numerical solution in the difference equation:\n\\[\nA^{n+1}I = (2-\\Delta t^2 \\omega^2) A^n I - A^{n-1}I\n\\]\n\n\nDivide by \\(I A^{n-1}\\) and rearrange\n\\[\nA^2 - (2-\\Delta t^2 \\omega^2)A + 1 = 0\n\\]"
  },
  {
    "objectID": "vibration.html#alternative-derivation-ctd",
    "href": "vibration.html#alternative-derivation-ctd",
    "title": "A simple vibration problem",
    "section": "Alternative derivation ct’d",
    "text": "Alternative derivation ct’d\nSet \\(p=\\Delta t \\omega\\) and solve second order equation\n\\[\nA = 1 - \\frac{p^2}{2} \\pm \\frac{p}{2}\\sqrt{p^2-4}\n\\]\n\nWe still want \\(|A|=1\\) for constant amplitude and stability. However try \\(p &gt; 2\\) in the above equation (using the minus in front of the last term) and you get\n\\[\nA &lt; -1\n\\]\nCheck:\n\np = sp.Symbol('p') \nf = 1 - p**2/2 - p/2*sp.sqrt(p**2-4)\nf.subs(p, 2.01).n() \n\n\\(\\displaystyle -1.22130109316473\\)"
  },
  {
    "objectID": "vibration.html#alternative-derivation-ctd-1",
    "href": "vibration.html#alternative-derivation-ctd-1",
    "title": "A simple vibration problem",
    "section": "Alternative derivation ct’d",
    "text": "Alternative derivation ct’d\nSet \\(p=\\Delta t \\omega\\) and solve second order equation\n\\[\nA = 1 - \\frac{p^2}{2} \\pm \\frac{p}{2}\\sqrt{p^2-4}\n\\]\nWe still want \\(|A|=1\\) for constant amplitude and stability. However try \\(p &gt; 2\\) in the above equation (using the minus in front of the last term) and you get\n\\[\nA &lt; -1\n\\]\nSo we have growth if \\(p &gt; 2\\), which is the same as \\(\\Delta t \\omega &gt; 2\\) or simply\n\\[\n\\Delta t &gt; \\frac{2}{\\omega}\n\\]\nwhich is the same result as we got using the numerical frequency \\(\\tilde{\\omega}\\)!"
  },
  {
    "objectID": "vibration.html#digression",
    "href": "vibration.html#digression",
    "title": "A simple vibration problem",
    "section": "Digression",
    "text": "Digression\n\n\n\n\n\n\nDigression\n\n\nThis alternative analysis is no different from what we did with the exponential decay. Only the exponential decay was so easy that we did not actually derive the generic A!\n\n\n\n\nConsider the difference equation for exponential decay\n\\[\n\\frac{u^{n+1}-u^{n}}{\\triangle t} = -(1-\\theta)au^{n} - \\theta a u^{n+1}\n\\]\nand assume again that \\(u^n = A^n I\\). Insert this into the above\n\\[\n\\frac{A^{n+1}I-A^{n}I}{\\triangle t} = -(1-\\theta)aA^{n}I - \\theta a A^{n+1}I\n\\]\nDivide by \\(A^n I\\) and rearrange to get the well-known \\(A = \\frac{1-(1-\\theta)\\Delta t a}{1+ \\theta \\Delta t a}\\)"
  },
  {
    "objectID": "vibration.html#key-observations",
    "href": "vibration.html#key-observations",
    "title": "A simple vibration problem",
    "section": "Key observations",
    "text": "Key observations\nWe can draw three important conclusions:\n\n\nThe key parameter in the formulas is \\(p=\\omega\\Delta t\\) or \\(\\tilde{p} = \\tilde{\\omega} \\Delta t\\) (dimensionless)\n\nPeriod of oscillations: \\(P=2\\pi/\\omega\\) vs numerical \\(\\tilde{P}=2 \\pi / \\tilde{\\omega}\\)\nNumber of time steps per period: \\(N_P=P/\\Delta t\\) vs \\(\\tilde{N}_P =\\tilde{P}/\\Delta t\\)\nAt stability limit \\(\\omega = 2/\\Delta t\\) and \\(\\tilde{\\omega}=\\pi/\\Delta t\\) \\(\\Rightarrow P = \\pi \\Delta t\\) and \\(\\tilde{P} = 2 \\Delta t\\). We get \\(\\tilde{N}_P = \\tilde{P}/\\Delta t = 2\\) and the critical parameter is really the number of time steps per numerical period.\n\nFor \\(p\\leq 2\\) the amplitude of \\(u^n\\) is constant (stable solution)\n\\(u^n\\) has a relative frequency error \\(\\tilde\\omega/\\omega \\approx 1 + \\frac{1}{24}p^2\\), making numerical peaks occur too early. This is also called a phase error, or a dispersive error."
  },
  {
    "objectID": "vibration.html#convergence-rates",
    "href": "vibration.html#convergence-rates",
    "title": "A simple vibration problem",
    "section": "Convergence rates",
    "text": "Convergence rates\nLets compute the convergence rate for our solver. However, let it also be possible to choose the numerical frequency \\(\\omega(1-\\omega^2\\Delta t^2/24)\\)\n\ndef solver_adjust(I, w, dt, T, adjust_w=False):\n    Nt = int(round(T/dt))\n    u = np.zeros(Nt+1)\n    t = np.linspace(0, Nt*dt, Nt+1)\n    w_adj = w*(1 - w**2*dt**2/24.) if adjust_w else w\n    u[0] = I\n    u[1] = u[0] - 0.5*dt**2*w_adj**2*u[0]\n    for n in range(1, Nt):\n        u[n+1] = 2*u[n] - u[n-1] - dt**2*w_adj**2*u[n]\n    return u, t\n \ndef u_exact(t, I, w):\n    return I*np.cos(w*t)\n\ndef l2_error(dt, T, w=0.35, I=0.3, adjust_w=False):\n    u, t = solver_adjust(I, w, dt, T, adjust_w)\n    ue = u_exact(t, I, w)\n    return np.sqrt(dt*np.sum((ue-u)**2))"
  },
  {
    "objectID": "vibration.html#convergence-rates-1",
    "href": "vibration.html#convergence-rates-1",
    "title": "A simple vibration problem",
    "section": "Convergence rates",
    "text": "Convergence rates\nWe compute the order of the convergence in the same manner as lecture 2\n\\[\nr = \\frac{\\log {\\frac{E_{i-1}}{E_i}}}{\\log {\\frac{\\Delta t_{i-1}}{\\Delta t_i}}}\n\\]\n\ndef convergence_rates(m, num_periods=8, w=0.35, I=0.3, adjust_w=False):\n    P = 2*np.pi/w\n    dt = 1 / w    # Half stability limit\n    T = P*num_periods\n    dt_values, E_values = [], []\n    for i in range(m):\n        E = l2_error(dt, T, w, I, adjust_w)\n        dt_values.append(dt)\n        E_values.append(E)\n        dt = dt/2.\n    # Compute m-1 orders that should all be the same\n    r = [np.log(E_values[i-1]/E_values[i])/\n         np.log(dt_values[i-1]/dt_values[i])\n         for i in range(1, m, 1)]\n    return r, E_values, dt_values"
  },
  {
    "objectID": "vibration.html#try-it",
    "href": "vibration.html#try-it",
    "title": "A simple vibration problem",
    "section": "Try it!",
    "text": "Try it!\nPrint the computed convergence rates\n\nconvergence_rates(5, w=0.5, I=1)[0]\n\n[1.9487030166752326, 2.022988970233581, 2.0050238810553545, 2.0013403129395835]\n\n\nAdjusted solver:\n\nconvergence_rates(5, w=0.5, I=1, adjust_w=True)[0]\n\n[4.133074797577663, 4.035054365992913, 4.008071155725109, 4.0020125447469646]"
  },
  {
    "objectID": "vibration.html#plot-convergence-regular-solver",
    "href": "vibration.html#plot-convergence-regular-solver",
    "title": "A simple vibration problem",
    "section": "plot convergence regular solver",
    "text": "plot convergence regular solver\n\nfrom plotslopes import slope_marker\nr, E, dt = convergence_rates(5)\nplt.loglog(dt, E, dt, 10*np.array(dt), dt, np.array(dt)**2)\nplt.title('Convergence of finite difference method')\nplt.legend(['Error', '$\\\\mathcal{O}(\\\\Delta t)$', '$\\\\mathcal{O}(\\\\Delta t^2)$'])\nslope_marker((dt[1], E[1]), (2,1))\nslope_marker((dt[1], 10*dt[1]), (1,1))\nslope_marker((dt[1], dt[1]**2), (2,1))"
  },
  {
    "objectID": "vibration.html#plot-convergence-adjusted-solver",
    "href": "vibration.html#plot-convergence-adjusted-solver",
    "title": "A simple vibration problem",
    "section": "plot convergence adjusted solver",
    "text": "plot convergence adjusted solver\n\nfrom plotslopes import slope_marker\nr, E, dt = convergence_rates(5, adjust_w=True)\nplt.loglog(dt, E, dt, np.array(dt)**4)\nplt.title('Convergence of finite difference method')\nplt.legend(['Error', '$\\\\mathcal{O}(\\\\Delta t^4)$'])\nslope_marker((dt[1], E[1]), (4,1))\nslope_marker((dt[1], dt[1]**4), (4,1))"
  },
  {
    "objectID": "vibration.html#add-tests",
    "href": "vibration.html#add-tests",
    "title": "A simple vibration problem",
    "section": "Add tests",
    "text": "Add tests\nThe exact solution will not equal the numerical, but the order of the error is something we can test for.\n\ndef test_order(m, w=1, I=1, adjust_w=False):\n    r, E, dt = convergence_rates(m, w=w, I=I, adjust_w=adjust_w)\n    true_order = 4 if adjust_w else 2\n    error = abs(r[-1]-true_order)\n    try:\n      assert error &lt; 0.005\n      print(f'Test passed!!')\n    except AssertionError:\n      print(f'Test failed!! orders = {r}')\n\n\nRun test for \\(m=4\\) levels, \\(w=0.5, I=1\\) and adjust_w=False\n\ntest_order(4, w=0.5, I=1, adjust_w=False)\n\nTest failed!! orders = [1.9487030166752326, 2.022988970233581, 2.0050238810553545]\n\n\n\n\nTest fails. Try one more level\n\ntest_order(5, w=0.5, I=1, adjust_w=False)\n\nTest passed!!\n\n\n\ntest_order(5, w=0.5, I=1, adjust_w=True)\n\nTest passed!!"
  },
  {
    "objectID": "vibration.html#final-test",
    "href": "vibration.html#final-test",
    "title": "A simple vibration problem",
    "section": "Final test",
    "text": "Final test\nUse simply an assert clause and do not catch the error.\n\ndef test_order(m, w=1, I=1, adjust_w=False):\n    r, E, dt = convergence_rates(m, w=w, I=I, adjust_w=adjust_w)\n    true_order = 4 if adjust_w else 2\n    error = abs(r[-1]-true_order)\n    assert error &lt; 0.005, r\n\ntest_order(5, w=0.5, I=1, adjust_w=True)\n\nThis test will work with pytest."
  },
  {
    "objectID": "analysis.html#recap---finite-differencing-of-exponential-decay",
    "href": "analysis.html#recap---finite-differencing-of-exponential-decay",
    "title": "Analysis of exponential decay models",
    "section": "Recap - Finite differencing of exponential decay",
    "text": "Recap - Finite differencing of exponential decay\n\n\n\n\n\n\n\n\n\nThe ordinary differential equation\n\n\n\\[\nu'(t) = -au(t),\\quad u(0)=I, \\quad y \\in (0, T]\n\\] where \\(a&gt;0\\) is a constant.\n\n\n\nSolve the ODE by finite difference methods:\n\nDiscretize in time:\n\\[0 = t_0 &lt; t_1 &lt; t_2 &lt; \\cdots &lt; t_{N_t-1} &lt; t_{N_t} = T\\]\nSatisfy the ODE at \\(N_t\\) discrete time steps:\n\\[\n\\begin{align}\nu'(t_n) &= -a u(t_n), \\quad &n\\in [1, \\ldots, N_t], \\text{ or} \\\\\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) &= -a u(t_{n+\\scriptstyle\\frac{1}{2}}), \\quad &n\\in [0, \\ldots, N_t-1]\n\\end{align}\n\\]"
  },
  {
    "objectID": "analysis.html#finite-difference-algorithms",
    "href": "analysis.html#finite-difference-algorithms",
    "title": "Analysis of exponential decay models",
    "section": "Finite difference algorithms",
    "text": "Finite difference algorithms\n\nDiscretization by a generic \\(\\theta\\)-rule\n\n\\[\n\\frac{u^{n+1}-u^{n}}{\\triangle t} = -(1-\\theta)au^{n} - \\theta a u^{n+1}\n\\]\n\\[\n\\begin{cases}\n  \\theta = 0 \\quad &\\text{Forward Euler} \\\\\n  \\theta = 1 \\quad &\\text{Backward Euler} \\\\\n  \\theta = 1/2 \\quad &\\text{Crank-Nicolson}\n  \\end{cases}\n\\]\nNote \\(u^n = u(t_n)\\)\n\nSolve recursively: Set \\(u^0 = I\\) and then\n\n\\[\nu^{n+1} = \\frac{1-(1-\\theta)a \\triangle t}{1+\\theta a \\triangle t}u^{n} \\quad \\text{for } n=0, 1, \\ldots\n\\]"
  },
  {
    "objectID": "analysis.html#analysis-of-finite-difference-equations",
    "href": "analysis.html#analysis-of-finite-difference-equations",
    "title": "Analysis of exponential decay models",
    "section": "Analysis of finite difference equations",
    "text": "Analysis of finite difference equations\nModel: \\[\nu'(t) = -au(t),\\quad u(0)=I\n\\]\nMethod:\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n\n\\]\n\n\n\n\n\n\nProblem setting\n\n\nHow good is this method? Is it safe to use it?"
  },
  {
    "objectID": "analysis.html#solver",
    "href": "analysis.html#solver",
    "title": "Analysis of exponential decay models",
    "section": "Solver",
    "text": "Solver\nWe already have a solver that we can use to experiment with. Lets run it for a range of different timesteps.\n\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n    return u, t"
  },
  {
    "objectID": "analysis.html#encouraging-numerical-solutions---backwards-euler",
    "href": "analysis.html#encouraging-numerical-solutions---backwards-euler",
    "title": "Analysis of exponential decay models",
    "section": "Encouraging numerical solutions - Backwards Euler",
    "text": "Encouraging numerical solutions - Backwards Euler\n\\(I=1\\), \\(a=2\\), \\(\\theta =1\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#discouraging-numerical-solutions---crank-nicolson",
    "href": "analysis.html#discouraging-numerical-solutions---crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Discouraging numerical solutions - Crank-Nicolson",
    "text": "Discouraging numerical solutions - Crank-Nicolson\n\\(I=1\\), \\(a=2\\), \\(\\theta=0.5\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#discouraging-numerical-solutions---forward-euler",
    "href": "analysis.html#discouraging-numerical-solutions---forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "Discouraging numerical solutions - Forward Euler",
    "text": "Discouraging numerical solutions - Forward Euler\n\\(I=1\\), \\(a=2\\), \\(\\theta=0\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#summary-of-observations",
    "href": "analysis.html#summary-of-observations",
    "title": "Analysis of exponential decay models",
    "section": "Summary of observations",
    "text": "Summary of observations\nThe characteristics of the displayed curves can be summarized as follows:\n\nThe Backward Euler scheme always gives a monotone solution, lying above the exact solution.\nThe Crank-Nicolson scheme gives the most accurate results, but for \\(\\Delta t=1.25\\) the solution oscillates.\nThe Forward Euler scheme gives a growing, oscillating solution for \\(\\Delta t=1.25\\); a decaying, oscillating solution for \\(\\Delta t=0.75\\); a strange solution \\(u^n=0\\) for \\(n\\geq 1\\) when \\(\\Delta t=0.5\\); and a solution seemingly as accurate as the one by the Backward Euler scheme for \\(\\Delta t = 0.1\\), but the curve lies below the exact solution.\nSmall enough \\(\\Delta t\\) gives stable and accurate solution for all methods!"
  },
  {
    "objectID": "analysis.html#problem-setting-1",
    "href": "analysis.html#problem-setting-1",
    "title": "Analysis of exponential decay models",
    "section": "Problem setting",
    "text": "Problem setting\n\n\n\n\n\n\nWe ask the question\n\n\n\nUnder what circumstances, i.e., values of the input data \\(I\\), \\(a\\), and \\(\\Delta t\\) will the Forward Euler and Crank-Nicolson schemes result in undesired oscillatory solutions?\n\nTechniques of investigation:\n\nNumerical experiments\nMathematical analysis\n\nAnother question to be raised is\n\nHow does \\(\\Delta t\\) impact the error in the numerical solution?"
  },
  {
    "objectID": "analysis.html#exact-numerical-solution",
    "href": "analysis.html#exact-numerical-solution",
    "title": "Analysis of exponential decay models",
    "section": "Exact numerical solution",
    "text": "Exact numerical solution\nFor the simple exponential decay problem we are lucky enough to have an exact numerical solution\n\\[\nu^{n} = IA^n,\\quad A = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}\n\\]\nSuch a formula for the exact discrete solution is unusual to obtain in practice, but very handy for our analysis here.\n\n\n\n\n\n\nNote\n\n\nAn exact dicrete solution fulfills a discrete equation (without round-off errors), whereas an exact solution fulfills the original mathematical equation."
  },
  {
    "objectID": "analysis.html#stability",
    "href": "analysis.html#stability",
    "title": "Analysis of exponential decay models",
    "section": "Stability",
    "text": "Stability\nSince \\(u^n=I A^n\\),\n\n\\(A &lt; 0\\) gives a factor \\((-1)^n\\) and oscillatory solutions\n\\(|A|&gt;1\\) gives growing solutions\nRecall: the exact solution is monotone and decaying\nIf these qualitative properties are not met, we say that the numerical solution is unstable\n\n\nFor stability we need\n\\[\nA &gt; 0 \\quad \\text{ and } \\quad |A| \\le 1\n\\]"
  },
  {
    "objectID": "analysis.html#computation-of-stability-in-this-problem",
    "href": "analysis.html#computation-of-stability-in-this-problem",
    "title": "Analysis of exponential decay models",
    "section": "Computation of stability in this problem",
    "text": "Computation of stability in this problem\n\\(A &lt; 0\\) if\n\\[\n\\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t} &lt; 0\n\\]\nTo avoid oscillatory solutions we must have \\(A&gt; 0\\), which happens for\n\n\\[\n\\Delta t &lt; \\frac{1}{(1-\\theta)a}, \\quad \\text{for} \\, \\theta &lt; 1\n\\]\n\nAlways fulfilled for Backward Euler (\\(\\theta=1 \\rightarrow 1 &lt; 1+a \\Delta t\\) always true)\n\\(\\Delta t \\leq 1/a\\) for Forward Euler (\\(\\theta=0\\))\n\\(\\Delta t \\leq 2/a\\) for Crank-Nicolson (\\(\\theta = 0.5\\))\n\nWe get oscillatory solutions for FE when \\(\\Delta t \\le 1/a\\) and for CN when \\(\\Delta t \\le 2/a\\)"
  },
  {
    "objectID": "analysis.html#computation-of-stability-in-this-problem-1",
    "href": "analysis.html#computation-of-stability-in-this-problem-1",
    "title": "Analysis of exponential decay models",
    "section": "Computation of stability in this problem",
    "text": "Computation of stability in this problem\n\\(|A|\\leq 1\\) means \\(-1\\leq A\\leq 1\\)\n\\[\n-1\\leq\\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t} \\leq 1\n\\]\n\n\\(-1\\) is the critical limit (because \\(A\\le 1\\) is always satisfied).\n\\(-1 &lt; A\\) is always fulfilled for Backward Euler (\\(\\theta=1\\)) and Crank-Nicolson (\\(\\theta=0.5\\)).\nFor forward Euler or simply \\(\\theta &lt; 0.5\\) we have \\[\n\\Delta t \\leq \\frac{2}{(1-2\\theta)a},\\quad\n\\] and thus \\(\\Delta t \\leq 2/a\\) for stability of the forward Euler (\\(\\theta=0\\)) method"
  },
  {
    "objectID": "analysis.html#explanation-of-problems-with-forward-euler",
    "href": "analysis.html#explanation-of-problems-with-forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "Explanation of problems with forward Euler",
    "text": "Explanation of problems with forward Euler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\Delta t= 2\\cdot 1.25=2.5\\) and \\(A=-1.5\\): oscillations and growth\n\\(a\\Delta t = 2\\cdot 0.75=1.5\\) and \\(A=-0.5\\): oscillations and decay\n\\(\\Delta t=0.5\\) and \\(A=0\\): \\(u^n=0\\) for \\(n&gt;0\\)\nSmaller \\(\\Delta t\\): qualitatively correct solution"
  },
  {
    "objectID": "analysis.html#explanation-of-problems-with-crank-nicolson",
    "href": "analysis.html#explanation-of-problems-with-crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Explanation of problems with Crank-Nicolson",
    "text": "Explanation of problems with Crank-Nicolson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Delta t=1.25\\) and \\(A=-0.25\\): oscillatory solution\n\nNever any growing solution"
  },
  {
    "objectID": "analysis.html#summary-of-stability",
    "href": "analysis.html#summary-of-stability",
    "title": "Analysis of exponential decay models",
    "section": "Summary of stability",
    "text": "Summary of stability\n\nForward Euler is conditionally stable\n\n\\(\\Delta t &lt; 2/a\\) for avoiding growth\n\\(\\Delta t\\leq 1/a\\) for avoiding oscillations\n\nThe Crank-Nicolson is unconditionally stable wrt growth and conditionally stable wrt oscillations\n\n\\(\\Delta t &lt; 2/a\\) for avoiding oscillations\n\nBackward Euler is unconditionally stable"
  },
  {
    "objectID": "analysis.html#comparing-amplification-factors",
    "href": "analysis.html#comparing-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Comparing amplification factors",
    "text": "Comparing amplification factors\n\\(u^{n+1}\\) is an amplification \\(A\\) of \\(u^n\\):\n\\[\nu^{n+1} = Au^n,\\quad A = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}\n\\]\nThe exact solution is also an amplification:\n\\[\n\\begin{align}\nu(t_{n+1}) &= e^{-a(t_n+\\Delta t)} \\\\\nu(t_{n+1}) &= e^{-a \\Delta t} e^{-a t_n} \\\\\nu(t_{n+1}) &= A_e u(t_n), \\quad A_e = e^{-a\\Delta t}\n\\end{align}\n\\]\nA possible measure of accuracy: \\(A_e - A\\)"
  },
  {
    "objectID": "analysis.html#plotting-amplification-factors",
    "href": "analysis.html#plotting-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Plotting amplification factors",
    "text": "Plotting amplification factors"
  },
  {
    "objectID": "analysis.html#padelta-t-is-the-important-parameter-for-numerical-performance",
    "href": "analysis.html#padelta-t-is-the-important-parameter-for-numerical-performance",
    "title": "Analysis of exponential decay models",
    "section": "\\(p=a\\Delta t\\) is the important parameter for numerical performance",
    "text": "\\(p=a\\Delta t\\) is the important parameter for numerical performance\n\n\\(p=a\\Delta t\\) is a dimensionless parameter\nall expressions for stability and accuracy involve \\(p\\)\nNote that \\(\\Delta t\\) alone is not so important, it is the combination with \\(a\\) through \\(p=a\\Delta t\\) that matters\n\n\n\n\n\n\n\nAnother evidence why \\(p=a\\Delta t\\) is key\n\n\nIf we scale the model by \\(\\bar t=at\\), \\(\\bar u=u/I\\), we get \\(d\\bar u/d\\bar t = -\\bar u\\), \\(\\bar u(0)=1\\) (no physical parameters!). The analysis show that \\(\\Delta \\bar t\\) is key, corresponding to \\(a\\Delta t\\) in the unscaled model."
  },
  {
    "objectID": "analysis.html#series-expansion-of-amplification-factors",
    "href": "analysis.html#series-expansion-of-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Series expansion of amplification factors",
    "text": "Series expansion of amplification factors\nTo investigate \\(A_e - A\\) mathematically, we can Taylor expand the expression, using \\(p=a\\Delta t\\) as variable.\n\nfrom sympy import *\n# Create p as a mathematical symbol with name 'p'\np = Symbol('p', positive=True)\n# Create a mathematical expression with p\nA_e = exp(-p)\n# Find the first 6 terms of the Taylor series of A_e\nA_e.series(p, 0, 6)\n\n\\(\\displaystyle 1 - p + \\frac{p^{2}}{2} - \\frac{p^{3}}{6} + \\frac{p^{4}}{24} - \\frac{p^{5}}{120} + O\\left(p^{6}\\right)\\)\n\n\nThis is the Taylor expansion of the exact amplification factor. How does it compare with the numerical amplification factors?"
  },
  {
    "objectID": "analysis.html#numerical-amplification-factors",
    "href": "analysis.html#numerical-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Numerical amplification factors",
    "text": "Numerical amplification factors\nCompute the Taylor expansions of \\(A_e - A\\)\n\nfrom IPython.display import display\ntheta = Symbol('theta', positive=True)\nA = (1-(1-theta)*p)/(1+theta*p)\nFE = A_e.series(p, 0, 4) - A.subs(theta, 0).series(p, 0, 4)\nBE = A_e.series(p, 0, 4) - A.subs(theta, 1).series(p, 0, 4)\nhalf = Rational(1, 2)  # exact fraction 1/2\nCN = A_e.series(p, 0, 4) - A.subs(theta, half).series(p, 0, 4)\ndisplay(FE)\ndisplay(BE)\ndisplay(CN)\n\n\\(\\displaystyle \\frac{p^{2}}{2} - \\frac{p^{3}}{6} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle - \\frac{p^{2}}{2} + \\frac{5 p^{3}}{6} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle \\frac{p^{3}}{12} + O\\left(p^{4}\\right)\\)\n\n\n\nForward/backward Euler have leading error \\(p^2\\), or more commonly \\(\\Delta t^2\\)\nCrank-Nicolson has leading error \\(p^3\\), or \\(\\Delta t^3\\)"
  },
  {
    "objectID": "analysis.html#the-trueglobal-error-at-a-point",
    "href": "analysis.html#the-trueglobal-error-at-a-point",
    "title": "Analysis of exponential decay models",
    "section": "The true/global error at a point",
    "text": "The true/global error at a point\n\nThe error in \\(A\\) reflects the local (amplification) error when going from one time step to the next\nWhat is the global (true) error at \\(t_n\\)?\n\n\\[\ne^n = u_e(t_n) - u^n = Ie^{-at_n} - IA^n\n\\]\n\nTaylor series expansions of \\(e^n\\) simplify the expression"
  },
  {
    "objectID": "analysis.html#computing-the-global-error-at-a-point",
    "href": "analysis.html#computing-the-global-error-at-a-point",
    "title": "Analysis of exponential decay models",
    "section": "Computing the global error at a point",
    "text": "Computing the global error at a point\n\nn = Symbol('n', integer=True, positive=True)\nu_e = exp(-p*n)   # I=1\nu_n = A**n        # I=1\nFE = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)\nBE = u_e.series(p, 0, 4) - u_n.subs(theta, 1).series(p, 0, 4)\nCN = u_e.series(p, 0, 4) - u_n.subs(theta, half).series(p, 0, 4)\ndisplay(simplify(FE))\ndisplay(simplify(BE))\ndisplay(simplify(CN))\n\n\\(\\displaystyle \\frac{n p^{2}}{2} + \\frac{n p^{3}}{3} - \\frac{n^{2} p^{3}}{2} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle - \\frac{n p^{2}}{2} + \\frac{n p^{3}}{3} + \\frac{n^{2} p^{3}}{2} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle \\frac{n p^{3}}{12} + O\\left(p^{4}\\right)\\)\n\n\nSubstitute \\(n\\) by \\(t/\\Delta t\\) and \\(p\\) by \\(a \\Delta t\\):\n\nForward and Backward Euler: leading order term \\(\\frac{1}{2} ta^2\\Delta t\\)\nCrank-Nicolson: leading order term \\(\\frac{1}{12}ta^3\\Delta t^2\\)"
  },
  {
    "objectID": "analysis.html#convergence",
    "href": "analysis.html#convergence",
    "title": "Analysis of exponential decay models",
    "section": "Convergence",
    "text": "Convergence\nThe numerical scheme is convergent if the global error \\(e^n\\rightarrow 0\\) as \\(\\Delta t\\rightarrow 0\\). If the error has a leading order term \\((\\Delta t)^r\\), the convergence rate is of order \\(r\\)."
  },
  {
    "objectID": "analysis.html#integrated-errors",
    "href": "analysis.html#integrated-errors",
    "title": "Analysis of exponential decay models",
    "section": "Integrated errors",
    "text": "Integrated errors\nThe \\(\\ell^2\\) norm of the numerical error is computed as\n\\[\n||e^n||_{\\ell^2} = \\sqrt{\\Delta t\\sum_{n=0}^{N_t} ({u_{e}}(t_n) - u^n)^2}\n\\]\nWe can compute this using Sympy. Forward/Backward Euler has \\(e^n \\sim np^2/2\\)\n\nh, N, a, T = symbols('h,N,a,T') # h represents Delta t\nsimplify(sqrt(h * summation((n*p**2/2)**2, (n, 0, N))).subs(p, a*h).subs(N, T/h))\n\n\\(\\displaystyle \\frac{\\sqrt{6} a^{2} h^{2} \\sqrt{T \\left(\\frac{2 T^{2}}{h^{2}} + \\frac{3 T}{h} + 1\\right)}}{12}\\)\n\n\nIf we keep only the leading term in the parenthesis, we get the first order \\[\n||e^n||_{\\ell^2} \\approx \\frac{1}{2}\\sqrt{\\frac{T^3}{3}} a^2\\Delta t\n\\]"
  },
  {
    "objectID": "analysis.html#crank-nicolson",
    "href": "analysis.html#crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Crank-Nicolson",
    "text": "Crank-Nicolson\nFor Crank-Nicolson the pointwise error is \\(e^n \\sim n p^3 / 12\\). We get\n\nsimplify(sqrt(h * summation((n*p**3/12)**2, (n, 0, N))).subs(p, a*h).subs(N, T/h))\n\n\\(\\displaystyle \\frac{\\sqrt{6} a^{3} h^{3} \\sqrt{T \\left(\\frac{2 T^{2}}{h^{2}} + \\frac{3 T}{h} + 1\\right)}}{72}\\)\n\n\nwhich is simplified to the second order accurate\n\\[\n||e^n||_{\\ell^2} \\approx \\frac{1}{12}\\sqrt{\\frac{T^3}{3}}a^3\\Delta t^2\n\\]\n\n\n\n\n\n\nSummary of errors\n\n\nAnalysis of both the pointwise and the time-integrated true errors:\n\n1st order for Forward and Backward Euler\n2nd order for Crank-Nicolson"
  },
  {
    "objectID": "analysis.html#truncation-error",
    "href": "analysis.html#truncation-error",
    "title": "Analysis of exponential decay models",
    "section": "Truncation error",
    "text": "Truncation error\n\nHow good is the discrete equation?\nPossible answer: see how well \\(u_{e}\\) fits the discrete equation\n\nConsider the forward difference equation \\[\n\\frac{u^{n+1}-u^n}{\\Delta t} = -au^n\n\\]\nInsert \\(u_{e}\\) to obtain a truncation error \\(R^n\\)\n\\[\n\\frac{u_{e}(t_{n+1})-u_{e}(t_n)}{\\Delta t} + au_{e}(t_n) = R^n \\neq 0\n\\]"
  },
  {
    "objectID": "analysis.html#computation-of-the-truncation-error",
    "href": "analysis.html#computation-of-the-truncation-error",
    "title": "Analysis of exponential decay models",
    "section": "Computation of the truncation error",
    "text": "Computation of the truncation error\n\nThe residual \\(R^n\\) is the truncation error. How does \\(R^n\\) vary with \\(\\Delta t\\)?\n\nTool: Taylor expand \\(u_{e}\\) around the point where the ODE is sampled (here \\(t_n\\))\n\\[\nu_{e}(t_{n+1}) = u_{e}(t_n) + u_{e}'(t_n)\\Delta t + \\frac{1}{2}u_{e}''(t_n)\n\\Delta t^2 + \\cdots\n\\]\nInserting this Taylor series for \\(u_{e}\\) in the forward difference equation\n\\[\nR^n = \\frac{u_{e}(t_{n+1})-u_{e}(t_n)}{\\Delta t} + au_{e}(t_n)\n\\]\nto get\n\\[\nR^n = u_{e}'(t_n) + \\frac{1}{2}u_{e}''(t_n)\\Delta t + \\ldots + au_{e}(t_n)\n\\]"
  },
  {
    "objectID": "analysis.html#the-truncation-error-forward-euler",
    "href": "analysis.html#the-truncation-error-forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "The truncation error forward Euler",
    "text": "The truncation error forward Euler\nWe have \\[\nR^n = u_{e}'(t_n) + \\frac{1}{2}u_{e}''(t_n)\\Delta t + \\ldots + au_{e}(t_n)\n\\]\nSince \\(u_{e}\\) solves the ODE \\(u_{e}'(t_n)=-au_{e}(t_n)\\), we get that \\(u_{e}'(t_n)\\) and \\(au_{e}(t_n)\\) cancel out. We are left with leading term\n\\[\nR^n \\approx \\frac{1}{2}u_{e}''(t_n)\\Delta t\n\\]\nThis is a mathematical expression for the truncation error."
  },
  {
    "objectID": "analysis.html#the-truncation-error-for-other-schemes",
    "href": "analysis.html#the-truncation-error-for-other-schemes",
    "title": "Analysis of exponential decay models",
    "section": "The truncation error for other schemes",
    "text": "The truncation error for other schemes\nBackward Euler:\n\\[\nR^n \\approx -\\frac{1}{2}u_{e}''(t_n)\\Delta t\n\\]\nCrank-Nicolson:\n\\[\nR^{n+\\scriptstyle\\frac{1}{2}} \\approx \\frac{1}{24}u_{e}'''(t_{n+\\scriptstyle\\frac{1}{2}})\\Delta t^2\n\\]"
  },
  {
    "objectID": "analysis.html#consistency-stability-and-convergence",
    "href": "analysis.html#consistency-stability-and-convergence",
    "title": "Analysis of exponential decay models",
    "section": "Consistency, stability, and convergence",
    "text": "Consistency, stability, and convergence\n\nTruncation error measures the residual in the difference equations. The scheme is consistent if the truncation error goes to 0 as \\(\\Delta t\\rightarrow 0\\). Importance: the difference equations approaches the differential equation as \\(\\Delta t\\rightarrow 0\\).\nStability means that the numerical solution exhibits the same qualitative properties as the exact solution. Here: monotone, decaying function.\nConvergence implies that the true (global) error \\(e^n =u_{e}(t_n)-u^n\\rightarrow 0\\) as \\(\\Delta t\\rightarrow 0\\). This is really what we want!\n\nThe Lax equivalence theorem for linear differential equations: consistency + stability is equivalent with convergence.\n(Consistency and stability is in most problems much easier to establish than convergence.)"
  },
  {
    "objectID": "analysis.html#numerical-computation-of-convergence-rate",
    "href": "analysis.html#numerical-computation-of-convergence-rate",
    "title": "Analysis of exponential decay models",
    "section": "Numerical computation of convergence rate",
    "text": "Numerical computation of convergence rate\nWe assume that the \\(\\ell^2\\) error norm on the mesh with level \\(i\\) can be written as\n\\[\nE_i = C (\\Delta t_i)^r\n\\]\nwhere \\(C\\) is a constant. This way, if we have the error on two levels, then we can compute\n\\[\n\\frac{E_{i-1}}{E_i} = \\frac{ (\\Delta t_{i-1})^r}{(\\Delta t_{i})^r} = \\left( \\frac{\\Delta t_{i-1}}{ \\Delta t_i} \\right)^r\n\\]\nand isolate \\(r\\) by computing\n\\[\nr = \\frac{\\log {\\frac{E_{i-1}}{E_i}}}{\\log {\\frac{\\Delta t_{i-1}}{\\Delta t_i}}}\n\\]"
  },
  {
    "objectID": "analysis.html#function-for-convergence-rate",
    "href": "analysis.html#function-for-convergence-rate",
    "title": "Analysis of exponential decay models",
    "section": "Function for convergence rate",
    "text": "Function for convergence rate\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\n\ndef l2_error(I, a, theta, dt): \n    u, t = solver(I, a, T, dt, theta)\n    en = u_exact(t, I, a) - u\n    return np.sqrt(dt*np.sum(en**2)) \n\ndef convergence_rates(m, I=1, a=2, T=8, theta=1, dt=1.):\n    dt_values, E_values = [], []\n    for i in range(m):\n        E = l2_error(I, a, theta, dt)\n        dt_values.append(dt)\n        E_values.append(E)\n        dt = dt/2\n    # Compute m-1 orders that should all be the same\n    r = [np.log(E_values[i-1]/E_values[i])/\n         np.log(dt_values[i-1]/dt_values[i])\n         for i in range(1, m, 1)]\n    return r"
  },
  {
    "objectID": "analysis.html#test-convergence-rates",
    "href": "analysis.html#test-convergence-rates",
    "title": "Analysis of exponential decay models",
    "section": "Test convergence rates",
    "text": "Test convergence rates\nBackward Euler:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 1.\nconvergence_rates(4, I, a, T, theta, dt)\n\n[0.9619265651066382, 0.98003334385805, 0.9897576131285538]\n\n\nForward Euler:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 0.\nconvergence_rates(4, I, a, T, theta, dt)\n\n[1.0472640894307232, 1.0222599097461846, 1.0108154242259877]\n\n\nCrank-Nicolson:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 0.5\nconvergence_rates(4, I, a, T, theta, dt)\n\n[2.0037335266421343, 2.0009433957768175, 2.000236481071457]\n\n\nAll in good agreement with theory:-)"
  },
  {
    "objectID": "intro.html#hans-petter-langtangen-1962-2016",
    "href": "intro.html#hans-petter-langtangen-1962-2016",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Hans Petter Langtangen 1962-2016",
    "text": "Hans Petter Langtangen 1962-2016\n\n\n\n\n\n\n\n\n\n\n2011-2015 Editor-In-Chief SIAM J of Scientific Computing\nAuthor of 13 published books on scientific computing\nProfessor of Mechanics, University of Oslo 1998\nDeveloped INF5620 (which became IN5270 and now MAT-MEK4270)\nMemorial page"
  },
  {
    "objectID": "intro.html#a-little-bit-about-myself",
    "href": "intro.html#a-little-bit-about-myself",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A little bit about myself",
    "text": "A little bit about myself\n  \n\nProfessor of mechanics (2019-)\nPhD (Chalmers University of Technology) in mathematical modelling of turbulent combustion\nNorwegian Defence Research Establishment (2007-2012)\nComputational Fluid Dynamics\nHigh Performance Computing\nSpectral methods"
  },
  {
    "objectID": "intro.html#principal-developer-of-shenfun",
    "href": "intro.html#principal-developer-of-shenfun",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Principal developer of Shenfun",
    "text": "Principal developer of Shenfun\nHigh performance computing platform for solving PDEs by the spectral Galerkin method. Written in Python (Cython). https://github.com/spectralDNS/shenfun"
  },
  {
    "objectID": "intro.html#mat-mek4270-in-a-nutshell",
    "href": "intro.html#mat-mek4270-in-a-nutshell",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "MAT-MEK4270 in a nutshell",
    "text": "MAT-MEK4270 in a nutshell\n\nNumerical methods for partial differential equations (PDEs)\nHow to solve the equations, not why\nHow do we solve a PDE in practice?\nHow do we trust the answer?\nIs the numerical scheme stable? accurate? consistent?\nFocus on programming (github, python, testing code)\nIN5670 -&gt; IN5270 -&gt; MAT-MEK4270 - Lots of old material"
  },
  {
    "objectID": "intro.html#syllabus",
    "href": "intro.html#syllabus",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\nImportant stuff\n\n\n\nLecture notes\nPresentations (including this one)\nGithub organization MATMEK-4270\n\n\n\n\n\n\n\n\n\n\nAlso important stuff, but less so as I will try to put all really important stuff in the lecture notes\n\n\n\nLangtangen, Finite Difference Computing with exponential decay - Chapters 1 and 2.\nLangtangen and Linge, Finite Difference Computing with PDEs - Parts of chapters 1 and 2.\nLangtangen and Mardal, Introduction to Numerical Methods for Variational Problems"
  },
  {
    "objectID": "intro.html#two-major-approaches",
    "href": "intro.html#two-major-approaches",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Two major approaches",
    "text": "Two major approaches\n\n\nFinite differences\n\\[\n\\frac{du(t)}{dt} \\approx \\frac{u(t+\\Delta t) - u(t)}{\\Delta t}\n\\]\n\nApproximate in points\nUniform grid\nTaylor expansions\n\n\nVariational methods\n\\[\n\\int_{\\Omega} u'' v d\\Omega = -\\int_{\\Omega} u' v' d\\Omega + \\int_{\\Gamma} u'v d\\Gamma\n\\]\n\nApproximate weakly\nFinite element method\nLeast squares method\nGalerkin method\n\n\n\nWe will use both approaches to first consider function approximations and then the approximation of equations."
  },
  {
    "objectID": "intro.html#required-software-skills",
    "href": "intro.html#required-software-skills",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Required software skills",
    "text": "Required software skills\n\nOur software platform: Python, Jupyter notebooks\nImportant Python packages: numpy, scipy, matplotlib, sympy, shenfun, …\nAnaconda Python, conda environments"
  },
  {
    "objectID": "intro.html#assumedideal-background",
    "href": "intro.html#assumedideal-background",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Assumed/ideal background",
    "text": "Assumed/ideal background\n\nIN1900: Python programming, solution of ODEs\nSome experience with finite difference methods\nSome analytical and numerical knowledge of PDEs\nMuch experience with calculus and linear algebra\nMuch experience with programming of mathematical problems\nExperience with mathematical modeling with PDEs (from physics, mechanics, geophysics, or …)"
  },
  {
    "objectID": "intro.html#exponential-decay-model",
    "href": "intro.html#exponential-decay-model",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Exponential decay model",
    "text": "Exponential decay model\n\n\n\n\n\n\nODE problem\n\n\n\\[\nu'=-au,\\quad u(0)=I,\\ t\\in (0,T]\n\\]\nwhere \\(a&gt;0\\) is a constant and \\(u(t)\\) is the time-dependent solution.\n\n\n\n\nWe study first a simple 1D ODE, because this will lead us to the building blocks that we need for solving PDEs!\nWe can more easily study the concepts of stability, accuracy, convergence and consistency.\n\n\nSee Langtangen, Finite Difference Computing - Chapter 1"
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example",
    "href": "intro.html#what-to-learn-in-the-start-up-example",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example",
    "text": "What to learn in the start-up example\n\nHow to think when constructing finite difference methods, with special focus on the Forward Euler, Backward Euler, and Crank-Nicolson (midpoint) schemes\nHow to formulate a computational algorithm and translate it into Python code\nHow to optimize the code for computational speed\nHow to plot the solutions\nHow to compute numerical errors and convergence rates\nHow to analyse the numerical solution"
  },
  {
    "objectID": "intro.html#finite-difference-methods",
    "href": "intro.html#finite-difference-methods",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Finite difference methods",
    "text": "Finite difference methods\n\nThe finite difference method is the simplest method for solving differential equations\nSatisfy the equations in discrete points, not continuously\nFast to learn, derive, and implement\nA very useful tool to know, even if you aim at using the finite element or the finite volume method"
  },
  {
    "objectID": "intro.html#the-steps-in-the-finite-difference-method",
    "href": "intro.html#the-steps-in-the-finite-difference-method",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The steps in the finite difference method",
    "text": "The steps in the finite difference method\nSolving a differential equation by a finite difference method consists of four steps:\n\ndiscretizing the domain,\nfulfilling the equation at discrete time points,\nreplacing derivatives by finite differences,\nsolve the discretized problem. (Often with a recursive algorithm in 1D)"
  },
  {
    "objectID": "intro.html#step-1-discretizing-the-domain",
    "href": "intro.html#step-1-discretizing-the-domain",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 1: Discretizing the domain",
    "text": "Step 1: Discretizing the domain\nThe time domain \\([0,T]\\) is represented by a mesh: a finite number of \\(N_t+1\\) points\n\\[\n0 = t_0 &lt; t_1 &lt; t_2 &lt; \\cdots &lt; t_{N_t-1} &lt; t_{N_t} = T\n\\]\n\n\nWe seek the solution \\(u\\) at the mesh points: \\(u(t_n)\\), \\(n=1,2,\\ldots,N_t\\).\nNote: \\(u^0\\) is known as \\(I\\).\nNotational short-form for the numerical approximation to \\(u(t_n)\\): \\(u^n\\)\nIn the differential equation: \\(u(t)\\) is the exact solution\nIn the numerical method and implementation: \\(u^n\\) is the numerical approximation"
  },
  {
    "objectID": "intro.html#step-1-discretizing-the-domain-1",
    "href": "intro.html#step-1-discretizing-the-domain-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 1: Discretizing the domain",
    "text": "Step 1: Discretizing the domain\n\\(u^n\\) is a mesh function, defined at the mesh points \\(t_n\\), \\(n=0,\\ldots,N_t\\) only."
  },
  {
    "objectID": "intro.html#what-about-a-mesh-function-between-the-mesh-points",
    "href": "intro.html#what-about-a-mesh-function-between-the-mesh-points",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What about a mesh function between the mesh points?",
    "text": "What about a mesh function between the mesh points?\nCan extend the mesh function to yield values between mesh points by linear interpolation:\n\\[\n\\begin{equation}\nu(t) \\approx u^n + \\frac{u^{n+1}-u^n}{t_{n+1}-t_n}(t - t_n)\n\\end{equation}\n\\]"
  },
  {
    "objectID": "intro.html#step-2-fulfilling-the-equation-at-discrete-time-points",
    "href": "intro.html#step-2-fulfilling-the-equation-at-discrete-time-points",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 2: Fulfilling the equation at discrete time points",
    "text": "Step 2: Fulfilling the equation at discrete time points\n\nThe ODE holds for all \\(t\\in (0,T]\\) (infinite no of points)\nIdea: let the ODE be valid at the mesh points only (finite no of points)\n\n\\[\nu'(t_n) = -au(t_n),\\quad n=1,\\ldots,N_t\n\\]"
  },
  {
    "objectID": "intro.html#step-3-replacing-derivatives-by-finite-differences",
    "href": "intro.html#step-3-replacing-derivatives-by-finite-differences",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 3: Replacing derivatives by finite differences",
    "text": "Step 3: Replacing derivatives by finite differences\nNow it is time for the finite difference approximations of derivatives:\n\\[\nu'(t_n) \\approx \\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n}\n\\]"
  },
  {
    "objectID": "intro.html#step-3-replacing-derivatives-by-finite-differences-1",
    "href": "intro.html#step-3-replacing-derivatives-by-finite-differences-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 3: Replacing derivatives by finite differences",
    "text": "Step 3: Replacing derivatives by finite differences\nInserting the finite difference approximation in\n\\[\nu'(t_n) = -au(t_n)\n\\]\ngives\n\\[\n\\begin{equation}\n\\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -au^{n},\\quad n=0,1,\\ldots,N_t-1\n\\end{equation}\n\\]\n(Known as discrete equation, or discrete problem, or finite difference method/scheme)"
  },
  {
    "objectID": "intro.html#step-4-formulating-a-recursive-algorithm",
    "href": "intro.html#step-4-formulating-a-recursive-algorithm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 4: Formulating a recursive algorithm",
    "text": "Step 4: Formulating a recursive algorithm\nHow can we actually compute the \\(u^n\\) values?\n\ngiven \\(u^0=I\\)\ncompute \\(u^1\\) from \\(u^0\\)\ncompute \\(u^2\\) from \\(u^1\\)\ncompute \\(u^3\\) from \\(u^2\\) (and so forth)\n\nIn general: we have \\(u^n\\) and seek \\(u^{n+1}\\)\n\n\n\n\n\n\nThe Forward Euler scheme\n\n\nSolve wrt \\(u^{n+1}\\) to get the computational formula: \\[\nu^{n+1} = u^n - a(t_{n+1} -t_n)u^n\n\\]"
  },
  {
    "objectID": "intro.html#let-us-apply-the-scheme-by-hand",
    "href": "intro.html#let-us-apply-the-scheme-by-hand",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Let us apply the scheme by hand",
    "text": "Let us apply the scheme by hand\nAssume constant time spacing: \\(\\Delta t = t_{n+1}-t_n=\\mbox{const}\\) such that \\(u^{n+1} = u^n (1- a \\Delta t)\\)\n\\[\n\\begin{align*}\nu^0 &= I,\\\\\nu^1 & = I(1-a\\Delta t),\\\\\nu^2 & = I(1-a\\Delta t)^2,\\\\\n&\\vdots\\\\\nu^{N_t} &= I(1-a\\Delta t)^{N_t}\n\\end{align*}\n\\]\nOoops - we can find the numerical solution by hand (in this simple example)! No need for a computer (yet)…"
  },
  {
    "objectID": "intro.html#a-backward-difference",
    "href": "intro.html#a-backward-difference",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A backward difference",
    "text": "A backward difference\nHere is another finite difference approximation to the derivative (backward difference):\n\\[\nu'(t_n) \\approx \\frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}}\n\\]"
  },
  {
    "objectID": "intro.html#the-backward-euler-scheme",
    "href": "intro.html#the-backward-euler-scheme",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Backward Euler scheme",
    "text": "The Backward Euler scheme\nInserting the finite difference approximation in \\(u'(t_n)=-au(t_n)\\) yields the Backward Euler (BE) scheme:\n\\[\n\\frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}} = -a u^n\n\\]\nSolve with respect to the unknown \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1}{1+ a(t_{n+1}-t_n)} u^n\n\\]\n\n\n\n\n\n\nNote\n\n\nWe use \\(u^{n+1}\\) as unknown and rename \\(u^n \\longrightarrow u^{n+1}\\) and \\(u^{n-1} \\longrightarrow u^{n}\\)"
  },
  {
    "objectID": "intro.html#a-centered-difference",
    "href": "intro.html#a-centered-difference",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A centered difference",
    "text": "A centered difference\nCentered differences are better approximations than forward or backward differences."
  },
  {
    "objectID": "intro.html#the-crank-nicolson-scheme-ideas",
    "href": "intro.html#the-crank-nicolson-scheme-ideas",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Crank-Nicolson scheme; ideas",
    "text": "The Crank-Nicolson scheme; ideas\nIdea 1: let the ODE hold at \\(t_{n+\\scriptstyle\\frac{1}{2}}\\). With \\(N_t+1\\) points, that is \\(N_t\\) equations for \\(n=0, 1, \\ldots N_t-1\\)\n\\[\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) = -au(t_{n+\\scriptstyle\\frac{1}{2}})\n\\]\nIdea 2: approximate \\(u'(t_{n+\\scriptstyle\\frac{1}{2}})\\) by a centered difference\n\\[\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) \\approx \\frac{u^{n+1}-u^n}{t_{n+1}-t_n}\n\\]\nProblem: \\(u(t_{n+\\scriptstyle\\frac{1}{2}})\\) is not defined, only \\(u^n=u(t_n)\\) and \\(u^{n+1}=u(t_{n+1})\\)\n\nSolution (linear interpolation):\n\\[\nu(t_{n+\\scriptstyle\\frac{1}{2}}) \\approx \\frac{1}{2} (u^n + u^{n+1})\n\\]"
  },
  {
    "objectID": "intro.html#the-crank-nicolson-scheme-result",
    "href": "intro.html#the-crank-nicolson-scheme-result",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Crank-Nicolson scheme; result",
    "text": "The Crank-Nicolson scheme; result\nResult:\n\\[\n\\frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -a\\frac{1}{2} (u^n + u^{n+1})\n\\]\nSolve wrt to \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1-\\frac{1}{2} a(t_{n+1}-t_n)}{1 + \\frac{1}{2} a(t_{n+1}-t_n)}u^n\n\\] This is a Crank-Nicolson (CN) scheme or a midpoint or centered scheme."
  },
  {
    "objectID": "intro.html#the-unifying-theta-rule",
    "href": "intro.html#the-unifying-theta-rule",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The unifying \\(\\theta\\)-rule",
    "text": "The unifying \\(\\theta\\)-rule\nThe Forward Euler, Backward Euler, and Crank-Nicolson schemes can be formulated as one scheme with a varying parameter \\(\\theta\\):\n\\[\n\\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -a (\\theta u^{n+1} + (1-\\theta) u^{n})\n\\]\n\n\\(\\theta =0\\): Forward Euler\n\\(\\theta =1\\): Backward Euler\n\\(\\theta =1/2\\): Crank-Nicolson\nWe may alternatively choose any \\(\\theta\\in [0,1]\\).\n\n\\(u^n\\) is known, solve for \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a(t_{n+1}-t_n)}{1 + \\theta a(t_{n+1}-t_n)} u^n\n\\]"
  },
  {
    "objectID": "intro.html#constant-time-step",
    "href": "intro.html#constant-time-step",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Constant time step",
    "text": "Constant time step\nVery common assumption (not important, but exclusively used for simplicity hereafter): constant time step \\(t_{n+1}-t_n\\equiv\\Delta t\\)\nSummary of schemes for constant time step \\[\n\\begin{align}\nu^{n+1} &= (1 - a\\Delta t )u^n  \\quad (\\hbox{FE}) \\\\\nu^{n+1} &= \\frac{1}{1+ a\\Delta t} u^n  \\quad (\\hbox{BE}) \\\\\nu^{n+1} &= \\frac{1-\\frac{1}{2} a\\Delta t}{1 + \\frac{1}{2} a\\Delta t} u^n \\quad (\\hbox{CN})\\\\\nu^{n+1} &= \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n \\quad (\\theta-\\hbox{rule})\n\\end{align}\n\\]"
  },
  {
    "objectID": "intro.html#implementation-1",
    "href": "intro.html#implementation-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Implementation",
    "text": "Implementation\nModel:\n\\[\nu'(t) = -au(t),\\quad t\\in (0,T], \\quad u(0)=I\n\\]\nNumerical method:\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n\n\\]\nfor \\(\\theta\\in [0,1]\\). Note\n\n\\(\\theta=0\\) gives Forward Euler\n\\(\\theta=1\\) gives Backward Euler\n\\(\\theta=1/2\\) gives Crank-Nicolson"
  },
  {
    "objectID": "intro.html#requirements-of-a-program",
    "href": "intro.html#requirements-of-a-program",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Requirements of a program",
    "text": "Requirements of a program\n\nCompute the numerical solution \\(u^n\\), \\(n=1,2,\\ldots,N_t\\)\nDisplay the numerical and exact solution \\(u_{e}(t)=e^{-at}\\)\nBring evidence to a correct implementation (verification)\nCompare the numerical and the exact solution in a plot\nQuantify the error \\(u_{e}(t_n) - u^n\\) using norms\nCompute the convergence rate of the numerical scheme\n(Optimize for speed)"
  },
  {
    "objectID": "intro.html#algorithm",
    "href": "intro.html#algorithm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Algorithm",
    "text": "Algorithm\n\nStore \\(u^n\\), \\(n=0,1,\\ldots,N_t\\) in an array \\(\\boldsymbol{u}\\).\nAlgorithm:\n\ninitialize \\(u^0\\)\nfor \\(n=1, 2, \\ldots, N_t\\): compute \\(u^n\\) using the \\(\\theta\\)-rule formula"
  },
  {
    "objectID": "intro.html#in-python",
    "href": "intro.html#in-python",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "In Python",
    "text": "In Python\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    for n in range(0, Nt):    # n=0,1,...,Nt-1\n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u, t\n\nu, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)\n# Write out a table of t and u values:\nfor i in range(len(t)):\n    print(f't={t[i]:6.3f} u={u[i]:g}')"
  },
  {
    "objectID": "intro.html#in-python-1",
    "href": "intro.html#in-python-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "In Python",
    "text": "In Python\n\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    for n in range(0, Nt):    # n=0,1,...,Nt-1\n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u, t\n\nu, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)\n# Write out a table of t and u values:\nfor i in range(len(t)):\n    print(f't={t[i]:6.3f} u={u[i]:g}')\n\nt= 0.000 u=1\nt= 0.800 u=0.384615\nt= 1.600 u=0.147929\nt= 2.400 u=0.0568958\nt= 3.200 u=0.021883\nt= 4.000 u=0.00841653\nt= 4.800 u=0.00323713\nt= 5.600 u=0.00124505\nt= 6.400 u=0.000478865\nt= 7.200 u=0.000184179\nt= 8.000 u=7.0838e-05"
  },
  {
    "objectID": "intro.html#plot-the-solution",
    "href": "intro.html#plot-the-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plot the solution",
    "text": "Plot the solution\nWe will also learn about plotting. It is very important to present data in a clear and consise manner. It is very easy to generate a naked plot\n\nimport matplotlib.pyplot as plt \nI, a, T, dt, theta = 1, 2, 8, 0.8, 1\nu, t = solver(I, a, T, dt, theta)\nfig = plt.figure(figsize=(6, 4))\nax = fig.gca()\nax.plot(t, u)"
  },
  {
    "objectID": "intro.html#plot-the-solution-1",
    "href": "intro.html#plot-the-solution-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plot the solution",
    "text": "Plot the solution\nBut you should always add legends, titles, exact solution, etc. Make the plot nice:-)\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\nu, t = solver(I=I, a=a, T=T, dt=0.8, theta=1)\nte = np.linspace(0, T, 1000)\nue = u_exact(te, I, a)\nfig = plt.figure(figsize=(6, 4))\nplt.plot(t, u, 'bs-', te, ue, 'r')\nplt.title('Decay')\nplt.legend(['numerical', 'exact'])\nplt.xlabel('Time'), plt.ylabel('u(t)');"
  },
  {
    "objectID": "intro.html#plotly-is-a-very-good-alternative",
    "href": "intro.html#plotly-is-a-very-good-alternative",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plotly is a very good alternative",
    "text": "Plotly is a very good alternative\n\nimport plotly.express as px\npfig = px.line(x=t, y=u, labels={'x': 'Time', 'y': 'u(t)'}, \n               width=600, height=400, title='Decay',\n               template=\"simple_white\")\npfig.show()"
  },
  {
    "objectID": "intro.html#verifying-the-implementation",
    "href": "intro.html#verifying-the-implementation",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Verifying the implementation",
    "text": "Verifying the implementation\n\nVerification = bring evidence that the program works\nFind suitable test problems\nMake function for each test problem\nLater: put the verification tests in a professional testing framework\n\npytest\ngithub actions"
  },
  {
    "objectID": "intro.html#comparison-with-exact-numerical-solution",
    "href": "intro.html#comparison-with-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Comparison with exact numerical solution",
    "text": "Comparison with exact numerical solution\n\n\n\n\n\n\nWhat is exact?\n\n\nThere is a difference between exact numerical solution and exact solution!\n\n\n\nRepeated use of the \\(\\theta\\)-rule gives exact numerical solution: \\[\n\\begin{align*}\nu^0 &= I,\\\\\nu^1 &= Au^0 = AI\\\\\nu^n &= A^nu^{n-1} = A^nI\n\\end{align*}\n\\]\nExact solution on the other hand:\n\\[\nu(t) = \\exp(-a t), \\quad u(t_n) = \\exp(-a t_n)\n\\]"
  },
  {
    "objectID": "intro.html#making-a-test-based-on-an-exact-numerical-solution",
    "href": "intro.html#making-a-test-based-on-an-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Making a test based on an exact numerical solution",
    "text": "Making a test based on an exact numerical solution\nThe exact discrete solution is\n\\[\nu^n = IA^n\n\\]\nTest if your solver gives\n\\[\n\\max_n |u^n - IA^n| &lt; \\epsilon\\sim 10^{-15}\n\\]\nfor a few precalculated steps.\n\n\n\n\n\n\nTip\n\n\nMake sure you understand what \\(n\\) in \\(u^n\\) and in \\(A^n\\) means! \\(n\\) is not used as a power in \\(u^n\\), but it is a power in \\(A^n\\)!"
  },
  {
    "objectID": "intro.html#run-a-few-numerical-steps-by-hand",
    "href": "intro.html#run-a-few-numerical-steps-by-hand",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Run a few numerical steps by hand",
    "text": "Run a few numerical steps by hand\nUse a calculator (\\(I=0.1\\), \\(\\theta=0.8\\), \\(\\Delta t =0.8\\)):\n\\[\nA\\equiv \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a \\Delta t} = 0.298245614035\n\\]\n\\[\n\\begin{align*}\nu^1 &= AI=0.0298245614035,\\\\\nu^2 &= Au^1= 0.00889504462912,\\\\\nu^3 &=Au^2= 0.00265290804728\n\\end{align*}\n\\]"
  },
  {
    "objectID": "intro.html#the-test-based-on-exact-numerical-solution",
    "href": "intro.html#the-test-based-on-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The test based on exact numerical solution",
    "text": "The test based on exact numerical solution\n\ndef test_solver_three_steps(solver):\n    \"\"\"Compare three steps with known manual computations.\"\"\"\n    theta = 0.8\n    a = 2\n    I = 0.1\n    dt = 0.8\n    u_by_hand = np.array([I,\n                          0.0298245614035,\n                          0.00889504462912,\n                          0.00265290804728])\n\n    Nt = 3  # number of time steps\n    u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)\n    tol = 1E-14  # tolerance for comparing floats\n    diff = abs(u - u_by_hand).max()\n    success = diff &lt; tol\n    assert success, diff\n\ntest_solver_three_steps(solver)\n\n\n\n\n\n\n\nNote\n\n\nWe do not use the exact solution because the numerical solution will not equal the exact!"
  },
  {
    "objectID": "intro.html#quantifying-the-error",
    "href": "intro.html#quantifying-the-error",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Quantifying the error",
    "text": "Quantifying the error\nComputing the norm of the error\n\n\\(e^n = u^n - u_e(t_n)\\) is a mesh function\nUsually we want one number for the error\nUse a norm of \\(e^n\\)\n\nNorms of a function \\(f(t)\\):\n\\[\n\\begin{align}\n||f||_{L^2} &= \\left( \\int_0^T f(t)^2 dt\\right)^{1/2} \\\\\n||f||_{L^1} &= \\int_0^T |f(t)| dt \\\\\n||f||_{L^\\infty} &= \\max_{t\\in [0,T]}|f(t)|\n\\end{align}\n\\]"
  },
  {
    "objectID": "intro.html#norms-of-mesh-functions",
    "href": "intro.html#norms-of-mesh-functions",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Norms of mesh functions",
    "text": "Norms of mesh functions\n\nProblem: \\(f^n =f(t_n)\\) is a mesh function and hence not defined for all \\(t\\). How to integrate \\(f^n\\)?\nIdea: Apply a numerical integration rule, using only the mesh points of the mesh function.\n\nThe Trapezoidal rule:\n\\[\n||f^n|| = \\left(\\Delta t\\left(\\scriptstyle\\frac{1}{2}(f^0)^2 + \\scriptstyle\\frac{1}{2}(f^{N_t})^2\n+ \\sum_{n=1}^{N_t-1} (f^n)^2\\right)\\right)^{1/2}\n\\]\nCommon simplification yields the \\(\\ell^2\\) norm of a mesh function:\n\\[\n||f^n||_{\\ell^2} = \\left(\\Delta t\\sum_{n=0}^{N_t} (f^n)^2\\right)^{1/2}\n\\]"
  },
  {
    "objectID": "intro.html#norms---notice",
    "href": "intro.html#norms---notice",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Norms - notice!",
    "text": "Norms - notice!\n\nThe continuous norms use capital \\(L^2, L^1, L^\\infty{}\\)\nThe discrete norm uses lowercase \\(\\ell^2, \\ell^1, \\ell^{\\infty}\\)"
  },
  {
    "objectID": "intro.html#implementation-of-the-error-norm",
    "href": "intro.html#implementation-of-the-error-norm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Implementation of the error norm",
    "text": "Implementation of the error norm\n\\[\nE = ||e^n||_{\\ell^2}  = \\sqrt{\\Delta t\\sum_{n=0}^{N_t} (e^n)^2}\n\\]\nPython code for the norm:\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\nI, a, T, dt, theta = 1., 2., 8., 0.2, 1\nu, t = solver(I, a, T, dt, theta)\nen = u_exact(t, I, a) - u\nE = np.sqrt(dt*np.sum(en**2))\nprint(f'Errornorm = {E}')\n\nErrornorm = 0.0637716295205199"
  },
  {
    "objectID": "intro.html#how-about-computational-speed",
    "href": "intro.html#how-about-computational-speed",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\n\nFor example, you have three arrays\n\\[\n\\boldsymbol{u} = (u_i)_{i=0}^N, \\boldsymbol{v} = (v_i)_{i=0}^N, \\boldsymbol{w} = (w_i)_{i=0}^N\n\\]\nNow compute\n\\[\nw_i = u_i \\cdot v_i, \\quad \\forall \\, i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "intro.html#how-about-computational-speed-1",
    "href": "intro.html#how-about-computational-speed-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\nRegular (scalar) implementation:\n\nN = 1000\nu = np.random.random(N)\nv = np.random.random(N)\nw = np.zeros(N)\n\nfor i in range(N):\n    w[i] = u[i] * v[i]\n\nVectorized:\n\nw[:] = u * v\n\nNumpy is heavily vectorized! So much so that mult, add, div, etc are vectorized by default!"
  },
  {
    "objectID": "intro.html#how-about-computational-speed-2",
    "href": "intro.html#how-about-computational-speed-2",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\n\n\n\n\n\n\nVectorization warning\n\n\nPretty much all the code you will see and get access to in this course will be vectorized!"
  },
  {
    "objectID": "intro.html#vectorizing-the-decay-solver",
    "href": "intro.html#vectorizing-the-decay-solver",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Vectorizing the decay solver",
    "text": "Vectorizing the decay solver\nGet rid of the for-loop!\nu[0] = I                  # assign initial condition\nfor n in range(0, Nt):    # n=0,1,...,Nt-1\n    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nHow? Difficult because it is a recursive update and not regular elementwise multiplication. But remember\n\\[\nA = (1 - (1- \\theta) a  \\Delta t)/(1 + \\theta \\Delta t a)\n\\]\n\\[\n\\begin{align*}\nu^1 & = A u^0,\\\\\nu^2 & = A u^1,\\\\\n&\\vdots\\\\\nu^{N_t} &= A u^{N_t-1}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "intro.html#vectorized-code",
    "href": "intro.html#vectorized-code",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Vectorized code",
    "text": "Vectorized code\nu[0] = I                  # assign initial condition\nfor n in range(0, Nt):    # n=0,1,...,Nt-1\n    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\nCan be implemented as\n\nu[0] = I                  # assign initial condition\nu[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\nu[:] = np.cumprod(u)     \n\n\nbecause\n\\[\nu^n = A^n u^0, \\quad \\text{since }\n\\begin{cases}\nu^1 & = A u^0,\\\\\nu^2 & = A u^1 = A^2 u^0,\\\\\n&\\vdots\\\\\nu^{N_t} &= A u^{N_t-1} = A^{N_t} u^0\n\\end{cases}\n\\]\n\nnp.cumprod([1, 2, 2, 2])\n\narray([1, 2, 4, 8])"
  },
  {
    "objectID": "intro.html#why-vectorization",
    "href": "intro.html#why-vectorization",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Why vectorization?",
    "text": "Why vectorization?\n\nPython for-loops are slow!\nPython for-loops usually requires more lines of code.\n\n\ndef f0(u, I, theta, a, dt):\n    u[0] = I                  \n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n    return u\n\ndef f1(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u\n\nI, a, T, dt, theta = 1, 2, 8, 0.8, 1\nu, t = solver(I, a, T, dt, theta)\n\nassert np.allclose(f0(u.copy(), I, theta, a, dt), \n                   f1(u.copy(), I, theta, a, dt))\n\n\nLets try some timings!"
  },
  {
    "objectID": "intro.html#why-vectorization-timings",
    "href": "intro.html#why-vectorization-timings",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Why vectorization? Timings",
    "text": "Why vectorization? Timings\n\ndef f0(u, I, theta, a, dt):\n    u[0] = I                  \n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n\ndef f1(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nLets try some timings:\n\n%timeit -q -o -n 1000 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.32 µs ± 94 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)&gt;\n\n\n\n%timeit -q -o -n 1000 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.26 µs ± 68.3 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)&gt;\n\n\n\nHmm. Not really what’s expected. Why? Because the array u is really short! Lets try a longer array\n\nprint(f\"Length of u = {u.shape[0]}\") \n\nLength of u = 11"
  },
  {
    "objectID": "intro.html#longer-array-timings",
    "href": "intro.html#longer-array-timings",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Longer array timings",
    "text": "Longer array timings\n\ndt = dt/10\nu, t = solver(I, a, T, dt, theta) \nprint(f\"Length of u = {u.shape[0]}\")\n\nLength of u = 101\n\n\n\n%timeit -q -o -n 100 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.86 µs ± 1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n%timeit -q -o -n 100 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 20.6 µs ± 517 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\nEven longer array:\n\ndt = dt/10\nu, t = solver(I, a, T, dt, theta) \nprint(f\"Length of u = {u.shape[0]}\")\n\nLength of u = 1001\n\n\n\n%timeit -q -o -n 100 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 3.86 µs ± 2.34 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n%timeit -q -o -n 100 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 220 µs ± 2.68 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\nVectorized code takes the same time! Only overhead costs, not the actual computation."
  },
  {
    "objectID": "intro.html#what-else-is-there-numba",
    "href": "intro.html#what-else-is-there-numba",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What else is there? Numba",
    "text": "What else is there? Numba\n\nimport numba as nb\n@nb.jit\ndef f2(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nTime it once\n\n%timeit -q -o -n 100 f2(u, I, theta, a, dt)\n\n&lt;TimeitResult : 48.1 µs ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\nSlow because the code needs to be compiled. Try again\n\n%timeit -q -o -n 100 f2(u, I, theta, a, dt)\n\n&lt;TimeitResult : 1.29 µs ± 8.08 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\nThat is even faster than the vectorized code!"
  },
  {
    "objectID": "intro.html#what-else-cython",
    "href": "intro.html#what-else-cython",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What else? Cython",
    "text": "What else? Cython\n\n%load_ext cython\n\nThe cython extension is already loaded. To reload it, use:\n  %reload_ext cython\n\n\n\n%%cython -a\n#cython: boundscheck=False, wraparound=False, cdivision=True\ncpdef void f3(double[::1] u, int  I, double theta, double a, double dt):\n    cdef int n\n    cdef int N = u.shape[0]\n    u[0] = I                 \n    for n in range(0, N-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return\n\n\n\n\n\n    \n    Cython: _cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.pyx\n    \n\n\nGenerated by Cython 3.0.10\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+1: #cython: boundscheck=False, wraparound=False, cdivision=True\n  __pyx_t_7 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_7);\n  if (PyDict_SetItem(__pyx_d, __pyx_n_s_test, __pyx_t_7) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;\n+2: cpdef void f3(double[::1] u, int  I, double theta, double a, double dt):\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic void __pyx_f_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__Pyx_memviewslice __pyx_v_u, int __pyx_v_I, double __pyx_v_theta, double __pyx_v_a, double __pyx_v_dt, CYTHON_UNUSED int __pyx_skip_dispatch) {\n  int __pyx_v_n;\n  int __pyx_v_N;\n/* … */\n  /* function exit code */\n  __pyx_L0:;\n}\n\n/* Python wrapper */\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3 = {\"f3\", (PyCFunction)(void*)(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  __Pyx_memviewslice __pyx_v_u = { 0, 0, { 0 }, { 0 }, { 0 } };\n  int __pyx_v_I;\n  double __pyx_v_theta;\n  double __pyx_v_a;\n  double __pyx_v_dt;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"f3 (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_MACROS\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject **__pyx_pyargnames[] = {&__pyx_n_s_u,&__pyx_n_s_I,&__pyx_n_s_theta,&__pyx_n_s_a,&__pyx_n_s_dt,0};\n  PyObject* values[5] = {0,0,0,0,0};\n    if (__pyx_kwds) {\n      Py_ssize_t kw_args;\n      switch (__pyx_nargs) {\n        case  5: values[4] = __Pyx_Arg_FASTCALL(__pyx_args, 4);\n        CYTHON_FALLTHROUGH;\n        case  4: values[3] = __Pyx_Arg_FASTCALL(__pyx_args, 3);\n        CYTHON_FALLTHROUGH;\n        case  3: values[2] = __Pyx_Arg_FASTCALL(__pyx_args, 2);\n        CYTHON_FALLTHROUGH;\n        case  2: values[1] = __Pyx_Arg_FASTCALL(__pyx_args, 1);\n        CYTHON_FALLTHROUGH;\n        case  1: values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      kw_args = __Pyx_NumKwargs_FASTCALL(__pyx_kwds);\n      switch (__pyx_nargs) {\n        case  0:\n        if (likely((values[0] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_u)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[0]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else goto __pyx_L5_argtuple_error;\n        CYTHON_FALLTHROUGH;\n        case  1:\n        if (likely((values[1] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_I)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[1]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 1); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  2:\n        if (likely((values[2] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_theta)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[2]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 2); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  3:\n        if (likely((values[3] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_a)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[3]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 3); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  4:\n        if (likely((values[4] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_dt)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[4]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 4); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n      }\n      if (unlikely(kw_args &gt; 0)) {\n        const Py_ssize_t kwd_pos_args = __pyx_nargs;\n        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, \"f3\") &lt; 0)) __PYX_ERR(0, 2, __pyx_L3_error)\n      }\n    } else if (unlikely(__pyx_nargs != 5)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);\n      values[1] = __Pyx_Arg_FASTCALL(__pyx_args, 1);\n      values[2] = __Pyx_Arg_FASTCALL(__pyx_args, 2);\n      values[3] = __Pyx_Arg_FASTCALL(__pyx_args, 3);\n      values[4] = __Pyx_Arg_FASTCALL(__pyx_args, 4);\n    }\n    __pyx_v_u = __Pyx_PyObject_to_MemoryviewSlice_dc_double(values[0], PyBUF_WRITABLE); if (unlikely(!__pyx_v_u.memview)) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_I = __Pyx_PyInt_As_int(values[1]); if (unlikely((__pyx_v_I == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_theta = __pyx_PyFloat_AsDouble(values[2]); if (unlikely((__pyx_v_theta == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_a = __pyx_PyFloat_AsDouble(values[3]); if (unlikely((__pyx_v_a == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_dt = __pyx_PyFloat_AsDouble(values[4]); if (unlikely((__pyx_v_dt == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, __pyx_nargs); __PYX_ERR(0, 2, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  {\n    Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n      __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);\n    }\n  }\n  __PYX_XCLEAR_MEMVIEW(&__pyx_v_u, 1);\n  __Pyx_AddTraceback(\"_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.f3\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__pyx_self, __pyx_v_u, __pyx_v_I, __pyx_v_theta, __pyx_v_a, __pyx_v_dt);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  __PYX_XCLEAR_MEMVIEW(&__pyx_v_u, 1);\n  {\n    Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n      __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);\n    }\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(CYTHON_UNUSED PyObject *__pyx_self, __Pyx_memviewslice __pyx_v_u, int __pyx_v_I, double __pyx_v_theta, double __pyx_v_a, double __pyx_v_dt) {\n  PyObject *__pyx_r = NULL;\n  __Pyx_XDECREF(__pyx_r);\n  if (unlikely(!__pyx_v_u.memview)) { __Pyx_RaiseUnboundLocalError(\"u\"); __PYX_ERR(0, 2, __pyx_L1_error) }\n  __pyx_f_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__pyx_v_u, __pyx_v_I, __pyx_v_theta, __pyx_v_a, __pyx_v_dt, 0); if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L1_error)\n  __pyx_t_1 = __Pyx_void_to_None(NULL); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_1);\n  __pyx_r = __pyx_t_1;\n  __pyx_t_1 = 0;\n  goto __pyx_L0;\n\n  /* function exit code */\n  __pyx_L1_error:;\n  __Pyx_XDECREF(__pyx_t_1);\n  __Pyx_AddTraceback(\"_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.f3\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __pyx_r = NULL;\n  __pyx_L0:;\n  __Pyx_XGIVEREF(__pyx_r);\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n/* … */\n  __pyx_tuple__20 = PyTuple_Pack(5, __pyx_n_s_u, __pyx_n_s_I, __pyx_n_s_theta, __pyx_n_s_a, __pyx_n_s_dt); if (unlikely(!__pyx_tuple__20)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_tuple__20);\n  __Pyx_GIVEREF(__pyx_tuple__20);\n/* … */\n  __pyx_t_7 = __Pyx_CyFunction_New(&__pyx_mdef_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3, 0, __pyx_n_s_f3, NULL, __pyx_n_s_cython_magic_56487b4d6695ae1d6d, __pyx_d, ((PyObject *)__pyx_codeobj__21)); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_7);\n  if (PyDict_SetItem(__pyx_d, __pyx_n_s_f3, __pyx_t_7) &lt; 0) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;\n 3:     cdef int n\n+4:     cdef int N = u.shape[0]\n  __pyx_v_N = (__pyx_v_u.shape[0]);\n+5:     u[0] = I\n  __pyx_t_1 = 0;\n  *((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_1)) )) = __pyx_v_I;\n+6:     for n in range(0, N-1):\n  __pyx_t_2 = (__pyx_v_N - 1);\n  __pyx_t_3 = __pyx_t_2;\n  for (__pyx_t_4 = 0; __pyx_t_4 &lt; __pyx_t_3; __pyx_t_4+=1) {\n    __pyx_v_n = __pyx_t_4;\n+7:         u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    __pyx_t_1 = __pyx_v_n;\n    __pyx_t_5 = (__pyx_v_n + 1);\n    *((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_5)) )) = (((1.0 - (((1.0 - __pyx_v_theta) * __pyx_v_a) * __pyx_v_dt)) / (1.0 + ((__pyx_v_theta * __pyx_v_dt) * __pyx_v_a))) * (*((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_1)) ))));\n  }\n+8:     return\n  goto __pyx_L0;"
  },
  {
    "objectID": "intro.html#cython-timing",
    "href": "intro.html#cython-timing",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Cython timing",
    "text": "Cython timing\n\n%timeit -q -o -n 100 f3(u, I, theta, a, dt)\n\n&lt;TimeitResult : 1.28 µs ± 21.6 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n\n\n\n\n\nCython and Numba are both fast!\n\n\nCython and Numba are both as fast as pure C. Either one can be used to speed up critical routines with very little additional effort!\n\n\n\n\n\n\n\n\n\nNote\n\n\nCython is very easy to use in notebooks, but requires some additional steps to be compiled used as extension modules with regular python programs."
  },
  {
    "objectID": "finitedifferences.html#initial-or-boundary-value-problems",
    "href": "finitedifferences.html#initial-or-boundary-value-problems",
    "title": "The finite difference method",
    "section": "Initial or Boundary Value Problems",
    "text": "Initial or Boundary Value Problems\nThe second-order differential equation\n\\[\nu''(t) + au'(t) + bu(t) = f(t), \\quad t \\in [0, T]\n\\]\nis classified as either an IVP or a BVP based on boundary/initial conditions.\n\n\n\nIVP\n\nTypically specifies \\(u(0)\\) and \\(u'(0)\\)\nExplicit recurrence relations, no use of linear algebra\nCan use the finite difference or similar methods for discretization\n\n\nBVP\n\nTypically specifies \\(u(0)\\) and \\(u(T)\\) or \\(u'(0)\\) and \\(u'(T)\\)\nImplicit linear algebra methods\nCan use the finite difference or similar methods for discretization\n\n\n\nSimilar characteristics for higher-order differential equations. IVP specifies all conditions at initial time."
  },
  {
    "objectID": "finitedifferences.html#the-explicit-ivp-approach-simply-computes-un1-from-un-un-1",
    "href": "finitedifferences.html#the-explicit-ivp-approach-simply-computes-un1-from-un-un-1",
    "title": "The finite difference method",
    "section": "The explicit IVP approach simply computes \\(u^{n+1}\\) from \\(u^n, u^{n-1}\\)…",
    "text": "The explicit IVP approach simply computes \\(u^{n+1}\\) from \\(u^n, u^{n-1}\\)…\n\n\n\nFor example, the exponential decay problem with initial condition\n\\[\nu' + au = 0, t \\in (0, T], \\, u(0)=I.\n\\]\nis discretized as\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a \\Delta t}{1 + \\theta a \\Delta t} u^n = g u^n.\n\\]\nThe solution vector \\(\\boldsymbol{u} = (u^0, u^1, \\ldots, u^{N})\\) is obtained from a recurrence algorithm\n\n\\(u^0 = I\\)\nfor n = 0, 1, … , N-1\n\nCompute \\(u^{n+1} = g u^n\\)\n\n\nEasy to understand and easy to solve. Equations are never assembled into matrix form. But may be unstable!"
  },
  {
    "objectID": "finitedifferences.html#the-bvp-approach-solves-the-difference-equations-by-assembling-matrices-and-vectors",
    "href": "finitedifferences.html#the-bvp-approach-solves-the-difference-equations-by-assembling-matrices-and-vectors",
    "title": "The finite difference method",
    "section": "The BVP approach solves the difference equations by assembling matrices and vectors",
    "text": "The BVP approach solves the difference equations by assembling matrices and vectors\nEach row in the matrix-problem represents one difference equation.\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}\n\\]\nFor the exponential decay problem the matrix problem looks like\n\\[\n\\underbrace{\n\\begin{bmatrix}\n  1  & 0 & 0 & 0 & 0  \\\\\n-g  & 1 & 0 & 0 & 0  \\\\\n0  & -g & 1 & 0 & 0  \\\\\n0  & 0 & -g & 1 & 0  \\\\\n0  & 0 & 0 & -g & 1  \n\\end{bmatrix}}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4\n\\end{bmatrix}}_{\\boldsymbol{u}} =\n\\underbrace{\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#understand-that-the-matrix-problem-is-the-same-as-the-recurrence",
    "href": "finitedifferences.html#understand-that-the-matrix-problem-is-the-same-as-the-recurrence",
    "title": "The finite difference method",
    "section": "Understand that the matrix problem is the same as the recurrence",
    "text": "Understand that the matrix problem is the same as the recurrence\n\\[\n\\underbrace{\n\\begin{bmatrix}\n1   & 0 & 0 & 0 & 0   \\\\\n-g  & 1 & 0 & 0 & 0   \\\\\n0  & -g & 1 & 0 & 0   \\\\\n0  & 0 & -g & 1 & 0   \\\\\n0  & 0 & 0 & -g & 1   \n\\end{bmatrix}}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4\n\\end{bmatrix}}_{\\boldsymbol{u}} =\n\\underbrace{\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]\n\n\nsimply means\n\n\\[\n\\begin{align}\nu^0 &= I \\\\\n-g u^0 + u^1 &= 0 \\\\\n-g u^1 + u^2 &= 0 \\\\\n\\vdots\n\\end{align}\n\\]\n\n\nStill explicit since matrix is lower triangular. But problem is assembled into matrix form."
  },
  {
    "objectID": "finitedifferences.html#solve-matrix-problem",
    "href": "finitedifferences.html#solve-matrix-problem",
    "title": "The finite difference method",
    "section": "Solve matrix problem",
    "text": "Solve matrix problem\nThe matrix problem\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}\n\\]\nis solved as\n\\[\n\\boldsymbol{u} = A^{-1} \\boldsymbol{b}\n\\]\nThere are many different ways to achieve this. We will not be very concerned with how in this course."
  },
  {
    "objectID": "finitedifferences.html#assemble-matrix-problem",
    "href": "finitedifferences.html#assemble-matrix-problem",
    "title": "The finite difference method",
    "section": "Assemble matrix problem",
    "text": "Assemble matrix problem\n\nN = 8\na = 2\nI = 1\ntheta = 0.5\ndt = 0.5\nT = N*dt\nt = np.linspace(0, N*dt, N+1)\nu = np.zeros(N+1)\ng = (1 - (1-theta) * a * dt)/(1 + theta * a * dt)\n\nAssemble\n\nfrom scipy import sparse\nA = sparse.diags([-g, 1], np.array([-1, 0]), (N+1, N+1), 'csr')\nb = np.zeros(N+1); b[0] = I"
  },
  {
    "objectID": "finitedifferences.html#solve-and-compare-with-recurrence",
    "href": "finitedifferences.html#solve-and-compare-with-recurrence",
    "title": "The finite difference method",
    "section": "Solve and compare with recurrence",
    "text": "Solve and compare with recurrence\nRecurrence:\n\nu[0] = I\nfor n in range(N):\n  u[n+1] = g * u[n] \n\nMatrix\n\num = sparse.linalg.spsolve(A, b)\nprint(u-um)\n\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\nOk, no difference. There’s really no advantage of assembling the explicit exponential decay problem.\nBut which is faster?\n\n\ndef reccsolve(u, N, I):\n  u[0] = I\n  for n in range(N):\n    u[n+1] = g * u[n] \nu = np.zeros(N+1)\n%timeit -n100 reccsolve(u, N, I) \n\n1.41 µs ± 812 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit -n100 um = sparse.linalg.spsolve(A, b) \n\n18.9 µs ± 432 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "finitedifferences.html#try-to-speed-up-by-computing-the-inverse",
    "href": "finitedifferences.html#try-to-speed-up-by-computing-the-inverse",
    "title": "The finite difference method",
    "section": "Try to speed up by computing the inverse",
    "text": "Try to speed up by computing the inverse\nA unit lower triangular matrix has a unit lower triangular inverse\n\nnp.set_printoptions(precision=3, suppress=True) \nAi = np.linalg.inv(A.toarray())\nprint(Ai)\n\n[[1.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n [0.333 1.    0.    0.    0.    0.    0.    0.    0.   ]\n [0.111 0.333 1.    0.    0.    0.    0.    0.    0.   ]\n [0.037 0.111 0.333 1.    0.    0.    0.    0.    0.   ]\n [0.012 0.037 0.111 0.333 1.    0.    0.    0.    0.   ]\n [0.004 0.012 0.037 0.111 0.333 1.    0.    0.    0.   ]\n [0.001 0.004 0.012 0.037 0.111 0.333 1.    0.    0.   ]\n [0.    0.001 0.004 0.012 0.037 0.111 0.333 1.    0.   ]\n [0.    0.    0.001 0.004 0.012 0.037 0.111 0.333 1.   ]]\n\n\n\nNow solve directly using \\(A^{-1}\\)\n\nui = Ai @ b\n\nCompare with previous - still the same\n\num = sparse.linalg.spsolve(A, b)\nprint(um-ui) \n\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\n\n\nbut it seems to be very fast\n\n%timeit -n 100 ui = Ai @ b \n\n542 ns ± 43 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "finitedifferences.html#until-we-try-numba-on-the-explicit-solver",
    "href": "finitedifferences.html#until-we-try-numba-on-the-explicit-solver",
    "title": "The finite difference method",
    "section": "Until we try Numba on the explicit solver",
    "text": "Until we try Numba on the explicit solver\n\nimport numba as nb \n\n@nb.jit\ndef nbreccsolve(u, N, I, g):\n  u[0] = I\n  for n in range(N):\n    u[n+1] = g * u[n]\nnbreccsolve(u, N, I, g)\n%timeit -n 100 nbreccsolve(u, N, I, g)\n\n139 ns ± 10.1 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nBottom line - explicit marching methods are very fast! Unfortunately, they may be unstable and cannot be used for BVPs."
  },
  {
    "objectID": "finitedifferences.html#the-bv-vibration-problem",
    "href": "finitedifferences.html#the-bv-vibration-problem",
    "title": "The finite difference method",
    "section": "The BV vibration problem",
    "text": "The BV vibration problem\n\\[\nu'' + \\omega^2 u = 0,\\, t \\in (0, T) \\quad u(0) = I, u(T) = I,\n\\]\ncannot be solved with recurrence, since it is not an initial value problem.\n\nHowever, we can solve this problem using a central finite difference for all internal points \\(n=1, 2, \\ldots, N-1\\)\n\\[\n\\frac{u^{n+1}-2u^n+u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0.\n\\]\nThis leads to an implicit linear algebra problem, because the equation for \\(u^n\\) (above) depends on \\(u^{n+1}\\)!"
  },
  {
    "objectID": "finitedifferences.html#vibration-on-matrix-form",
    "href": "finitedifferences.html#vibration-on-matrix-form",
    "title": "The finite difference method",
    "section": "Vibration on matrix form",
    "text": "Vibration on matrix form\nThe matrix problem is now, using \\(g = 2-\\omega^2 \\Delta t^2\\),\n\\[\n\\begin{bmatrix}\n  1  & 0 & 0 & 0 & 0 & 0 & 0  \\\\\n1  & -g & 1 & 0 & 0 & 0 & 0  \\\\\n0  & 1 & -g & 1 & 0 & 0 & 0  \\\\\n0  & 0 & 1 & -g & 1 & 0 & 0  \\\\\n0  & 0 & 0 & 1 & -g & 1 & 0  \\\\\n0  & 0 & 0 & 0 & 1 & -g & 1  \\\\\n0  & 0 & 0 & 0 & 0 & 0 & 1  \n\\end{bmatrix}\n\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4 \\\\\nu^5 \\\\\nu^6\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\nI\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe matrix contains items both over and under the main diagonal, which characterizes an implicit method. Explicit marching method leads to lower triangular matrices.\nThe first and last rows are modified in order to apply boundary conditions implicitly. More about that later."
  },
  {
    "objectID": "finitedifferences.html#assemble-and-solve-the-implicit-bvp",
    "href": "finitedifferences.html#assemble-and-solve-the-implicit-bvp",
    "title": "The finite difference method",
    "section": "Assemble and solve the implicit BVP",
    "text": "Assemble and solve the implicit BVP\n\nT, N, I, w = 3., 35, 1., 2*np.pi\ndt = T/N \ng = 2 - w**2*dt**2\nA = sparse.diags([1, -g, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nb = np.zeros(N+1)\nA[0, :3] = 1, 0, 0    # Fix first row\nA[-1, -3:] = 0, 0, 1  # Fix last row\nb[0], b[-1] = I, I\nu2 = sparse.linalg.spsolve(A.tocsr(), b)\nt = np.linspace(0, T, N+1)\nplt.plot(t, u2, 'bo', t, I*np.cos(w*t), 'r--')\nplt.legend(['Numerical', 'Exact']);"
  },
  {
    "objectID": "finitedifferences.html#taylor-expansions-can-be-used-to-design-finite-difference-methods-of-any-order",
    "href": "finitedifferences.html#taylor-expansions-can-be-used-to-design-finite-difference-methods-of-any-order",
    "title": "The finite difference method",
    "section": "Taylor expansions can be used to design finite difference methods of any order",
    "text": "Taylor expansions can be used to design finite difference methods of any order\nFor example, we can create two backward and two forward Taylor expansions starting from \\(u^n=u(t_n)\\)\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nRemember: \\(u^{n+a} = u(t_{n+a})\\) and \\(t_{n+a} = (n+a)h\\) and we use \\(h=\\Delta t\\) for simplicity.\n\nAdd equations (-1) and (1) and isolate \\(u''(t_n)\\)\n\\[\n\\begin{equation*}\nu''(t_n) = \\frac{u^{n+1}-2u^n + u^{n-1}}{h^2}  + \\frac{h^2}{12}u'''' +\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-fd-stencil-can-be-applied-to-the-entire-mesh",
    "href": "finitedifferences.html#the-fd-stencil-can-be-applied-to-the-entire-mesh",
    "title": "The finite difference method",
    "section": "The FD stencil can be applied to the entire mesh",
    "text": "The FD stencil can be applied to the entire mesh\nThat is, we can compute \\(\\boldsymbol{u}^{(2)}=(u''(t_n))_{n=0}^{N}\\) from the mesh function \\(\\boldsymbol{u} = (u(t_n))_{n=0}^N \\in \\mathbb{R}^{N+1}\\) as\n\\[\n\\boldsymbol{u}^{(2)} = D^{(2)} \\boldsymbol{u},\n\\]\nwhere \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\) is the second derivative matrix.\n\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{(2)}_0 \\\\\nu^{(2)}_1 \\\\\nu^{(2)}_2 \\\\\n\\vdots \\\\\nu^{(2)}_{N-2} \\\\\nu^{(2)}_{N-1} \\\\\nu^{(2)}_{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{u}^{(2)}} =\n\\underbrace{\n  \\frac{1}{h^2} \\begin{bmatrix}\n? & ? & ? & ?  & ? & ? & ? & ?  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n? & ? & ? & ?  & ? & ? & ? & ? \\\\\n\\end{bmatrix}\n}_{D^{(2)}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-1} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#how-about-the-boundary-nodes",
    "href": "finitedifferences.html#how-about-the-boundary-nodes",
    "title": "The finite difference method",
    "section": "How about the boundary nodes?",
    "text": "How about the boundary nodes?\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{(2)}_0 \\\\\nu^{(2)}_1 \\\\\nu^{(2)}_2 \\\\\n\\vdots \\\\\nu^{(2)}_{N-2} \\\\\nu^{(2)}_{N-1} \\\\\nu^{(2)}_{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{u}^{(2)}} =\n\\underbrace{\n  \\frac{1}{h^2} \\begin{bmatrix}\n? & ? & ? & ?  & ? & ? & ? & ?  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n? & ? & ? & ?  & ? & ? & ? & ? \\\\\n\\end{bmatrix}\n}_{D^{(2)}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-1} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n\\]\nWhat to do with the first and last rows? The central stencils do not work here.\nWe still want to compute \\(u_0^{(2)}=u''(t_0)\\) and \\(u_N^{(2)}=u''(t_N)\\). How?\n\nForward and backward stencils will do"
  },
  {
    "objectID": "finitedifferences.html#we-can-do-a-forward-difference-at-n0",
    "href": "finitedifferences.html#we-can-do-a-forward-difference-at-n0",
    "title": "The finite difference method",
    "section": "We can do a forward difference at \\(n=0\\)",
    "text": "We can do a forward difference at \\(n=0\\)\nRemember: \\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nSubtract 2 times Eq. (1) from Eq. (2) and rearrange\n\\[\\small\n(2)-2(1): \\, u^{n+2} - 2u^{n+1} = -u^n +  \\frac{h^2}{1}u'' + h^3 u''' + \\frac{7 h^4}{12}u'''' +\n\\]\nRearrange to isolate a  first order  accurate stencil for \\(u''(0)\\)\n\\[\\small\nu''(0) = \\frac{u^{2}-2u^{1}+u^0}{h^2} - \\color{red} h \\color{black} u'''(0) - \\frac{7 h^2}{12}u''''(0) +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#we-can-do-a-backward-difference-at-nn",
    "href": "finitedifferences.html#we-can-do-a-backward-difference-at-nn",
    "title": "The finite difference method",
    "section": "We can do a backward difference at \\(n=N\\)",
    "text": "We can do a backward difference at \\(n=N\\)\nUse two backward Taylor expansions:\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2 h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n\\end{align*}\n\\]\nSubtract 2 times Eq. (-1) from Eq. (-2) and rearrange\n\\[\\small\n(-2)-2(-1): \\, u^{n-2} - 2u^{n-1} = -u^n +  \\frac{h^2}{1}u'' - h^3 u''' + \\frac{7 h^4}{12}u'''' +\n\\]\nRearrange to isolate a  first order  accurate backward stencil for \\(u''(T)\\)\n\\[\\small\nu''(T) = \\frac{u^{N-2}-2u^{N-1}+u^N}{h^2} - \\color{red} h \\color{black} u'''(T) - \\frac{7 h^2}{12}u''''(T) +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#we-get-a-second-derivative-matrix",
    "href": "finitedifferences.html#we-get-a-second-derivative-matrix",
    "title": "The finite difference method",
    "section": "We get a second derivative matrix",
    "text": "We get a second derivative matrix\n\\[\nD^{(2)} = \\frac{1}{h^2} \\begin{bmatrix}\n1 & -2 & 1 & 0  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -2 & 1 \\\\\n\\end{bmatrix}\n\\]\nbut with merely first order accuracy at the boundaries. Can we do better?\n\nYes! Of course, just use more points near the boundaries!"
  },
  {
    "objectID": "finitedifferences.html#use-three-forward-points-small-un1-un2-un3-to-get-a-second-order-forward-stencil",
    "href": "finitedifferences.html#use-three-forward-points-small-un1-un2-un3-to-get-a-second-order-forward-stencil",
    "title": "The finite difference method",
    "section": "Use three forward points \\(\\small u^{n+1}, u^{n+2}, u^{n+3}\\) to get a second order forward stencil",
    "text": "Use three forward points \\(\\small u^{n+1}, u^{n+2}, u^{n+3}\\) to get a second order forward stencil\n\\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots \\\\\n(3)\\quad u^{n+3} &= u^n + 3h u' + \\frac{9 h^2}{2}u'' + \\frac{9 h^3}{2}u''' + \\frac{27 h^4}{8}u'''' + \\cdots \\\\\n\\end{align*}\n\\]\nNow to eliminate both \\(u'\\) and \\(u'''\\) terms add the three equations as \\(-(3) + 4\\cdot(2) - 5\\cdot(1)\\) (don’t worry about how I know this yet)\n\\[\n-(3)+4\\cdot(2)-5\\cdot(1): \\, -u^{n+3}+4u^{n+2}-5u^{n+1} = -2 u^n + h^2 u'' - \\frac{11 h^4}{12}u'''' +  \n\\]\nIsolate \\(u''(0)\\)\n\\[\nu''(0) = \\frac{-u^{3} + 4u^{2} - 5u^{1} + 2u^0}{h^2} + \\frac{11 \\color{red} h^2 \\color{black}}{12} u'''' +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#fully-second-order-d2",
    "href": "finitedifferences.html#fully-second-order-d2",
    "title": "The finite difference method",
    "section": "Fully second order \\(D^{(2)}\\)",
    "text": "Fully second order \\(D^{(2)}\\)\n\\[\nD^{(2)} = \\frac{1}{h^2} \\begin{bmatrix}\n2 & -5 & 4 & -1  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & -1 & 4 & -5 & 2 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#assemble-in-python",
    "href": "finitedifferences.html#assemble-in-python",
    "title": "The finite difference method",
    "section": "Assemble in Python",
    "text": "Assemble in Python\n\nN = 8\ndt = 0.5\nT = N*dt\nD2 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD2[0, :4] = 2, -5, 4, -1\nD2[-1, -4:] = -1, 4, -5, 2\nD2 *= (1/dt**2) # don't forget h\nD2.toarray()*dt**2\n\narray([[ 2., -5.,  4., -1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1., -2.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1., -2.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1., -2.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1., -2.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1., -2.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1., -2.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -2.,  1.],\n       [ 0.,  0.,  0.,  0.,  0., -1.,  4., -5.,  2.]])\n\n\nApply matrix to a mesh function \\(f(t_n) = t_n^2\\)\n\nt = np.linspace(0, N*dt, N+1)\nf = t**2\nD2 @ f\n\narray([2., 2., 2., 2., 2., 2., 2., 2., 2.])\n\n\nExact for all \\(n\\)!"
  },
  {
    "objectID": "finitedifferences.html#try-with-merely-first-order-boundary",
    "href": "finitedifferences.html#try-with-merely-first-order-boundary",
    "title": "The finite difference method",
    "section": "Try with merely first order boundary",
    "text": "Try with merely first order boundary\n\nD21 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD21[0, :4] = 1, -2, 1, 0\nD21[-1, -4:] = 0, 1, -2, 1\nD21 *= (1/dt**2) \nD21 @ f\n\narray([2., 2., 2., 2., 2., 2., 2., 2., 2.])\n\n\nStill exact! Why?\n\nConsider the forward first order stencil\n\\[\nu''(0) = \\frac{u^{2}-2u^{1}+u^0}{h^2} - \\color{red} h u'''(0) \\color{black} - \\frac{7 h^2}{12}u''''(0) +\n\\]\n\n\nThe leading error is \\(h u'''(0)\\) and \\(u'''(0)\\) is zero for \\(u(t)=t^2\\). The second order stencil captures the second order polynomial \\(t^2\\) exactly!"
  },
  {
    "objectID": "finitedifferences.html#more-challenging-example",
    "href": "finitedifferences.html#more-challenging-example",
    "title": "The finite difference method",
    "section": "More challenging example",
    "text": "More challenging example\nLet \\(f(t) = \\sin(\\pi t / T)\\) such that \\(f''(t) = -(\\pi/T)^2 f(t)\\). Here none of the error terms will disappear and we see the effect of the poor first order boundary.\n\nf = np.sin(np.pi*t / T)\nd2fe = -(np.pi/T)**2*f\nd2f = D2 @ f\nd2f1 = D21 @ f\nplt.plot(t, d2fe, 'k', t, d2f, 'b', t, d2f1, 'r')\nplt.legend(['Exact', '2nd order', '1st order']);"
  },
  {
    "objectID": "finitedifferences.html#first-derivative-matrix",
    "href": "finitedifferences.html#first-derivative-matrix",
    "title": "The finite difference method",
    "section": "First derivative matrix",
    "text": "First derivative matrix\nLets create a similar matrix for a second order accurate single derivative.\n\\[ \\small\n\\begin{align}\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots\n\\end{align}\n\\]\nHere Eq. (1) minus Eq. (-1) leads to\n\\[\nu'(t_n) = \\frac{u^{n+1}-u^{n-1}}{2 h} + \\frac{h^2}{6} u''' +\n\\]\nwhich is second order accurate.\nThe central scheme cannot be used for \\(n=0\\) or \\(n=N\\)."
  },
  {
    "objectID": "finitedifferences.html#we-use-a-forward-scheme-for-n0",
    "href": "finitedifferences.html#we-use-a-forward-scheme-for-n0",
    "title": "The finite difference method",
    "section": "We use a forward scheme for \\(n=0\\)",
    "text": "We use a forward scheme for \\(n=0\\)\n\\[ \\small\n\\begin{align}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align}\n\\]\nFrom Eq. (1):\n\\[\nu'(t_n) = \\frac{u^{n+1}-u^n}{h} - \\frac{\\color{red}h\\color{black}}{2}u'' -\n\\]\nAdding one more equation (2) we get second order: \\((2)-4\\cdot (1)\\) (Note that the terms with \\(u''\\) then cancel)\n\\[\nu'(t_n) = \\frac{-u^{n+2}+4u^{n+1}-3u^n}{2h} + \\frac{\\color{red}h^2\\color{black}}{3}u''' +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#with-forward-and-backward-on-the-edges-we-get-a-second-order-first-derivative-matrix",
    "href": "finitedifferences.html#with-forward-and-backward-on-the-edges-we-get-a-second-order-first-derivative-matrix",
    "title": "The finite difference method",
    "section": "With forward and backward on the edges we get a second order first derivative matrix",
    "text": "With forward and backward on the edges we get a second order first derivative matrix\n\\[ \\small\nD^{(1)} = \\frac{1}{2 h}\\begin{bmatrix}\n-3 & 4 & -1 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 0 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 0 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 0& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& 0& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3 \\\\\n\\end{bmatrix}\n\\]\n\nD1 = sparse.diags([-1, 1], np.array([-1, 1]), (N+1, N+1), 'lil')\nD1[0, :3] = -3, 4, -1\nD1[-1, -3:] = 1, -4, 3 \nD1 *= (1/(2*dt))\nf = t \nD1 @ f \n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
  },
  {
    "objectID": "finitedifferences.html#is-d2-the-same-as-d1d1",
    "href": "finitedifferences.html#is-d2-the-same-as-d1d1",
    "title": "The finite difference method",
    "section": "Is \\(D^{(2)}\\) the same as \\(D^{(1)}D^{(1)}\\)?",
    "text": "Is \\(D^{(2)}\\) the same as \\(D^{(1)}D^{(1)}\\)?\nIs it the same to compute \\(D^{(2)} \\boldsymbol{u}\\) as \\(D^{(1)}(D^{(1)}\\boldsymbol{u})\\)?\n\nAnalytically, we know that \\(u'' = \\frac{d^2u}{dt^2} = \\frac{d}{dt}\\left(\\frac{du}{dt}\\right)\\), but this is not necessarily so numerically.\n\nD22 = D1 @ D1 # @ represents matrix-matrix product \nD22.toarray()*4*dt**2\n\narray([[  5., -11.,   7.,  -1.,   0.,   0.,   0.,   0.,   0.],\n       [  3.,  -5.,   1.,   1.,   0.,   0.,   0.,   0.,   0.],\n       [  1.,   0.,  -2.,   0.,   1.,   0.,   0.,   0.,   0.],\n       [  0.,   1.,   0.,  -2.,   0.,   1.,   0.,   0.,   0.],\n       [  0.,   0.,   1.,   0.,  -2.,   0.,   1.,   0.,   0.],\n       [  0.,   0.,   0.,   1.,   0.,  -2.,   0.,   1.,   0.],\n       [  0.,   0.,   0.,   0.,   1.,   0.,  -2.,   0.,   1.],\n       [  0.,   0.,   0.,   0.,   0.,   1.,   1.,  -5.,   3.],\n       [  0.,   0.,   0.,   0.,   0.,  -1.,   7., -11.,   5.]])\n\n\nThis stencil is wider than \\(D^{(2)}\\), using neighboring points farther away. The central stencil is\n\\[\nu''(t_n) = \\frac{u^{n+2}-2u^{n}+u^{n-2}}{4h^2}\n\\]\nHow accurate is \\(D^{(1)}D^{(1)}\\)?"
  },
  {
    "objectID": "finitedifferences.html#internal-points",
    "href": "finitedifferences.html#internal-points",
    "title": "The finite difference method",
    "section": "Internal points",
    "text": "Internal points\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nTake Eq. (2) plus Eq. (-2)\n\\[\nu^{n+2} + u^{n-2} = 2u^n + 4h^2 u'' + \\frac{4h^4}{3}u'''' +\n\\]\nand\n\\[\nu''(t_n) = \\frac{u^{n+2}-2u^{n}+u^{n-2}}{4h^2} - \\frac{h^2}{3}u'''' + \\cdots\n\\]\nThe stencil in the center (for \\(1&lt;n&lt;N-1\\)) of \\(D^{(1)}D^{(1)}\\) is second order!"
  },
  {
    "objectID": "finitedifferences.html#how-about-boundaries",
    "href": "finitedifferences.html#how-about-boundaries",
    "title": "The finite difference method",
    "section": "How about boundaries?",
    "text": "How about boundaries?\nWe have obtained \\(u''(t_1) = \\frac{u^{n+2}+u^{n+1}-5u^{n}+3u^{n-1}}{4h^2}\\). Compute the error in this stencil using one backward and two forward points\n\\[ \\small\n\\begin{align*}\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nTake Eq. \\((2) + (1) + 3 \\cdot (-1)\\)\n\\[\nu''(t_1) = \\frac{u^{n+2}+u^{n+1}-5u^{n}+3u^{n-1}}{4h^2} + \\frac{\\color{red}h\\color{black}}{4}u''' + \\cdots\n\\]\nOnly first order."
  },
  {
    "objectID": "finitedifferences.html#how-about-the-border-point-for-n0",
    "href": "finitedifferences.html#how-about-the-border-point-for-n0",
    "title": "The finite difference method",
    "section": "How about the border point for \\(n=0\\)?",
    "text": "How about the border point for \\(n=0\\)?\nWe observe that \\(u''(t_0) = \\frac{-u^{n+3}+7u^{n+2}-11u^{n+1}+5u^{n}}{4h^2}\\)\n\\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots \\\\\n(3)\\quad u^{n+3} &= u^n + 3h u' + \\frac{9 h^2}{2}u'' + \\frac{9 h^3}{2}u''' + \\frac{27 h^4}{8}u'''' + \\cdots \\\\\n\\end{align*}\n\\]\nTake Eq. \\(-(3)+7\\cdot (2) -11 \\cdot (1)\\) to obtain\n\\[\nu''(t_0) = \\frac{-u^{n+3}+7u^{n+2}-11u^{n+1}+5u^{n}}{4h^2} -\\frac{3 \\color{red}h\\color{black}}{4}u''' \\cdots\n\\]\nFirst order. So we conclude that \\(D^{(1)}D^{(1)}\\) is still an approximation of the second derivative, but it is only first order accurate near the boundaries."
  },
  {
    "objectID": "finitedifferences.html#test-the-accuracy-of-d2-d2-with-only-first-order-boundary-and-d1d1",
    "href": "finitedifferences.html#test-the-accuracy-of-d2-d2-with-only-first-order-boundary-and-d1d1",
    "title": "The finite difference method",
    "section": "Test the accuracy of \\(D^{(2)}\\), \\(D^{(2)}\\) with only first order boundary and \\(D^{(1)}D^{(1)}\\)",
    "text": "Test the accuracy of \\(D^{(2)}\\), \\(D^{(2)}\\) with only first order boundary and \\(D^{(1)}D^{(1)}\\)\n\nf = np.sin(np.pi*t / T)\nd2fe = -(np.pi/T)**2*f\nd2f = D2 @ f\nd2f1 = D21 @ f\nd2f2 = D1 @ D1 @ f\nplt.plot(t, d2fe, 'k', t, d2f, 'b', t, d2f1, 'r', t, d2f2, 'm')\nplt.legend(['Exact', '2nd order', '1st order', 'D1 @ D1']);"
  },
  {
    "objectID": "finitedifferences.html#the-main-advantage-of-using-difference-matrices-is-that-they-make-it-easy-to-solve-equations",
    "href": "finitedifferences.html#the-main-advantage-of-using-difference-matrices-is-that-they-make-it-easy-to-solve-equations",
    "title": "The finite difference method",
    "section": "The main advantage of using difference matrices is that they make it easy to solve equations",
    "text": "The main advantage of using difference matrices is that they make it easy to solve equations\n\nDiscretize \\(\\rightarrow\\) Simply replace \\(k\\)’th derivative in the ODE with the \\(k\\)’th derivative matrix\nApply boundary conditions to the assembled matrix\n\n\nConsider the exponential decay model\n\\[\nu' + au = 0, \\, t \\in (0, T], u(0)=I.\n\\]\nReplace derivatives with derivative matrices (\\(u'=D^{(1)}\\boldsymbol{u}, u = \\mathbb{I}\\boldsymbol{u}\\)):\n\\[\n(D^{(1)} + a \\mathbb{I})\\boldsymbol{u} = \\boldsymbol{0},\n\\]\nwhere \\(\\mathbb{I} \\in \\mathbb{R}^{N+1\\times N+1}\\) is the identity matrix and \\(\\boldsymbol{0}\\in \\mathbb{R}^{N+1}\\) is a null-vector. We get\n\\[\nA \\boldsymbol{u} = \\boldsymbol{0}, \\quad A = D^{(1)}+a\\mathbb{I}\n\\]\nwhich is trivially solved to \\(\\boldsymbol{u}=\\boldsymbol{0}\\) before adding boundary conditions. Not after."
  },
  {
    "objectID": "finitedifferences.html#apply-boundary-condition-by-modifying-both-the-coefficient-matrix-a-and-the-rhs-vector-boldsymbolb",
    "href": "finitedifferences.html#apply-boundary-condition-by-modifying-both-the-coefficient-matrix-a-and-the-rhs-vector-boldsymbolb",
    "title": "The finite difference method",
    "section": "Apply boundary condition by modifying both the coefficient matrix \\(A\\) and the rhs vector \\(\\boldsymbol{b}\\)",
    "text": "Apply boundary condition by modifying both the coefficient matrix \\(A\\) and the rhs vector \\(\\boldsymbol{b}\\)\nWith just the derivative matrix we have the problem\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}, \\quad \\text{for} \\quad \\boldsymbol{b}\\in \\mathbb{R}^{N+1}\n\\]\n\\[ \\small\n\\underbrace{\n  \\frac{1}{2h} \\begin{bmatrix}\n-3+2ah & 4 & -1 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 2ah & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 2ah & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 2ah& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& -2ah& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3+2ah\n\\end{bmatrix}\n}_{D^{(1)}+a\\mathbb{I}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-2} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n=\n\\underbrace{\n\\begin{bmatrix}\nb^0 \\\\\nb^1 \\\\\nb^2 \\\\\n\\vdots \\\\\nb^{N-2} \\\\\nb^{N-1} \\\\\nb^{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{b}} =\n\\]\nIn order to enforce that \\(u(0)= u^0 = I\\), we can modify the first row of the coefficient matrix \\(A\\) and the right hand side vector \\(\\boldsymbol{b}\\)"
  },
  {
    "objectID": "finitedifferences.html#modify-a-and-boldsymbolb-to-enforce-boundary-conditions",
    "href": "finitedifferences.html#modify-a-and-boldsymbolb-to-enforce-boundary-conditions",
    "title": "The finite difference method",
    "section": "Modify \\(A\\) and \\(\\boldsymbol{b}\\) to enforce boundary conditions",
    "text": "Modify \\(A\\) and \\(\\boldsymbol{b}\\) to enforce boundary conditions\n\\[ \\small\n\\underbrace{\n  \\frac{1}{2h} \\begin{bmatrix}\n2h & 0 & 0 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 2ah & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 2ah & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 2ah& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& -2ah& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3+2ah\n\\end{bmatrix}\n}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-2} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n=\n\\underbrace{\n\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]\nNow the equation in the first row states that \\(u^0 = I\\), whereas the remaining rows are unchanged and solve the central and implicit problem for row \\(n\\) (row \\(N\\) is slightly different)\n\\[\n\\frac{u^{n+1}-u^{n-1}}{2h} + au^n = 0\n\\]"
  },
  {
    "objectID": "finitedifferences.html#implement-decay-problem",
    "href": "finitedifferences.html#implement-decay-problem",
    "title": "The finite difference method",
    "section": "Implement decay problem",
    "text": "Implement decay problem\n\nN = 10\ndt = 0.5\na = 5\nT = N*dt\nt = np.linspace(0, N*dt, N+1) \nD1 = sparse.diags([-1, 1], np.array([-1, 1]), (N+1, N+1), 'lil')\nD1[0, :3] = -3, 4, -1 \nD1[-1, -3:] = 1, -4, 3\nD1 *= (1/(2*dt))\nId = sparse.eye(N+1)\nA = D1 + a*Id\nb = np.zeros(N+1)\nb[0] = I\nA[0, :3] = 1, 0, 0 # boundary condition\nA.toarray()\n\narray([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [-1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -4.,  8.]])"
  },
  {
    "objectID": "finitedifferences.html#compare-with-exact-solution",
    "href": "finitedifferences.html#compare-with-exact-solution",
    "title": "The finite difference method",
    "section": "Compare with exact solution",
    "text": "Compare with exact solution\n\n\n\n\n\n\n\n\n\nThe accuracy is similar to the Crank-Nicolson method discussed in lectures 1-2. However, the FD method is not a recursive marching method, since the equation for \\(u^n\\) depends on the solution at \\(u^{n+1}\\)! This implicit FD method is unconditionally stable. Normally, only marching methods are analysed for stability."
  },
  {
    "objectID": "finitedifferences.html#the-vibration-eq.-is-often-considered-as-a-bvp",
    "href": "finitedifferences.html#the-vibration-eq.-is-often-considered-as-a-bvp",
    "title": "The finite difference method",
    "section": "The vibration eq. is often considered as a BVP",
    "text": "The vibration eq. is often considered as a BVP\n\\[\nu''(t) + \\omega^2 u(t) = 0, \\quad u(0) = u(T) = I, \\quad t=(0, T)\n\\]\nDiscretize by replacing the derivative with a second differentiation matrix\n\\[\n(D^{(2)} + \\omega^2 \\mathbb{I}) \\boldsymbol{u} = \\boldsymbol{b}\n\\]\n\nw = 2*np.pi \nT, N, I = 3., 35, 1.\ndt = T/N \nD2 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD2 *= (1/(dt**2)) # never mind the edges n=0 or n=N because these rows will be overwritten\nId = sparse.eye(N+1)\nA = D2 + w**2*Id\nb = np.zeros(N+1)\nA.toarray()*dt**2\n\narray([[-1.71,  1.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 1.  , -1.71,  1.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  1.  , -1.71, ...,  0.  ,  0.  ,  0.  ],\n       ...,\n       [ 0.  ,  0.  ,  0.  , ..., -1.71,  1.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  1.  , -1.71,  1.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  1.  , -1.71]])"
  },
  {
    "objectID": "finitedifferences.html#apply-boundary-conditions-to-the-first-and-last-rows",
    "href": "finitedifferences.html#apply-boundary-conditions-to-the-first-and-last-rows",
    "title": "The finite difference method",
    "section": "Apply boundary conditions to the first and last rows",
    "text": "Apply boundary conditions to the first and last rows\n\\[\n\\begin{align}\nu^0 &= I \\\\\nu^N &= I\n\\end{align}\n\\]\n\nA[0, :3] = 1, 0, 0\nA[-1, -3:] = 0, 0, 1\nb[0] = I\nb[-1] = I\nA.toarray()\n\narray([[   1.   ,    0.   ,    0.   , ...,    0.   ,    0.   ,    0.   ],\n       [ 136.111, -232.744,  136.111, ...,    0.   ,    0.   ,    0.   ],\n       [   0.   ,  136.111, -232.744, ...,    0.   ,    0.   ,    0.   ],\n       ...,\n       [   0.   ,    0.   ,    0.   , ..., -232.744,  136.111,    0.   ],\n       [   0.   ,    0.   ,    0.   , ...,  136.111, -232.744,  136.111],\n       [   0.   ,    0.   ,    0.   , ...,    0.   ,    0.   ,    1.   ]])"
  },
  {
    "objectID": "finitedifferences.html#solve-the-vibration-equation",
    "href": "finitedifferences.html#solve-the-vibration-equation",
    "title": "The finite difference method",
    "section": "Solve the vibration equation",
    "text": "Solve the vibration equation\n\nu2 = sparse.linalg.spsolve(A, b)\nt = np.linspace(0, T, N+1)\nplt.plot(t, u2, 'bo', t, I*np.cos(w*t), 'r--')\nplt.legend(['Numerical', 'Exact'])"
  },
  {
    "objectID": "finitedifferences.html#generic-finite-difference-stencils",
    "href": "finitedifferences.html#generic-finite-difference-stencils",
    "title": "The finite difference method",
    "section": "Generic finite difference stencils",
    "text": "Generic finite difference stencils\nWe have seen that it is quite simple to develop finite difference stencils for any derivative, using either forward or backward points. Can this be generalized?\n\nYes! Of course it can. The generic Taylor expansion around \\(x=x_0\\) reads\n\\[\nu(x) = \\sum_{i=0}^{M} \\frac{(x-x_0)^i}{i!} u^{(i)}(x_0) + \\mathcal{O}((x-x_0)^{M}),\n\\]\nwhere \\(u^{(i)}(x_0) = \\frac{d^{i}u}{dx^{i}}|_{x=x_0}\\) and the are \\(M+1\\) terms in the expansion.\nUse only \\(x=x_0+ph\\), where \\(p\\) is an integer and \\(h\\) is a constant (\\(\\Delta t\\) or \\(\\Delta x\\))\n\\[\nu^{n+p} = \\sum_{i=0}^{M} \\frac{(ph)^i}{i!} u^{(i)}(x_0) + \\mathcal{O}(h^{M+1}),\n\\]\nwhere \\(u^{n+p} = u(x_0+ph)\\)"
  },
  {
    "objectID": "finitedifferences.html#generic-fd",
    "href": "finitedifferences.html#generic-fd",
    "title": "The finite difference method",
    "section": "Generic FD",
    "text": "Generic FD\nThe truncated Taylor expansions for a given \\(p\\) can be written \\[\nu^{n+p} = \\sum_{i=0}^{M} \\frac{(ph)^i}{i!} u^{(i)}(x_0) = \\sum_{i=0}^{M} c_{pi} du_i\n\\]\nusing \\(c_{pi} = \\frac{(ph)^i}{i!}\\) and \\(du_i = u^{(i)}(x_0)\\).\n\nThis can be understood as a matrix-vector product\n\\[\n\\boldsymbol{u} = C \\boldsymbol{du},\n\\]\nwhere \\(\\boldsymbol{u} = (u^{n+p})_{p=p_0}^{M+p_0}\\), \\(C = (c_{p_0+p,i})_{p,i=0}^{M,M}\\) and \\(\\boldsymbol{du}=(du_i)_{i=0}^M\\). Here \\(p_0\\) is an integer representing the lowest value of \\(p\\) in the stencil.\nFor \\(p_0=-2\\) and \\(M=4\\):\n\\[\n\\boldsymbol{u} = (u^{n-2}, u^{n-1}, u^{n}, u^{n+1}, u^{n+2})^T \\quad\n\\boldsymbol{du} =(u^{(0)}, u^{(1)}, u^{(2)}, u^{(3)}, u^{(4)})^T\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-stencil-matrix",
    "href": "finitedifferences.html#the-stencil-matrix",
    "title": "The finite difference method",
    "section": "The stencil matrix",
    "text": "The stencil matrix\nFor \\(p_0=-2\\) and \\(M=4\\) we get 5 Taylor expansions\n\\[\n\\begin{align*}\nu^{n-2} &= \\sum_{i=0}^{M} \\frac{(-2h)^i}{i!} du_i  \\\\\nu^{n-1} &= \\sum_{i=0}^{M} \\frac{(-h)^i}{i!} du_i  \\\\\nu^{n} &= u^{n} \\\\\nu^{n+1} &= \\sum_{i=0}^{M} \\frac{(h)^i}{i!} du_i  \\\\\nu^{n+2} &= \\sum_{i=0}^{M} \\frac{(2h)^i}{i!} du_i  \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-stencil-matrix-ctd",
    "href": "finitedifferences.html#the-stencil-matrix-ctd",
    "title": "The finite difference method",
    "section": "The stencil matrix ctd",
    "text": "The stencil matrix ctd\nExpanding the sums these 5 Taylor expansions can be written in matrix form\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{n-2}\\\\\nu^{n-1}\\\\\nu^{n}\\\\\nu^{n+1}\\\\\nu^{n+2}\\\\\n\\end{bmatrix}}_{\\boldsymbol{u}}\n= \\underbrace{\\begin{bmatrix}\n\\frac{(-2h)^0}{0!} & \\frac{(-2h)^1}{1!} & \\frac{(-2h)^2}{2!} & \\frac{(-2h)^3}{3!} & \\frac{(-2h)^4}{4!}  \\\\\n\\frac{(-h)^0}{0!} & \\frac{(-h)^1}{1!} & \\frac{(-h)^2}{2!} & \\frac{(-h)^3}{3!} & \\frac{(-h)^4}{4!} \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n\\frac{(h)^0}{0!} & \\frac{(h)^1}{1!} & \\frac{(h)^2}{2!} & \\frac{(h)^3}{3!} & \\frac{(h)^4}{4!} \\\\\n\\frac{(2h)^0}{0!} & \\frac{(2h)^1}{1!} & \\frac{(2h)^2}{2!} & \\frac{(2h)^3}{3!} & \\frac{(2h)^4}{4!} \\\\\n\\end{bmatrix}}_{C}\n\\underbrace{\n\\begin{bmatrix}\ndu_{0} \\\\\ndu_1 \\\\\ndu_2 \\\\\ndu_3 \\\\\ndu_4\n\\end{bmatrix}}_{\\boldsymbol{du}}\n\\]\n\\[\n\\boldsymbol{u} = C \\boldsymbol{du}\n\\]\n\nInvert to obtain\n\\[\n\\boldsymbol{du} = C^{-1} \\boldsymbol{u}\n\\]\nSince \\(du_i\\) is an approximation to the i’th derivative we can now compute any derivative stencil!"
  },
  {
    "objectID": "finitedifferences.html#second-order-2nd-derivative-stencil",
    "href": "finitedifferences.html#second-order-2nd-derivative-stencil",
    "title": "The finite difference method",
    "section": "Second order 2nd derivative stencil",
    "text": "Second order 2nd derivative stencil\nWe have been using the following stencil\n\\[\ndu_2 = u^{(2)}(x_0) = \\frac{u^{n+1}-2u^n+u^{n-1}}{h^2}.\n\\]\nLet’s derive this with the approach above. The scheme is central and second order so we use \\(p_0=-1\\) and \\(M=2\\) (hence \\(m=(-1, 0, 1)\\)). Insert into the recipe for \\(C\\)\n\\[\nC = \\begin{bmatrix}\n1 & -h & \\frac{h^2}{2} \\\\\n1 & 0 & 0 \\\\\n1 & h & \\frac{h^2}{2}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#in-sympy",
    "href": "finitedifferences.html#in-sympy",
    "title": "The finite difference method",
    "section": "In Sympy",
    "text": "In Sympy\n\nimport sympy as sp\nx, h = sp.symbols('x,h')\nC = sp.Matrix([[1, -h, h**2/2], [1, 0, 0], [1, h, h**2/2]])\n\n\n\nPrint \\(C\\) matrix\n\nC\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & - h & \\frac{h^{2}}{2}\\\\1 & 0 & 0\\\\1 & h & \\frac{h^{2}}{2}\\end{matrix}\\right]\\)\n\n\n\nPrint \\(C^{-1}\\) matrix\n\nC.inv() \n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 0\\\\- \\frac{1}{2 h} & 0 & \\frac{1}{2 h}\\\\\\frac{1}{h^{2}} & - \\frac{2}{h^{2}} & \\frac{1}{h^{2}}\\end{matrix}\\right]\\)\n\n\n\n\nThe last row in \\(C^{-1}\\) represents \\(u''\\)! The middle row represents a second order central \\(u'\\).\nCreate a vector for \\(\\boldsymbol{u}\\) and print \\(u'\\) and \\(u''\\)\n\nu = sp.Function('u')\ncoef = sp.Matrix([u(x-h), u(x), u(x+h)]) \n\n\n\n\n(C.inv()[1, :] @ coef)[0]\n\n\\(\\displaystyle - \\frac{u{\\left(- h + x \\right)}}{2 h} + \\frac{u{\\left(h + x \\right)}}{2 h}\\)\n\n\n\n\n(C.inv()[2, :] @ coef)[0]\n\n\\(\\displaystyle - \\frac{2 u{\\left(x \\right)}}{h^{2}} + \\frac{u{\\left(- h + x \\right)}}{h^{2}} + \\frac{u{\\left(h + x \\right)}}{h^{2}}\\)"
  },
  {
    "objectID": "finitedifferences.html#we-can-get-any-stencil",
    "href": "finitedifferences.html#we-can-get-any-stencil",
    "title": "The finite difference method",
    "section": "We can get any stencil",
    "text": "We can get any stencil\nCreate a function that computes \\(C\\) for any \\(p_0\\) and \\(M\\)\n\ndef Cmat(p0, M):\n  C = np.zeros((M+1, M+1), dtype=object)\n  for j, p in enumerate(range(p0, p0+M+1)):\n    for i in range(M+1):\n      C[j, i] = (p*h)**i / sp.factorial(i) \n  return sp.Matrix(C)\n\n\\(p_0=-1, M=2\\)\n\nCmat(-1, 2)\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & - h & \\frac{h^{2}}{2}\\\\1 & 0 & 0\\\\1 & h & \\frac{h^{2}}{2}\\end{matrix}\\right]\\)\n\n\n\nA central stencil of order \\(l\\) for the \\(k\\)’th’ derivative requires \\(M+1\\) points, where\n\\[\nM = l + 2 \\left\\lfloor \\frac{k-1}{2} \\right\\rfloor\n\\]"
  },
  {
    "objectID": "finitedifferences.html#forward-and-backward",
    "href": "finitedifferences.html#forward-and-backward",
    "title": "The finite difference method",
    "section": "Forward and backward",
    "text": "Forward and backward\nNon-central schemes requires one more point for the same accuracy as central\n\n\nForward \\(u''\\)\n\\(p_0=0, M=3\\)\n\nC = Cmat(0, 3)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\- \\frac{11}{6 h} & \\frac{3}{h} & - \\frac{3}{2 h} & \\frac{1}{3 h}\\\\\\frac{2}{h^{2}} & - \\frac{5}{h^{2}} & \\frac{4}{h^{2}} & - \\frac{1}{h^{2}}\\\\- \\frac{1}{h^{3}} & \\frac{3}{h^{3}} & - \\frac{3}{h^{3}} & \\frac{1}{h^{3}}\\end{matrix}\\right]\\)\n\n\n\nBackward \\(u''\\)\n\\(p_0 = -3, M=3\\)\n\nC = Cmat(-3, 3)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 1\\\\- \\frac{1}{3 h} & \\frac{3}{2 h} & - \\frac{3}{h} & \\frac{11}{6 h}\\\\- \\frac{1}{h^{2}} & \\frac{4}{h^{2}} & - \\frac{5}{h^{2}} & \\frac{2}{h^{2}}\\\\- \\frac{1}{h^{3}} & \\frac{3}{h^{3}} & - \\frac{3}{h^{3}} & \\frac{1}{h^{3}}\\end{matrix}\\right]\\)\n\n\n\n\nRecognize the third row in the forward scheme:\n\\[\nu'' = \\frac{-u^{n+3} + 4u^{n+2} - 5u^{n+1} +2 u^{n}}{h^2}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#fourth-order-central-small-p_0-2-m4",
    "href": "finitedifferences.html#fourth-order-central-small-p_0-2-m4",
    "title": "The finite difference method",
    "section": "Fourth order central \\(\\small p_0=-2, M=4\\)",
    "text": "Fourth order central \\(\\small p_0=-2, M=4\\)\n\nC = Cmat(-2, 4)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 1 & 0 & 0\\\\\\frac{1}{12 h} & - \\frac{2}{3 h} & 0 & \\frac{2}{3 h} & - \\frac{1}{12 h}\\\\- \\frac{1}{12 h^{2}} & \\frac{4}{3 h^{2}} & - \\frac{5}{2 h^{2}} & \\frac{4}{3 h^{2}} & - \\frac{1}{12 h^{2}}\\\\- \\frac{1}{2 h^{3}} & \\frac{1}{h^{3}} & 0 & - \\frac{1}{h^{3}} & \\frac{1}{2 h^{3}}\\\\\\frac{1}{h^{4}} & - \\frac{4}{h^{4}} & \\frac{6}{h^{4}} & - \\frac{4}{h^{4}} & \\frac{1}{h^{4}}\\end{matrix}\\right]\\)\n\n\nThird row gives us:\n\\[\nu'' = \\frac{-u^{n+2} + 16u^{n+1} - 30 u^n + 16 u^{n-1} - u^{n-2}}{12 h^2} + \\mathcal{O}(h^4)\n\\]\nwhere the order \\(l = M - 2 \\left\\lfloor \\frac{2-1}{2} \\right \\rfloor = M = 4\\).\n\nWhat is the order of the fourth derivative \\(u''''=\\frac{u^{n+2}-4u^{n+1}+6u^{n}-4u^{n-1}+u^{n-2}}{h^4}\\)?\n\n\n\\(l = M - 2 \\left\\lfloor \\frac{4-1}{2} \\right \\rfloor = M - 2 = 2\\)"
  },
  {
    "objectID": "wave.html#the-wave-equation-is-a-partial-differential-equation-pde",
    "href": "wave.html#the-wave-equation-is-a-partial-differential-equation-pde",
    "title": "Finite difference methods for the wave equation",
    "section": "The wave equation is a partial differential equation (PDE)",
    "text": "The wave equation is a partial differential equation (PDE)\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2}.\n\\]\nWe will consider the time and space domains: \\(t\\in[0, T]\\), \\(x \\in [0, L]\\).\n\n\nThe wave equation is an initial-boundary value problem!\nTwo initial conditions required since two derivatives in time\nTwo boundary conditions required since two derivatives in space\nThe solutions are waves that can be written as \\(u(x+ct)\\) and \\(u(x-ct)\\)"
  },
  {
    "objectID": "wave.html#wave-solution-with-different-boundary-conditions",
    "href": "wave.html#wave-solution-with-different-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Wave solution with different boundary conditions",
    "text": "Wave solution with different boundary conditions"
  },
  {
    "objectID": "wave.html#boundary-conditions",
    "href": "wave.html#boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Boundary conditions",
    "text": "Boundary conditions\n\n\nDirichlet (Fixed end)\n\n\\[\nu(0, t) = u(L, t) = 0\n\\]\n\n\nThe wave will be reflected, but \\(u\\) will change sign. A nonzero Dirichlet condition is also possible, but will not be considered here.\n\n\n\nNeumann (Loose end)\n\n\\[\n\\frac{\\partial u}{\\partial x}(0, t) = \\frac{\\partial u}{\\partial x}(L, t) = 0\n\\]\n\n\nThe wave will be reflected without change in sign. A nonzero Neumann condition is also possible, but will not be considered here."
  },
  {
    "objectID": "wave.html#boundary-conditions-continued",
    "href": "wave.html#boundary-conditions-continued",
    "title": "Finite difference methods for the wave equation",
    "section": "Boundary conditions continued",
    "text": "Boundary conditions continued\n\n\nOpen boundary (No end)\n\n\\[\n\\frac{\\partial u(0, t)}{\\partial t} - c \\frac{\\partial u(0, t)}{\\partial x}= 0\n\\]\n\\[\n\\frac{\\partial u(L, t)}{\\partial t} + c \\frac{\\partial u(L, t)}{\\partial x}=0\n\\]\n\n\nThe wave will simply pass undisturbed and unreflected through an open boundary.\n\n\n\nPeriodic boundary (No end)\n\n\\[\nu(0, t) = u(L, t)\n\\]\n\n\nThe solution repeats itself indefinitely."
  },
  {
    "objectID": "wave.html#discretization",
    "href": "wave.html#discretization",
    "title": "Finite difference methods for the wave equation",
    "section": "Discretization",
    "text": "Discretization\nThe simplest possible discretization is uniform in time and space\n\\[\nt_n = n \\Delta t, \\quad n = 0, 1, \\ldots, N_t\n\\]\n\\[\nx_j = j \\Delta x, \\quad j = 0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "wave.html#a-mesh-function-in-space-and-time-is-defined-as",
    "href": "wave.html#a-mesh-function-in-space-and-time-is-defined-as",
    "title": "Finite difference methods for the wave equation",
    "section": "A mesh function in space and time is defined as",
    "text": "A mesh function in space and time is defined as\n\\[\nu^n_j = u(x_j, t_n)\n\\]\nThe mesh function has one value at each node in the mesh. For simplicity in later algorithms we will use the vectors\n\\[\nu^n = (u^n_0, u^n_1, \\ldots, u^n_N)^T,\n\\]\nwhich is the solution vector at time \\(t_n\\).\nA second order accurate discretization of the wave equation is\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points",
    "href": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil makes use of 5 neighboring points",
    "text": "The finite difference stencil makes use of 5 neighboring points\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\nCan only be used for internal points"
  },
  {
    "objectID": "wave.html#we-use-a-marching-method-in-time",
    "href": "wave.html#we-use-a-marching-method-in-time",
    "title": "Finite difference methods for the wave equation",
    "section": "We use a marching method in time",
    "text": "We use a marching method in time\n\nInitialize \\(u^0\\) and \\(u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\nfor j in range(\\(1, N-1\\)):\n\n\\(u^{n+1}_j = 2u^n_j - u^{n-1}_j + \\left(\\frac{c \\Delta t}{ \\Delta x}\\right)^2 (u^{n}_{j+1}-2u^n_{j} + u^n_{j-1})\\)\n\n\n\nand apply the chosen boundary conditions.\n\nAll the indices makes it a bit messy. Lets make use of a differentiation matrix for the spatial dimension! And the Courant (or CFL) number\n\\[\n\\overline{c} = \\frac{c \\Delta t}{\\Delta x}\n\\]"
  },
  {
    "objectID": "wave.html#use-differentiation-matrix-to-simplify-the-notation",
    "href": "wave.html#use-differentiation-matrix-to-simplify-the-notation",
    "title": "Finite difference methods for the wave equation",
    "section": "Use differentiation matrix to simplify the notation",
    "text": "Use differentiation matrix to simplify the notation\nWe define the second differentiation matrix without the scaling \\(1/(\\Delta x)^2\\) such that\n\\[\\small\nD^{(2)} = \\begin{bmatrix}\n-2 & 1 & 0 & 0  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 0 & 1 & -2 \\\\\n\\end{bmatrix}\n\\]\nand thus row \\(0 &lt; j &lt; N\\) of \\(D^{(2)}u^n\\) becomes\n\\[\n(D^{(2)}u^n)_j = u^n_{j+1}-2u^n_j+u^n_{j-1}\n\\]"
  },
  {
    "objectID": "wave.html#the-vectorized-marching-method-becomes",
    "href": "wave.html#the-vectorized-marching-method-becomes",
    "title": "Finite difference methods for the wave equation",
    "section": "The vectorized marching method becomes",
    "text": "The vectorized marching method becomes\n\nInitialize \\(u^0\\) and \\(u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\\)\nApply boundary conditions to \\(u^{n+1}_0\\) and \\(u^{n+1}_N\\)\n\n\n\n\nThe boundary step can often, but not always, be incorporated into the matrix \\(D^{(2)}\\)\nVery easy to vectorize using the matrix vector product!"
  },
  {
    "objectID": "wave.html#pde-solvers-of-time-dependent-problems-should-use-memory-carefully",
    "href": "wave.html#pde-solvers-of-time-dependent-problems-should-use-memory-carefully",
    "title": "Finite difference methods for the wave equation",
    "section": "PDE solvers (of time-dependent problems) should use memory carefully",
    "text": "PDE solvers (of time-dependent problems) should use memory carefully\n\n\n\n\n\n\nNote\n\n\n\nAt any time we only need to store three vectors: \\(u^{n+1}, u^{n}\\) and \\(u^{n-1}\\).\n\nMemory requirement = \\(3(N+1)\\) floating point numbers\n\nStoring all time steps requires \\((N_t+1) \\times (N+1)\\) floating point numbers\nNot a huge problem for our case, but for 2 or 3 spatial dimensions it is very important!"
  },
  {
    "objectID": "wave.html#implementation---a-low-memory-marching-method-needs-to-update-solution-vectors",
    "href": "wave.html#implementation---a-low-memory-marching-method-needs-to-update-solution-vectors",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation - A low-memory marching method needs to update solution vectors",
    "text": "Implementation - A low-memory marching method needs to update solution vectors\n\nAllocate three vectors \\(u^{nm1}, u^n, u^{np1}\\), representing \\(u^{n-1}, u^n, u^{n+1}\\).\nInitialize \\(u^0\\) and \\(u^1\\) by setting \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 D^{(2)} u^n\\)\nApply boundary conditions to \\(u^{np1}_0\\) and \\(u^{np1}_N\\)\nUpdate to next iteration:\n\n\\(u^{nm1} \\gets u^n\\)\n\\(u^n \\gets u^{np1}\\)"
  },
  {
    "objectID": "wave.html#in-python",
    "href": "wave.html#in-python",
    "title": "Finite difference methods for the wave equation",
    "section": "In Python",
    "text": "In Python\nSet up solver\n\nimport numpy as np \nfrom scipy import sparse\nimport sympy as sp \nx, t = sp.symbols('x,t')\nN = 100\nNt = 500 \nL = 2\nc = 1 # wavespeed\ndx = L / N\nCFL = 1.0\ndt = CFL*dx/c\nxj = np.linspace(0, L, N+1)\nunm1, un, unp1 = np.zeros((3, N+1))\nD2 = sparse.diags([1, -2, 1], [-1, 0, 1], (N+1, N+1))\nu0 = sp.exp(-200*(x-L/2+t)**2)\n\nSolve by marching method\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1"
  },
  {
    "objectID": "wave.html#store-results-at-intermediate-intervals-for-plotting",
    "href": "wave.html#store-results-at-intermediate-intervals-for-plotting",
    "title": "Finite difference methods for the wave equation",
    "section": "Store results at intermediate intervals for plotting",
    "text": "Store results at intermediate intervals for plotting\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nplotdata = {0: unm1.copy()}\nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1\n  if n % 10 == 0:\n    plotdata[n] = unp1.copy()\n\nFor example every tenth time step. Normally you do not need every time step to get a good animation."
  },
  {
    "objectID": "wave.html#create-animation-after-the-simulation-is-finished",
    "href": "wave.html#create-animation-after-the-simulation-is-finished",
    "title": "Finite difference methods for the wave equation",
    "section": "Create animation after the simulation is finished",
    "text": "Create animation after the simulation is finished\n\n\ndef animation(data):\n  from matplotlib import animation \n  fig, ax = plt.subplots()\n  v = np.array(list(data.values()))\n  t = np.array(list(data.keys()))\n  save_step = t[1]-t[0]\n  line, = ax.plot(xj, data[0])\n  ax.set_ylim(v.min(), v.max())\n  def update(frame):\n    line.set_ydata(data[frame*save_step])\n    return (line,)\n  ani = animation.FuncAnimation(fig=fig, func=update, frames=len(data), blit=True)\n  ani.save('wavemovie.apng', writer='pillow', fps=5) # This animated png opens in a browser"
  },
  {
    "objectID": "wave.html#how-to-implement-the-initial-conditions",
    "href": "wave.html#how-to-implement-the-initial-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "How to implement the initial conditions?",
    "text": "How to implement the initial conditions?\nTo initialize a mesh function \\(u^0\\), we write \\[\nu^0 = I(x)\n\\]\nwhich represents\n\\[\nu^0_j = I(x_j), \\quad \\forall \\, j=0, 1, \\ldots, N\n\\]\n\n\nu0 = sp.exp(-200*(x-L/2+t)**2) \nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj) \n\n\n\nHow about the second condition \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\)?\n\n\nJust like for the vibration equation there are several options. If you have an analytical solution \\(I(x, t)\\) that is time-dependent you can specify one wave as:\n\\[\nu^1 = I(x, \\Delta t)\n\\]\n\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj)"
  },
  {
    "objectID": "wave.html#if-you-do-not-have-ix-t-then-what",
    "href": "wave.html#if-you-do-not-have-ix-t-then-what",
    "title": "Finite difference methods for the wave equation",
    "section": "If you do not have \\(I(x, t)\\), then what?",
    "text": "If you do not have \\(I(x, t)\\), then what?\n\n\nHow to fix \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 1\nUse a forward difference\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) \\approx \\frac{u^1-u^0}{\\Delta t}=0, \\quad \\text{such that} \\quad u^1 = u^0\n\\]\nOnly first order accurate, but still a possibility.\n\n\nUse a second order forward difference\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) \\approx \\frac{-u^2+4u^1-3u^0}{2 \\Delta t}=0, \\quad \\text{such that} \\quad u^1 = \\frac{3u^0+u^2}{4}\n\\]\nSecond order accurate, but implicit. And you get two waves!"
  },
  {
    "objectID": "wave.html#how-to-implement-fracpartial-upartial-tx-0-0-option-2",
    "href": "wave.html#how-to-implement-fracpartial-upartial-tx-0-0-option-2",
    "title": "Finite difference methods for the wave equation",
    "section": "How to implement \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 2",
    "text": "How to implement \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 2\nUse a second order central differece\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) = \\frac{u^1-u^{-1}}{2 \\Delta t}=0, \\quad \\text{such that} \\quad u^1 = u^{-1}\n\\]\nand the PDE at \\(n=0\\)\n\\[\nu^1 = 2u^0 - u^{-1} + \\underline{c}^2 D^{(2)}u^{0}\n\\]\n\nInsert for \\(u^{-1}=u^1\\) to obtain\n\\[\nu^1 = u^0 + \\frac{\\underline{c}^2}{2} D^{(2)}u^{0}\n\\]\nSecond order accurate and explicit"
  },
  {
    "objectID": "wave.html#how-to-fix-boundary-conditions",
    "href": "wave.html#how-to-fix-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "How to fix boundary conditions?",
    "text": "How to fix boundary conditions?\nWe will consider 4 different types of boundary conditions\n\n\n\n\n\n\n\nDirichlet\n\\(u(0, t)\\) and \\(u(L, t)\\)\n\n\nNeumann\n\\(\\frac{\\partial u}{\\partial x}(0, t)\\) and \\(\\frac{\\partial u}{\\partial x}(L, t)\\)\n\n\nOpen\n\\(\\frac{\\partial u}{\\partial t}(0, t)-c\\frac{\\partial u}{\\partial x}(0, t) = 0\\) and \\(\\frac{\\partial u}{\\partial t}(L, t)+c\\frac{\\partial u}{\\partial x}(L, t) = 0\\)\n\n\nPeriodic\n\\(u(L, t) = u(0, t)\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAccounting for boundary conditions very often takes more than 50 % of the lines of code in a PDE solver!"
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions",
    "href": "wave.html#dirichlet-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions",
    "text": "Dirichlet boundary conditions\nWe need to fix \\(u(0, t) = I(0)\\) and \\(u(L, t) = I(L)\\) and start by fixing this at \\(t=0\\)\n\\[\nu^0_0=I(0) \\quad \\text{and}\\quad u^0_N=I(L)\n\\]\n\nNext, we compute\n\\[\nu^1 = u^0 + \\frac{\\underline{c}^2}{2} D^{(2)}u^{0}\n\\]\nHere, if the first and last rows of \\(D^{(2)}\\) are set to zero, then \\(u^1_0 = u^0_0\\) and \\(u^1_N=u^0_N\\).\n\n\nNext, for \\(n=1, 2, \\ldots, N_t-1\\)\n\\[\nu^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\n\\]\nAgain, if the first and last rows of \\(D^{(2)}\\) are zero, then \\(u^{n+1}_0 = u^0_0\\) and \\(u^{n+1}_N=u^0_N\\) for all \\(n\\). The boundary values remain as initially set at \\(t=0\\)."
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions-summary",
    "href": "wave.html#dirichlet-boundary-conditions-summary",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions summary",
    "text": "Dirichlet boundary conditions summary\nSet \\(u^0=I(x)\\) and define a modified differentiation matrix\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0 \\\\\n\\end{bmatrix}\n\\]\nNow boundary conditions will be ok at all time steps simply by:\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\nUpdate to next iteration: \\(u^{nm1} = u^n; u^n = u^{np1}\\)"
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions-summary-1",
    "href": "wave.html#dirichlet-boundary-conditions-summary-1",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions summary",
    "text": "Dirichlet boundary conditions summary\n\n\n\n\n\n\nNote\n\n\nIt is also possible to do nothing with \\(D^{(2)}\\) and simply fix the boundary conditions after updating all the internal points\n\n\n\n\n\nInitialize \\(u^{nm1}=u^0\\) and compute \\(u^n = u^1 = u^0+\\frac{\\underline{c}^2}{2}{D}^{(2)} u^0\\).\nSet \\(u^{nm1}_0=u^n_0=0\\) and \\(u^{nm1}_N=u^n_N=0\\).\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 {D}^{(2)} u^n\\)\nSet \\(u^{np1}_0=0\\) and \\(u^{np1}_N=0\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)\n\n\n\n\n\n\n\n\nNote\n\n\nRegular, unmodified \\(D^{(2)}\\), where the first and last rows are completely irrelevant."
  },
  {
    "objectID": "wave.html#neumann-boundary-conditions",
    "href": "wave.html#neumann-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann boundary conditions",
    "text": "Neumann boundary conditions\nWe need to fix \\(\\frac{\\partial u}{\\partial x}(0, t) = 0\\) and \\(\\frac{\\partial u}{\\partial x}(L, t) = 0\\). We already have \\(u^0=I(x)\\).\n\nA second order central scheme at \\(x=0\\) is using ghost cell at \\(j=-1\\)\n\\[\n\\frac{\\partial u}{\\partial x}(0, t_n) = \\frac{u^{n}_1 - u^n_{-1}}{2 \\Delta x} = 0 \\rightarrow u^n_{-1} = u^{n}_{1}\n\\]"
  },
  {
    "objectID": "wave.html#neumann-at-xl-is-the-same",
    "href": "wave.html#neumann-at-xl-is-the-same",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann at \\(x=L\\) is the same",
    "text": "Neumann at \\(x=L\\) is the same\n\\[\n\\frac{\\partial u}{\\partial x}(L, t_n) = \\frac{u^{n}_{N+1} - u^n_{N-1}}{2 \\Delta x} = 0 \\rightarrow u^n_{N+1} = u^{n}_{N-1}\n\\]\nThe PDE at the right hand side \\(j=N\\) using ghost cell:\n\\[\nu^{n+1}_N = 2u^{n}_N - u^{n-1}_N + \\underline{c}^2(u^n_{N+1}-2u^n_N+u^n_{N-1})\n\\]\nInsert for \\(u^{n}_{N+1}=u^n_{N-1}\\) and obtain\n\\[\nu^{n+1}_N = 2u^{n}_N - u^{n-1}_N + \\underline{c}^2(2u^n_{N-1}-2u^n_N)\n\\]\n\nAnd for \\(n=1\\) we similarly get\n\\[\nu^{1}_0 = u^{0}_0 +\\frac{\\underline{c}^2}{2}(2u^n_{1}-2u^n_0) \\quad \\text{and} \\quad u^{1}_N = u^{0}_N + \\frac{\\underline{c}^2}{2}(2u^n_{N-1}-2u^n_N)\n\\]"
  },
  {
    "objectID": "wave.html#neumann-summary",
    "href": "wave.html#neumann-summary",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann summary",
    "text": "Neumann summary\nSet \\(u^0=I(x)\\) and define a modified differentiation matrix\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}-2 & \\color{red}2 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}2 & \\color{red}-2 \\\\\n\\end{bmatrix}\n\\]\nNow boundary conditions will be ok at all time steps simply by:\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)"
  },
  {
    "objectID": "wave.html#open-boundary",
    "href": "wave.html#open-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "Open boundary",
    "text": "Open boundary\nThe wave simply disappears through the boundary\n\\[\n\\frac{\\partial u}{\\partial t}(0, t) - c \\frac{\\partial u}{\\partial x}(0, t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial t}(L, t) + c \\frac{\\partial u}{\\partial x}(L, t) = 0\n\\]\nAs for Neumann there are several ways to implement these boundary conditions. The simplest option is to solve the first order accurate\n\\[\n\\frac{u^{n+1}_0-u^{n}_0}{\\Delta t} - c \\frac{u^n_1-u^{n}_{0}}{\\Delta x} = 0\n\\]\nsuch that\n\\[\nu^{n+1}_0 = u^n_0 + \\frac{c \\Delta t}{\\Delta x}(u^n_1-u^n_0)\n\\]"
  },
  {
    "objectID": "wave.html#second-order-option",
    "href": "wave.html#second-order-option",
    "title": "Finite difference methods for the wave equation",
    "section": "Second order option",
    "text": "Second order option\n\\[\n\\frac{u^{n+1}_0-u^{n-1}_0}{2\\Delta t} - c \\frac{-u^n_2+4u^n_1-3u^{n}_{0}}{2 \\Delta x} = 0\n\\]\nSolve for the boundary node \\(u^{n+1}_0\\)\n\\[\nu^{n+1}_0 = u^{n-1}_0 + \\frac{c \\Delta t}{\\Delta x}(-u^n_2+4u^n_1-3u^n_0)\n\\]\nNice option, but difficult to incorporate in the \\(D^{(2)}\\) matrix, since there is no way to modify the first and last rows of \\(D^{(2)}\\) such that\n\\[\nu^{n+1}_0 = 2u^n_0-u^{n-1}_0 + \\underline{c}^2 (D^{(2)} u^n)_0\n\\]"
  },
  {
    "objectID": "wave.html#second-second-order-option",
    "href": "wave.html#second-second-order-option",
    "title": "Finite difference methods for the wave equation",
    "section": "Second second order option",
    "text": "Second second order option\nUse central, second order scheme\n\\[\n\\frac{u^{n+1}_0-u^{n-1}_0}{2\\Delta t} - c \\frac{u^n_1-u^{n}_{-1}}{2 \\Delta x} = 0\n\\]\nand isolate the ghost node \\(u^{n}_{-1}\\):\n\\[\n\\color{red}u^{n}_{-1} \\color{black} = u^n_1 - \\frac{1}{\\underline{c}}(u^{n+1}_0 - u^{n-1}_0)\n\\]\n\nUse regular PDE at the boundary that includes the ghost node:\n\\[\nu^{n+1}_0 = 2u^n_0 - u^{n-1}_0 + \\underline{c}^2(u^{n}_1-2u^n_0+\\color{red}u^n_{-1}\\color{black})\n\\]\nThis gives an equation for \\(u^{n+1}_0\\) that fixes the open boundary condition:\n\\[\nu^{n+1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_1\n\\]"
  },
  {
    "objectID": "wave.html#open-boundary-conditions",
    "href": "wave.html#open-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Open boundary conditions",
    "text": "Open boundary conditions\nLeft boundary:\n\\[\nu^{n+1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_1\n\\]\nRight boundary:\n\\[\nu^{n+1}_N = 2(1-\\underline{c})u^n_N - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_N + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_{N-1}\n\\]\nBoth explicit and second order. But not possible to implement into the matrix such that\n\\[\nu^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\n\\]"
  },
  {
    "objectID": "wave.html#implementation-open-boundaries",
    "href": "wave.html#implementation-open-boundaries",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation open boundaries",
    "text": "Implementation open boundaries\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 {D}^{(2)} u^n\\)\n\\(u^{np1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{nm1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{nm1}_1\\)\n\\(u^{np1}_N = 2(1-\\underline{c})u^n_N - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{nm1}_N + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{nm1}_{N-1}\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)\n\n\n\n\n\n\n\n\nNote\n\n\nThere is no need to use a modified \\(D^{(2)}\\). The two updates of \\(u^{np1}_0\\) and \\(u^{np1}_N\\) will overwrite anything computed in the first step."
  },
  {
    "objectID": "wave.html#periodic-boundary-conditions",
    "href": "wave.html#periodic-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic boundary conditions",
    "text": "Periodic boundary conditions\nA periodic solution is a solution that is repeating itself indefinitely. For example \\(u(x) = \\sin(2\\pi x)\\):\n\n\n\n\n\n\n\n\n\nWe solve the problem for example for \\(x \\in [0, 1]\\), but the actual solution will be like above, with no boundaries.\n\n\n\n\n\n\nNote\n\n\nA periodic domain is also referred to as a domain with no boundaries."
  },
  {
    "objectID": "wave.html#a-periodic-mesh-in-time",
    "href": "wave.html#a-periodic-mesh-in-time",
    "title": "Finite difference methods for the wave equation",
    "section": "A periodic mesh in time",
    "text": "A periodic mesh in time\n\n\\[u(t_N) = u(0) \\quad \\text{or} \\quad u^N = u^0\\]\n\n\n\n\n\n\nNote\n\n\nThere are only \\(N\\) unknowns \\(u^0, u^1, \\ldots, u^{N-1}\\) for a mesh with \\(N+1\\) nodes."
  },
  {
    "objectID": "wave.html#consider-the-discretization-of-u",
    "href": "wave.html#consider-the-discretization-of-u",
    "title": "Finite difference methods for the wave equation",
    "section": "Consider the discretization of \\(u''\\)",
    "text": "Consider the discretization of \\(u''\\)\n\nAt the left hand side of the domain, the point to the left of \\(u^0\\) is \\(u^{N-1}\\)\n\\[\nu''(0) \\approx \\frac{u^1 - 2u^0 + \\color{red}u^{-1}}{h^2} = \\frac{u^1 - 2u^0 + \\color{red}u^{N-1}}{h^2}\n\\]\nAt the right hand side of the domain the point to the right of \\(u^{N-1}\\) is \\(u^N=u^0\\)\n\\[\nu''(t_{N-1}) \\approx \\frac{\\color{red}u^N \\color{black} - 2u^{N-1} + u^{N-2}}{h^2} = \\frac{\\color{red}u^0 \\color{black} - 2u^{N-1} + u^{N-2}}{h^2}\n\\]"
  },
  {
    "objectID": "wave.html#periodic-boundary-conditions-can-be-implemented-in-the-matrix-d2-in-mathbbrn1-times-n1",
    "href": "wave.html#periodic-boundary-conditions-can-be-implemented-in-the-matrix-d2-in-mathbbrn1-times-n1",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic boundary conditions can be implemented in the matrix \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\)",
    "text": "Periodic boundary conditions can be implemented in the matrix \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\)\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}-2 & \\color{red}1 & 0 & 0  & 0 & 0 & \\color{red}1 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 0 & 1 & -2 \\\\\n\\end{bmatrix}\n\\]\nNote that the matrix expects \\(\\boldsymbol{u} = (u^0, u^1, \\ldots, u^{N-1}, u^N)\\), even though \\(u^0=u^N\\). The last row in \\(\\tilde{D}^{(2)}\\) is thus irrelevant, because we wil set \\(u^0=u^N\\) manually."
  },
  {
    "objectID": "wave.html#implementation-periodic-boundaries",
    "href": "wave.html#implementation-periodic-boundaries",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation periodic boundaries",
    "text": "Implementation periodic boundaries\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(1, \\(N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\n\\(u^{np1}_N = u^{np1}_0\\)\nUpdate to next iteration: \\(u^{nm1} = u^n; u^n = u^{np1}\\)"
  },
  {
    "objectID": "wave.html#periodic-wave",
    "href": "wave.html#periodic-wave",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic wave",
    "text": "Periodic wave"
  },
  {
    "objectID": "wave.html#properties-of-the-wave-equation",
    "href": "wave.html#properties-of-the-wave-equation",
    "title": "Finite difference methods for the wave equation",
    "section": "Properties of the wave equation",
    "text": "Properties of the wave equation\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2}\n\\]\nIf the initial condition is \\(u(x, 0)=I(x)\\) and \\(\\frac{\\partial u}{\\partial t}(x, 0)=0\\), then the solution at \\(t&gt;0\\) is\n\\[\nu(x, t) = \\frac{1}{2}\\left( I(x-ct) + I(x+ct)\\right)\n\\]\nThese are two waves - one traveling to the left and the other traveling to the right\n\nIf the initial condition \\(I(x)=e^{i k x}\\), then\n\\[\nu(x, t) = \\frac{1}{2} \\left(e^{ik(x-ct)} + e^{ik(x+ct)} \\right)\n\\]\nis a solution"
  },
  {
    "objectID": "wave.html#representation-of-waves-as-complex-exponentials",
    "href": "wave.html#representation-of-waves-as-complex-exponentials",
    "title": "Finite difference methods for the wave equation",
    "section": "Representation of waves as complex exponentials",
    "text": "Representation of waves as complex exponentials\nIf the initial condition is a sum of waves (superposition, each wave is a solution of the wave equation)\n\\[\nI(x) = \\sum_{k=0}^K a_k e^{i k x} = \\sum_{k=0}^K a_k \\left(\\cos kx + i \\sin kx\\right)\n\\]\nfor some \\(K\\), then the solution is\n\\[\nu(x, t) = \\frac{1}{2}\\sum_{k=0}^K a_k \\left(e^{ik(x-ct)} + e^{ik(x+ct)}\\right)\n\\]\nWe will analyze one component \\(e^{ik(x+ct)} = e^{ikx+ \\omega t}\\), where \\(\\omega = kc\\) is the frequency in time. This is very similar to the investigation we did for the numerical frequency for the vibration equation."
  },
  {
    "objectID": "wave.html#assume-that-the-numerical-solution-is-a-complex-wave",
    "href": "wave.html#assume-that-the-numerical-solution-is-a-complex-wave",
    "title": "Finite difference methods for the wave equation",
    "section": "Assume that the numerical solution is a complex wave",
    "text": "Assume that the numerical solution is a complex wave\n\\[\nu(x_j, t_n) = u^{n}_j = e^{ik(x_j+\\tilde{\\omega} t_n)}\n\\]\n\nHow accurate is \\(\\tilde{\\omega}\\) compared to the exact \\(\\omega=kc\\)?\nWhat can be concluded about stability?\n\n\nNote that the solution is a recurrence relation\n\\[\nu^n_j = e^{ikx_j} e^{i\\tilde{\\omega} n \\Delta t} = (e^{i\\tilde{\\omega} \\Delta t})^n e^{ikx_j}   \n\\]\nwith an amplification factor \\(A = e^{i\\tilde{\\omega} \\Delta t}\\) such that\n\\[\nu^n_j = A^n e^{ikx_j}\n\\]"
  },
  {
    "objectID": "wave.html#numerical-dispersion-relation",
    "href": "wave.html#numerical-dispersion-relation",
    "title": "Finite difference methods for the wave equation",
    "section": "Numerical dispersion relation",
    "text": "Numerical dispersion relation\nWe can find \\(\\tilde{\\omega}\\) by inserting for \\(e^{ik(x_j+\\tilde{\\omega}t_n)}\\) in the discretized wave equation\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\n\nThis is a lot of work, just like it was for the vibration equation. In the end we should get\n\\[\n\\tilde{\\omega} = \\frac{2}{\\Delta t} \\sin^{-1}\\left(C \\sin \\left(\\frac{k \\Delta x}{2}\\right)\\right)\n\\]\nwhere the CFL number is \\(C = \\frac{c \\Delta t}{\\Delta x}\\)\n\n\\(\\tilde{\\omega}(k, c, \\Delta x, \\Delta t)\\) is the numerical dispersion relation\n\\(\\omega = kc\\) is the exact dispersion relation\nWe can compare the two to investigate numerical accuracy and stability"
  },
  {
    "objectID": "wave.html#stability",
    "href": "wave.html#stability",
    "title": "Finite difference methods for the wave equation",
    "section": "Stability",
    "text": "Stability\nA simpler approach is to insert for \\(u^n_j = A^n e^{ikx_j}\\) directly in\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\nand solve for \\(A\\). We get\n\\[\n\\frac{\\left(A^{n+1} - 2A^n + A^{n-1} \\right) e^{ikx_j}}{\\Delta t^2} = c^2 A^n \\frac{e^{ik(x_j+\\Delta x)} - 2e^{ikx_j} + e^{ik(x_j-\\Delta x)}}{\\Delta x^2}\n\\]\n\nDivide by \\(A^ne^{ikx_j}\\), multiply by \\(\\Delta t^2\\) and use \\(C=c\\Delta t/\\Delta x\\) to get\n\\[\nA -2 + A^{-1} = C^2 (e^{ik \\Delta x} - 2 + e^{-ik\\Delta x})\n\\]\ncontinue on next slide"
  },
  {
    "objectID": "wave.html#stability-1",
    "href": "wave.html#stability-1",
    "title": "Finite difference methods for the wave equation",
    "section": "Stability",
    "text": "Stability\n\\[\nA + A^{-1} = 2 + C^2 (e^{ik \\Delta x} - 2 + e^{-ik\\Delta x})\n\\]\nUse \\(e^{ix}+e^{-ix}=2\\cos x\\) to obtain\n\\[\nA + A^{-1} = 2 + 2 C^2(\\cos k\\Delta x - 1)\n\\]\n\nThis is a quadratic equation to solve for A. Using \\(\\beta=2(1+C^2 (\\cos(k\\Delta x)-1))\\) we get that\n\\[\nA = \\frac{\\beta \\pm \\sqrt{\\beta^2-4}}{2}\n\\]\nWe see that \\(|A| = 1\\) for any real numbers \\(-2 \\le \\beta \\le 2\\).\n\n\n\n\n\n\nFor all real numbers \\(-2 \\le \\beta \\le 2\\)\n\n\n\\[\n|\\beta \\pm \\sqrt{\\beta^2-4}| = 2\n\\] since \\(|\\beta \\pm \\sqrt{\\beta^2-4}| = |\\beta + i \\sqrt{4-\\beta^2}| = \\sqrt{\\beta^2 + 4 - \\beta^2} = 2\\)"
  },
  {
    "objectID": "wave.html#for-a-le-1-and-stability-we-need--2-le-beta-le-2-and-thus",
    "href": "wave.html#for-a-le-1-and-stability-we-need--2-le-beta-le-2-and-thus",
    "title": "Finite difference methods for the wave equation",
    "section": "For \\(|A| \\le 1\\) and stability we need \\(-2 \\le \\beta \\le 2\\) and thus",
    "text": "For \\(|A| \\le 1\\) and stability we need \\(-2 \\le \\beta \\le 2\\) and thus\n\\[\n-2 \\le 2(1+C^2(\\cos(k\\Delta x)-1)) \\le 2\n\\]\nRearrange to get that\n\\[\n-2 \\le C^2 (\\cos (k\\Delta x)-1) \\le 0\n\\]\nSince \\(\\cos(k\\Delta x)\\) can at worst be \\(-1\\) we get that the positive real CFL number must be smaller than 1\n\\[\nC \\le 1\n\\]\n\nHence (since \\(C=c\\Delta t/\\Delta x\\)) for stability we require that\n\\[\n\\Delta t \\le  \\frac{\\Delta x}{c}\n\\]"
  },
  {
    "objectID": "wave.html#test-dirichlet-solver-using-cfl1.01-vs-cfl1.0",
    "href": "wave.html#test-dirichlet-solver-using-cfl1.01-vs-cfl1.0",
    "title": "Finite difference methods for the wave equation",
    "section": "Test Dirichlet solver using CFL=1.01 vs CFL=1.0",
    "text": "Test Dirichlet solver using CFL=1.01 vs CFL=1.0\n\n\n\n\n\n\n\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nplotdata = {0: unm1.copy()}\nCFL = 1.01\nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1\n  if n % 10 == 0:\n    plotdata[n] = unp1.copy()"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "",
    "text": "2011-2015 Editor-In-Chief SIAM J of Scientific Computing\nAuthor of 13 published books on scientific computing\nProfessor of Mechanics, University of Oslo 1998\nDeveloped INF5620 (which became IN5270 and now MAT-MEK4270)\nMemorial page"
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example-programming-topics",
    "href": "intro.html#what-to-learn-in-the-start-up-example-programming-topics",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example; programming topics",
    "text": "What to learn in the start-up example; programming topics\n\nHow to verify an implementation and automate verification through tests in Python\nHow to structure code in terms of functions, classes, and modules\nHow to work with Python concepts such as arrays, lists, dictionaries, lambda functions, functions in functions (closures), unit tests, command-line interfaces\nHow to perform array computing and understand the difference from scalar computing. Vectorization."
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example-mathematical-analysis",
    "href": "intro.html#what-to-learn-in-the-start-up-example-mathematical-analysis",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example; mathematical analysis",
    "text": "What to learn in the start-up example; mathematical analysis\n\nHow to uncover numerical artifacts in the computed solution\nHow to analyze the numerical schemes mathematically to understand why artifacts occur\nHow to derive mathematical expressions for various measures of the error in numerical methods, frequently by using the sympy software for symbolic computation\nIntroduce concepts such as finite difference operators, mesh (grid), mesh functions, stability, truncation error, consistency, and convergence"
  },
  {
    "objectID": "intro.html#topics-in-the-first-intro-to-the-finite-difference-method",
    "href": "intro.html#topics-in-the-first-intro-to-the-finite-difference-method",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Topics in the first intro to the finite difference method",
    "text": "Topics in the first intro to the finite difference method\n\nHow to think about finite difference discretization\nKey concepts:\n\nmesh\nmesh function\nfinite difference approximations\n\nThe Forward Euler, Backward Euler, and Crank-Nicolson methods\nFinite difference operator notation\nHow to derive an algorithm and implement it in Python\nHow to test the implementation"
  },
  {
    "objectID": "intro.html#python---pros-and-cons",
    "href": "intro.html#python---pros-and-cons",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Python - pros and cons",
    "text": "Python - pros and cons"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentations for MATMEK-4270",
    "section": "",
    "text": "Lecture 1 - Algorithms and implementations for exponential decay models\nLecture 2 - Analysis of exponential decay models\nLecture 3 - A simple vibration problem\nLecture 4 - The finite difference method\nLecture 5 - Finite difference methods for the wave equation\nLecture 7 - Postprocessing and interpolation"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points-1",
    "href": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points-1",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil makes use of 5 neighboring points",
    "text": "The finite difference stencil makes use of 5 neighboring points\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-is-not-used-at-the-boundary",
    "href": "wave.html#the-finite-difference-stencil-is-not-used-at-the-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil is not used at the boundary",
    "text": "The finite difference stencil is not used at the boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-is-not-used-at-the-spatial-boundary",
    "href": "wave.html#the-finite-difference-stencil-is-not-used-at-the-spatial-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil is not used at the spatial boundary",
    "text": "The finite difference stencil is not used at the spatial boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#ny",
    "href": "wave.html#ny",
    "title": "Finite difference methods for the wave equation",
    "section": "Ny",
    "text": "Ny\n\nThe PDE at the left hand side \\(j=0\\) using ghost cell (same for \\(j=N\\)):\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(u^n_{1}-2u^n_0+u^n_{-1})\n\\]\nInsert for \\(u^{n}_{-1}=u^n_{1}\\) and obtain\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(2u^n_{1}-2u^n_0)\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit. Can be implemented by modifying \\(D^{(2)}\\)!"
  },
  {
    "objectID": "wave.html#use-the-ghost-cell-and-the-pde-to-fix-the-neumann-condition",
    "href": "wave.html#use-the-ghost-cell-and-the-pde-to-fix-the-neumann-condition",
    "title": "Finite difference methods for the wave equation",
    "section": "Use the ghost cell and the PDE to fix the Neumann condition",
    "text": "Use the ghost cell and the PDE to fix the Neumann condition\nThe PDE at the left hand side \\(j=0\\) using ghost cell:\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(u^n_{1}-2u^n_0+u^n_{-1})\n\\]\nInsert for \\(u^{n}_{-1}=u^n_{1}\\) and obtain\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(2u^n_{1}-2u^n_0)\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit. Can be implemented by modifying \\(D^{(2)}\\)!"
  },
  {
    "objectID": "postprocessing.html#postprocessing",
    "href": "postprocessing.html#postprocessing",
    "title": "Postprocessing and Interpolation",
    "section": "Postprocessing",
    "text": "Postprocessing\nWe have computed the solution. Now what?\n\n\nWe analyze the solution!\nWe process the solution into presentable tables and figures\nWe extract important numbers that the solution allows us to compute. For example lift or drag on an airplane wing, or the flux through a boundary.\n\n\n\nIn order to achieve this we need\n\n\nInterpolation - to get the solution everywhere and not just in nodes\nIntegrals - Averages arise from integrals over domains and boundaries\nDerivatives - For example, drag is the integral of the gradient over a boundary \\(\\int_{\\Gamma}\\nu \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\\).\nLagrange interpolation polynomials will help us get there!"
  },
  {
    "objectID": "postprocessing.html#one-dimensional-interpolation",
    "href": "postprocessing.html#one-dimensional-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "One-dimensional interpolation",
    "text": "One-dimensional interpolation\n\\[\nu(x) = x(1-x)\n\\]\n\n\n\n\n\n\n\n\n\n\nWe have the mesh function \\((u(x_i))_{i=0}^{5}\\). What is \\(u(0.75)\\)?\nLinear interpolation requires interpolating between \\(u(x_3)\\) and \\(u(x_4)\\)\n\\[\n\\overline{u}(x) = u_3 + (u_4-u_3)\\frac{x-x_3}{x_4-x_3}.\n\\]"
  },
  {
    "objectID": "postprocessing.html#section",
    "href": "postprocessing.html#section",
    "title": "Postprocessing and Interpolation",
    "section": "",
    "text": "N = 20\nxij, yij = mesh2D(N, N, 1, 1, False)\nu2 = np.cos(xij)*(1-xij)*np.sin(yij)*(1-yij)\nue = sp.cos(x)*(1-x)*sp.sin(y)*(1-y)\n\ndef I(u, dx, dy):\n  um = (u[:-1, :-1]+u[1:, :-1]+u[:-1, 1:]+u[1:, 1:])/4\n  return np.sqrt(np.sum(um**2*dx*dy))\n\nI(u2, 1/N, 1/N)\n\n0.09552823074589575\n\n\nCompared to the exact\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue**2, (y, 0, 1)), (x, 0, 1))))\n\n0.09586332023451888"
  },
  {
    "objectID": "postprocessing.html#linear-interpolation-using-sympy",
    "href": "postprocessing.html#linear-interpolation-using-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Linear interpolation using Sympy",
    "text": "Linear interpolation using Sympy\n\nx = sp.Symbol('x')\nuo = u[3] + (u[4]-u[3])/(xj[4]-xj[3])*(x-xj[3])\n\n\n\n\n\n\n\n\n\n\nLinear interpolation is not very accurate. How do we increase accuracy?"
  },
  {
    "objectID": "postprocessing.html#lagrange-interpolation",
    "href": "postprocessing.html#lagrange-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange interpolation",
    "text": "Lagrange interpolation\nLagrange interpolation is a generic approach that uses any number of points from the mesh function and defines a Lagrange interpolation polynomial\n\\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\nwhere \\(\\{u^j\\}_{j=0}^k\\) are the mesh function values at the chosen \\(k+1\\) mesh points. For the linear example we have just seen\n\\[\n(u^0, u^1) = (u_3, u_4)\n\\]\n\n\n\n\n\n\nNote\n\n\nLagrange mesh function values are given a superscript that starts counting from 0."
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions",
    "href": "postprocessing.html#lagrange-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions",
    "text": "Lagrange basis functions\nThe Lagrange basis functions are defined as\n\\[\n    \\ell_j(x) = \\frac{x-x^0}{x^j-x^0} \\cdots \\frac{x-x^{j-1}}{x^j-x^{j-1}}\\frac{x-x^{j+1}}{x^j-x^{j+1}} \\cdots \\frac{x-x^{k}}{x^j-x^{k}}\n\\]"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions-and-nodes",
    "href": "postprocessing.html#lagrange-basis-functions-and-nodes",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions and nodes",
    "text": "Lagrange basis functions and nodes\nThe Lagrange interpolation polynomial \\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\ncontains \\(k+1\\) basis functions \\(\\{\\ell_j(x)\\}_{j=0}^k\\).\n\nThe basis functions are defined using any number of chosen interpolation nodes \\((x^0, \\ldots, x^k)\\). For our linear example the nodes are simply\n\\[\n(x^0, x^1) = (x_3, x_4)\n\\]\n\n\n\n\n\n\nNote\n\n\nLagrange nodes are given a superscript that starts counting from 0. Choosing different nodes leads to different results!"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions-are-defined-as",
    "href": "postprocessing.html#lagrange-basis-functions-are-defined-as",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions are defined as",
    "text": "Lagrange basis functions are defined as\n\\[\n    \\ell_j(x) = \\frac{x-x^0}{x^j-x^0} \\cdots \\frac{x-x^{j-1}}{x^j-x^{j-1}}\\frac{x-x^{j+1}}{x^j-x^{j+1}} \\cdots \\frac{x-x^{k}}{x^j-x^{k}}\n\\]\nLet that one sink in.\n\n\nNote index \\(j\\) in \\(\\ell_j(x)\\), the \\(j\\)’th basis function.\nNumerator contains the product of all differences \\((x-x^m)\\) for all \\(m\\) except \\(m=j\\)\nDenominator contains the product of all differences \\((x^j-x^m)\\) for all \\(m\\) except \\(m=j\\)\n\n\n\nWe can write\n\\[\n\\ell_j(x) = \\prod_{\\substack{0 \\le m \\le k \\\\ m \\ne j}} \\frac{x-x^m}{x^j-x^m}.\n\\]"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-function-in-sympy",
    "href": "postprocessing.html#lagrange-basis-function-in-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis function in Sympy",
    "text": "Lagrange basis function in Sympy\n\ndef Lagrangebasis(xj, x=x):\n    \"\"\"Construct Lagrange basis function for points in xj\n\n    Parameters\n    ----------\n    xj : array\n        Interpolation points\n    x : Sympy Symbol\n\n    Returns\n    -------\n    Lagrange basis functions as Sympy function \n    \"\"\"\n    from sympy import Mul\n    n = len(xj)\n    ell = []\n    numert = Mul(*[x - xj[i] for i in range(n)])\n\n    for i in range(n):\n        numer = numert/(x - xj[i])\n        denom = Mul(*[(xj[i] - xj[j]) for j in range(n) if i != j])\n        ell.append(numer/denom)\n    return ell"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-in-sympy",
    "href": "postprocessing.html#lagrange-basis-in-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis in Sympy",
    "text": "Lagrange basis in Sympy\nA basis is a collection of basis functions\n\\[\n\\{\\ell_j(x)\\}_{j=0}^k\n\\]\n\ndef Lagrangebasis(xj, x=x):\n  \"\"\"Construct Lagrange basis for points in xj\n\n  Parameters\n  ----------\n  xj : array\n    Interpolation points (nodes)\n  x : Sympy Symbol\n\n  Returns\n  -------\n  Lagrange basis as a list of Sympy functions \n  \"\"\"\n  from sympy import Mul\n  n = len(xj)\n  ell = []\n  numert = Mul(*[x - xj[i] for i in range(n)])\n  for i in range(n):\n    numer = numert/(x - xj[i])\n    denom = Mul(*[(xj[i] - xj[j]) for j in range(n) if i != j])\n    ell.append(numer/denom)\n  return ell"
  },
  {
    "objectID": "postprocessing.html#the-basis-for-linear-interpolation-of-our-first-example-is",
    "href": "postprocessing.html#the-basis-for-linear-interpolation-of-our-first-example-is",
    "title": "Postprocessing and Interpolation",
    "section": "The basis for linear interpolation of our first example is",
    "text": "The basis for linear interpolation of our first example is\n\nell = Lagrangebasis(xj[3:5], x=x)\nell\n\n[4.0 - 5.0*x, 5.0*x - 3.0]\n\n\nWe can plot these two linear functions between \\(x_3\\) and \\(x_4\\)\n\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nxf = np.linspace(xj[3], xj[4], 10)\nplt.plot(xf, sp.lambdify(x, ell[0])(xf), 'g--')\nplt.plot(xf, sp.lambdify(x, ell[1])(xf), 'r--')"
  },
  {
    "objectID": "postprocessing.html#lagrange-interpolation-polynomial",
    "href": "postprocessing.html#lagrange-interpolation-polynomial",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange interpolation polynomial",
    "text": "Lagrange interpolation polynomial\nWith the basis we can now compute the Lagrange interpolation polynomial\n\\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\n\n\n\ndef Lagrangefunction(u, basis):\n  \"\"\"Return Lagrange polynomial\n\n  Parameters\n  ----------\n  u : array\n    Mesh function values\n  basis : tuple of Lagrange basis functions\n    Output from Lagrangebasis\n  \"\"\"\n  f = 0\n  for j, uj in enumerate(u):\n    f += basis[j]*uj\n  return f\n\n\n\nL = Lagrangefunction(u[3:5], ell)\nxf = np.linspace(xj[3]-0.1, xj[4]+0.1, 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#the-great-thing-about-lagrange-polynomials-is-that-we-can-choose-the-number-of-basis-functions",
    "href": "postprocessing.html#the-great-thing-about-lagrange-polynomials-is-that-we-can-choose-the-number-of-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "The great thing about Lagrange polynomials is that we can choose the number of basis functions",
    "text": "The great thing about Lagrange polynomials is that we can choose the number of basis functions\n\n\nChoose \\((x^0, x^1, x^2) = (x_2, x_3, x_4)\\)\n\nell3 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell3)\nxf = np.linspace(xj[2], xj[4], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')\n\n\n\n\n\n\n\n\n\nChoose \\((x^0, x^1, x^2) = (x_3, x_4, x_5)\\)\n\nell3 = Lagrangebasis(xj[3:6], x=x) \nL = Lagrangefunction(u[3:6], ell3)\nxf = np.linspace(xj[3], xj[5], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#we-can-choose-any-and-any-number-of-basis-functions",
    "href": "postprocessing.html#we-can-choose-any-and-any-number-of-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "We can choose any, and any number of basis functions",
    "text": "We can choose any, and any number of basis functions\n\n\nChoose \\((x^0, x^1, x^2) = (x_2, x_3, x_4)\\)\n\nell2 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell2)\nxf = np.linspace(xj[2], xj[4], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')\n\n\n\n\n\n\n\n\n\nChoose \\((x^0, x^1, x^2) = (x_3, x_4, x_5)\\)\n\nell3 = Lagrangebasis(xj[3:6], x=x) \nL = Lagrangefunction(u[3:6], ell3)\nxf = np.linspace(xj[3], xj[5], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#with-three-nodes-the-lagrange-basis-functions-are-second-order-polynomials",
    "href": "postprocessing.html#with-three-nodes-the-lagrange-basis-functions-are-second-order-polynomials",
    "title": "Postprocessing and Interpolation",
    "section": "With three nodes the Lagrange basis functions are second order polynomials",
    "text": "With three nodes the Lagrange basis functions are second order polynomials\n\n\n\nplt.figure(figsize=(6, 4)) \nell2 = Lagrangebasis(xj[2:5], x=x) \nplt.plot(xj, u, 'bo-')\nxl = np.linspace(xj[2], xj[4], 100)\nplt.plot(xl, sp.lambdify(x, ell2[0])(xl), 'r:')\nplt.plot(xl, sp.lambdify(x, ell2[1])(xl), 'g:')\nplt.plot(xl, sp.lambdify(x, ell2[2])(xl), 'b:')\nplt.legend([r'$u(x_i)$', r'$\\ell_0(x)$', r'$\\ell_1(x)$', r'$\\ell_2(x)$'], loc='upper left'); \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll Lagrange basis functions are such that on the chosen mesh points \\(\\{x^{i}\\}_{i=0}^{k}\\) we have\n\\[\n\\ell_j(x^i) = \\delta_{ij} = \\begin{cases} 1 \\text{ for } i=j \\\\\n0 \\text{ for }  i\\ne j\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe Lagrange basis functions do not depend on the mesh function values, only on the mesh points. The mesh points do not need to be uniform, any mesh will do as long as all points are different."
  },
  {
    "objectID": "postprocessing.html#interpolation-of-ux",
    "href": "postprocessing.html#interpolation-of-ux",
    "title": "Postprocessing and Interpolation",
    "section": "Interpolation of \\(u'(x)\\)",
    "text": "Interpolation of \\(u'(x)\\)\nThe solution has been obtained in mesh points \\(\\{u(x_j)\\}_{j=0}^N\\). How can we compute \\(u'(x)\\) for any given \\(x\\)?\n\nCan we use the derivative matrix \\(D^{(1)}\\) and thus\n\\[\nu' \\approx f = D^{(1)} u\n\\]\nsuch that the mesh function \\(f=\\{u'(x_j)\\}_{j=0}^N\\)?\n\n\nYes, but you need to interpolate \\(f\\) just like the mesh function \\(\\{u(x_j)\\}_{j=0}^N\\)\nIs there a better way?\n\n\nYes! Just take the derivative of the Lagrange function\n\\[\nu' \\approx L'(x) = \\sum_{j=0}^k u^j \\ell'_j(x)\n\\]"
  },
  {
    "objectID": "postprocessing.html#interpolation-of-derivatives",
    "href": "postprocessing.html#interpolation-of-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "Interpolation of derivatives",
    "text": "Interpolation of derivatives\nThe solution has been obtained in mesh points \\(\\boldsymbol{u}=(u(x_j))_{j=0}^N\\). How can we compute \\(u'(x)\\) for any given \\(x\\)?\n\nCan we use the derivative matrix \\(D^{(1)}\\) and thus\n\\[\n\\boldsymbol{f} = D^{(1)} \\boldsymbol{u}\n\\]\nsuch that the mesh function \\(\\boldsymbol{f}=(u'(x_j))_{j=0}^N\\)?\n\n\nYes, but you then need to interpolate \\(\\boldsymbol{f}\\)!\nIs there a better way?\n\n\nYes! Just take the derivative of the Lagrange function\n\n\n\\[\nu' \\approx L'(x) = \\sum_{j=0}^k u^j \\ell'_j(x)\n\\]\n\n\nell2 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell2) \nf = L.diff(x, 1)\ndisplay(L)\ndisplay(f)\n\n\\(\\displaystyle 3.0 \\left(x - 0.8\\right) \\left(x - 0.6\\right) - 6.0 \\left(x - 0.8\\right) \\left(x - 0.4\\right) + 2.0 \\left(x - 0.6\\right) \\left(x - 0.4\\right)\\)\n\n\n\\(\\displaystyle 1.0 - 2.0 x\\)"
  },
  {
    "objectID": "postprocessing.html#two-dimensional-interpolation",
    "href": "postprocessing.html#two-dimensional-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "Two-dimensional interpolation",
    "text": "Two-dimensional interpolation\nA solution in 2D is the mesh function\n\\[\nu_{ij} = u(x_i, y_j), \\quad i=0, 1, \\ldots, N \\, \\text{and} \\, j=0, 1, \\ldots, N\n\\]\nHow do we use \\(\\boldsymbol{u} = (u_{ij})_{i,j=0}^N\\) to compute \\(u(x)\\) for any \\(x\\)?\n\n\n\n\n\n\n\n2D interpolation\n\n\nWe need to do interpolation in 2D!"
  },
  {
    "objectID": "postprocessing.html#if-the-problem-is-two-dimensional-then-we-have-the-solution-as-a-mesh-function",
    "href": "postprocessing.html#if-the-problem-is-two-dimensional-then-we-have-the-solution-as-a-mesh-function",
    "title": "Postprocessing and Interpolation",
    "section": "If the problem is two-dimensional, then we have the solution as a mesh function",
    "text": "If the problem is two-dimensional, then we have the solution as a mesh function\n\\[\nu_{ij} = u(x_i, y_j), \\quad i=0, 1, \\ldots, N \\, \\text{and} \\, j=0, 1, \\ldots, N\n\\]\nHow do we use \\(U = (u_{ij})_{i,j=0}^N\\) to compute \\(u(x, y)\\) for any point \\((x, y)\\) in the domain?\n\n\n\n\n\n\n\n2D interpolation\n\n\nWe need to do interpolation in 2D!"
  },
  {
    "objectID": "postprocessing.html#d-interpolation-1",
    "href": "postprocessing.html#d-interpolation-1",
    "title": "Postprocessing and Interpolation",
    "section": "2D-interpolation",
    "text": "2D-interpolation\nWe consider first a simple function\n\\[\nu(x, y) = x(1-x)y(1-y), \\quad x, y \\in \\Omega = [0, 1]^2\n\\]\nand we want to find \\(u(0.55, 0.65)\\) from the mesh function \\((u(x_i, y_j))_{i,j=0}^{10}\\).\n\ndef mesh2D(Nx, Ny, Lx, Ly, sparse=False):\n  x = np.linspace(0, Lx, Nx+1)\n  y = np.linspace(0, Ly, Ny+1)\n  return np.meshgrid(x, y, indexing='ij', sparse=sparse)\n\nN = 10\nxij, yij = mesh2D(N, N, 1, 1, False)\nU = xij*(1-xij)*yij*(1-yij)\nplt.figure(figsize=(3, 3))\nplt.contourf(xij, yij, U)\nplt.plot(0.55, 0.65, 'ro')"
  },
  {
    "objectID": "postprocessing.html#d",
    "href": "postprocessing.html#d",
    "title": "Postprocessing and Interpolation",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d",
    "href": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need to choose interpolation points in 2D",
    "text": "In 2D we need to choose interpolation points in 2D"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d-surrounding-the-point-of-interest",
    "href": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d-surrounding-the-point-of-interest",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need to choose interpolation points in 2D surrounding the point of interest",
    "text": "In 2D we need to choose interpolation points in 2D surrounding the point of interest"
  },
  {
    "objectID": "postprocessing.html#d-lagrange-interpolation-polynomials-makes-use-of-tensor-product-basis-functions",
    "href": "postprocessing.html#d-lagrange-interpolation-polynomials-makes-use-of-tensor-product-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "2D Lagrange interpolation polynomials makes use of tensor product basis functions",
    "text": "2D Lagrange interpolation polynomials makes use of tensor product basis functions\n\\[\nL(x, y) = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\ell_{m}(x) \\ell_{n}(y),\n\\]\nHere one (tensor product) basis function is\n\\[\n\\psi_{mn}(x, y) = \\ell_{m}(x) \\ell_{n}(y)\n\\]\nand\n\\[\nu^{m,n}  \\quad m=0, \\ldots, k \\quad \\text{and} \\quad n=0, \\ldots, l\n\\]\nare the \\((k+1)(l+1)\\) mesh points used for the interpolation.\n\nTwo sets of mesh points: \\((x^0, \\ldots, x^k)\\) and \\((y^0, \\ldots, y^l)\\)\n\\(\\{\\ell_m(x)\\}_{m=0}^k\\) and \\(\\{\\ell_n(y)\\}_{n=0}^l\\) are computed exactly as in 1D."
  },
  {
    "objectID": "postprocessing.html#d-lagrange",
    "href": "postprocessing.html#d-lagrange",
    "title": "Postprocessing and Interpolation",
    "section": "2D Lagrange",
    "text": "2D Lagrange\n\nxij, yij = mesh2D(N, N, 1, 1, False) \ny = sp.Symbol('y')\nlx = Lagrangebasis(xij[5:7, 0], x=x)\nly = Lagrangebasis(yij[0, 6:8], x=y)\n\ndef Lagrangefunction2D(u, basisx, basisy):\n  N, M = u.shape\n  f = 0\n  for i in range(N):\n    for j in range(M):\n      f += basisx[i]*basisy[j]*u[i, j]\n  return f\n\nf = Lagrangefunction2D(U[5:7, 6:8], lx, ly)\nprint('The 2D Lagrange polynomial is:')\nsp.nsimplify(sp.simplify(f), tolerance=1e-8)\n\nThe 2D Lagrange polynomial is:\n\n\n\\(\\displaystyle \\frac{3 x y}{100} - \\frac{21 x}{500} - \\frac{9 y}{100} + \\frac{63}{500}\\)\n\n\nCompare with exact. Not perfect since only linear interpolation.\n\nue = x*(1-x)*y*(1-y)\nprint('Numerical          Exact')\nprint(f.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0551250000000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#d-interpolation-with-more-points",
    "href": "postprocessing.html#d-interpolation-with-more-points",
    "title": "Postprocessing and Interpolation",
    "section": "2D interpolation with more points",
    "text": "2D interpolation with more points\n\nlx = Lagrangebasis(xij[5:8, 0], x=x)\nly = Lagrangebasis(yij[0, 5:8], x=y)\nL2 = Lagrangefunction2D(u2[5:8, 5:8], lx, ly)\nprint('Numerical          Exact')\nprint(L2.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0563062500000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#improve-accuracy-using-more-interpolation-points",
    "href": "postprocessing.html#improve-accuracy-using-more-interpolation-points",
    "title": "Postprocessing and Interpolation",
    "section": "Improve accuracy using more interpolation points",
    "text": "Improve accuracy using more interpolation points\n\nlx = Lagrangebasis(xij[5:8, 0], x=x)\nly = Lagrangebasis(yij[0, 5:8], x=y)\nL2 = Lagrangefunction2D(U[5:8, 5:8], lx, ly)\nprint('Numerical          Exact')\nprint(L2.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0563062500000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-partial-derivatives",
    "href": "postprocessing.html#in-2d-we-need-partial-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need partial derivatives",
    "text": "In 2D we need partial derivatives\nUsing finite difference matrices is possible, but requires interpolation of outcome:\n\\[ \\small\n\\left(\\frac{\\partial u}{\\partial x}(x_i, y_j)\\right)_{i,j=0}^{N} = D^{(1)} U \\quad \\text{and} \\quad \\left(\\frac{\\partial u}{\\partial y}(x_i, y_j)\\right)_{i,j=0}^{N} =  U (D^{(1)})^T.\n\\]\n\nAs in 1D take the derivatives of the Lagrange polynomials:\n\\[ \\small\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial L(x, y)}{\\partial x} = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\frac{\\partial \\ell_{m}(x)}{\\partial x} \\ell_{n}(y)\n\\] \\[ \\small\n\\frac{\\partial u}{\\partial y} = \\frac{\\partial L(x, y)}{\\partial y} = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\ell_{m}(x) \\frac{\\partial \\ell_{n}(y)}{\\partial y}\n\\]\nVerification:\n\ndLx = sp.diff(L2, x)\ndLy = sp.diff(L2, y)\nprint('Numerical          Exact')\nprint(dLx.subs({x: 0.55, y: 0.65}), sp.diff(ue, x).subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n-0.0227500000000000 -0.0227500000000000"
  },
  {
    "objectID": "postprocessing.html#other-tools-for-interpolation-are-available",
    "href": "postprocessing.html#other-tools-for-interpolation-are-available",
    "title": "Postprocessing and Interpolation",
    "section": "Other tools for interpolation are available",
    "text": "Other tools for interpolation are available\nScipy.interpolate\n\nfrom scipy.interpolate import interpn\nprint('Numerical          Exact')\nprint(interpn((xij[5:8, 0], yij[0, 5:8]), U[5:8, 5:8], np.array([0.55, 0.65])), ue.subs({x: 0.55, y: 0.65}))\n\nNumerical          Exact\n[0.055125] 0.0563062500000000\n\n\nDefaults to linear interpolation, so not very accurate.\n\nEasy to use more points and cubic interpolation:\n\nprint('Numerical          Exact') \nprint(interpn((xij[5:9, 0], yij[0, 5:9]), U[5:9, 5:9], np.array([0.55, 0.65]), method='cubic'), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n[0.05630625] 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#how-to-compute-errors-in-2d",
    "href": "postprocessing.html#how-to-compute-errors-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "How to compute errors in 2D",
    "text": "How to compute errors in 2D\nFor the numerical solution \\(u\\) and the exact solution \\(u^{e}\\) the \\(L^2\\) error norm can in general, for any domain \\(\\Omega\\) and number of dimensions, be defined as\n\\[\n\\|u-u^{e}\\|_{L^2(\\Omega)} = \\sqrt{\\int_{\\Omega} (u-u^{e})^2 d\\Omega}.\n\\]\nIn 2D \\(\\Omega = [0, L_x] \\times [0, L_y]\\) and the integral becomes\n\\[\n\\|u-u^{e}\\|_{L^2(\\Omega)} = \\sqrt{\\int_{0}^{L_x}\\int_0^{L_y}(u-u^{e})^2 dy dx}.\n\\]\n\nNumerical integration can be performed using, e.g., the midpoint rule\n\\[\nI(u)  = \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} u((i+0.5)\\Delta x, (j+0.5) \\Delta y) \\Delta x \\Delta y \\approx \\int_{0}^{L_x}\\int_0^{L_y}u(x,y) dy dx.\n\\]"
  },
  {
    "objectID": "postprocessing.html#midpoint-integration",
    "href": "postprocessing.html#midpoint-integration",
    "title": "Postprocessing and Interpolation",
    "section": "Midpoint integration",
    "text": "Midpoint integration\n\n\n\n\n\n\n\n\n\nAll values \\(u(x, y)\\) are required in the center of computational cells and not in nodes.\n\\[\n\\| u-u^{e} \\|_{L^2} = \\sqrt{I((u - u^{e})^2)}\n\\]"
  },
  {
    "objectID": "postprocessing.html#numerical-midpoint-integration-tested",
    "href": "postprocessing.html#numerical-midpoint-integration-tested",
    "title": "Postprocessing and Interpolation",
    "section": "Numerical midpoint integration tested",
    "text": "Numerical midpoint integration tested\n\nN = 20\nxij, yij = mesh2D(N, N, 1, 1, False)\nU = np.cos(xij)*(1-xij)*np.sin(yij)*(1-yij)\nue = sp.cos(x)*(1-x)*sp.sin(y)*(1-y)\n\ndef I(u, dx, dy):\n  um = (u[:-1, :-1]+u[1:, :-1]+u[:-1, 1:]+u[1:, 1:])/4\n  return np.sqrt(np.sum(um*dx*dy))\n\nI(U, 1/N, 1/N)\n\n0.2696557024695919\n\n\nCompared to the exact integral\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue, (y, 0, 1)), (x, 0, 1))).n())\n\n0.26995448271292816\n\n\n\nThe error norm \\(\\|u-u^{e}\\|_{L^2} = \\sqrt{I((u-u^{e})^2)}\\) is\n\nnp.sqrt(I((U-sp.lambdify((x, y), ue)(xij, yij))**2, 1/N, 1/N))\n\n3.1759481967064892e-09\n\n\nwhich is very small because the Lagrange interpolator happens to be close to exact exact for all the midpoints for this function. It is not zero in general."
  },
  {
    "objectID": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-using-the-small-ell2-norm-instead",
    "href": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-using-the-small-ell2-norm-instead",
    "title": "Postprocessing and Interpolation",
    "section": "Just like for the 1D case there is a simplification using the small \\(\\ell^2\\) norm instead",
    "text": "Just like for the 1D case there is a simplification using the small \\(\\ell^2\\) norm instead\n\\[\n\\|(u-u_e)\\|_{\\ell^2} = \\sqrt{ \\Delta x \\Delta y \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} (u_{ij}-u^{e}_{ij})^2 }.\n\\]\nThis is an approximation of the numerical integration using the trapezoidal method\nImplementation:\n\ndef l2_error(u, dx, dy):\n    return np.sqrt(dx*dy*np.sum(u**2))\nl2_error(u2-sp.lambdify((x, y), ue)(xij, yij), 1/N, 1/N)\n\n1.00866469481632e-17"
  },
  {
    "objectID": "postprocessing.html#numerical-integration-can-also-for-example-use-the-trapezoidal-or-simpson-methods",
    "href": "postprocessing.html#numerical-integration-can-also-for-example-use-the-trapezoidal-or-simpson-methods",
    "title": "Postprocessing and Interpolation",
    "section": "Numerical integration can also, for example, use the trapezoidal or Simpson methods",
    "text": "Numerical integration can also, for example, use the trapezoidal or Simpson methods\nTrapezoidal:\n\ndef I_trapz(u, dx, dy):\n  return np.sqrt(np.trapz(np.trapz(u**2, dx=dy, axis=1), dx=dx))\nI_trapz(U, 1/N, 1/N)\n\n0.09592899344305951\n\n\nSimpson’s rule:\n\ndef I_simps(u, dx, dy):\n  from scipy.integrate import simpson as simp\n  return np.sqrt(simp(simp(u**2, dx=dy, axis=1), dx=dx))\nI_simps(U, 1/N, 1/N)\n\n0.09586418448463656\n\n\nExact:\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue**2, (y, 0, 1)), (x, 0, 1))).n())\n\n0.09586332023451888"
  },
  {
    "objectID": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-for-l2-using-the-small-ell2-norm-instead",
    "href": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-for-l2-using-the-small-ell2-norm-instead",
    "title": "Postprocessing and Interpolation",
    "section": "Just like for the 1D case there is a simplification for \\(L^2\\) using the small \\(\\ell^2\\) norm instead",
    "text": "Just like for the 1D case there is a simplification for \\(L^2\\) using the small \\(\\ell^2\\) norm instead\n\\[\n\\|(u-u_e)\\|_{\\ell^2} = \\sqrt{ \\Delta x \\Delta y \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} (u_{ij}-u^{e}_{ij})^2 }.\n\\]\nThis is an approximation of the numerical integration using the trapezoidal method\nImplementation:\n\ndef l2_error(u, dx, dy):\n  return np.sqrt(dx*dy*np.sum(u**2))\nl2_error(U-sp.lambdify((x, y), ue)(xij, yij), 1/N, 1/N)\n\n1.00866469481632e-17"
  },
  {
    "objectID": "postprocessing.html#integration-over-boundary",
    "href": "postprocessing.html#integration-over-boundary",
    "title": "Postprocessing and Interpolation",
    "section": "Integration over boundary",
    "text": "Integration over boundary\nIn many real problems we are not interested in the solution at a point, but rather in average values, or values integrated over a boundary."
  },
  {
    "objectID": "postprocessing.html#our-computational-domain-is-simpler-and-boundary-integrals-are-simpler-as-well",
    "href": "postprocessing.html#our-computational-domain-is-simpler-and-boundary-integrals-are-simpler-as-well",
    "title": "Postprocessing and Interpolation",
    "section": "Our computational domain is simpler and boundary integrals are simpler as well",
    "text": "Our computational domain is simpler and boundary integrals are simpler as well\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrate over boundary at \\(x=L_x\\)\n\\[\n\\int_{0}^{L_y} u(x=L_x, y) dy\n\\]\n\nnp.trapz(U[-1], dx=1/N, axis=0) \n\n0.0\n\n\nIntegrate over boundary at \\(y=L_y\\)\n\\[\n\\int_{0}^{L_x} u(x, y=L_y) dx\n\\]\n\nnp.trapz(U[:, -1], dx=1/N, axis=0) \n\n0.0"
  },
  {
    "objectID": "postprocessing.html#friction-requires-derivatives",
    "href": "postprocessing.html#friction-requires-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "Friction requires derivatives",
    "text": "Friction requires derivatives\n\n\n\n\n\n\n\n\n\n\n\nGradient in normal direction\n\\[\n\\int_{\\Gamma} \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\n\\]\n\nIntegrate gradient at \\(y=0\\)\n\\[\n\\int_{0}^{L_x} \\frac{\\partial u}{\\partial y}(x, y=0) dx\n\\]\n\ndudy = (N/2)*(-3*U[:, 0] + 4*U[:, 1] - U[:, 2])\nnp.trapz(dudy, dx=1/N)\n\n0.4601188642377852\n\n\n\ndudye = sp.diff(ue, y)\nfloat(sp.integrate(dudye.subs(y, 0), (x, 0, 1)).n())\n\n0.4596976941318603\n\n\nIntegrate over boundary at \\(x=L_x\\)\n\\[\n\\int_{0}^{L_y} \\frac{\\partial u}{\\partial x}(x=L_x, y) dy\n\\]\n\ndudx = (N/2)*(3*U[-1] - 4*U[-2] + U[-3])\nnp.trapz(dudx, dx=1/N) \n\n-0.08567622273572727"
  },
  {
    "objectID": "postprocessing.html#friction-requires-derivatives-use-forward-or-backward-stencils",
    "href": "postprocessing.html#friction-requires-derivatives-use-forward-or-backward-stencils",
    "title": "Postprocessing and Interpolation",
    "section": "Friction requires derivatives, use forward or backward stencils",
    "text": "Friction requires derivatives, use forward or backward stencils\n\n\n\n\n\n\n\n\n\n\n\nGradient in normal direction\n\\[\n\\int_{\\Gamma} \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\n\\]\n\nIntegrate gradient at \\(x=1\\)\n\\[\n\\int_{0}^{L_y} \\frac{\\partial u}{\\partial x}(x=L_x, y) dy\n\\]\n\nduy = (N/2)*(-3*u2[:, 0] + 4*u2[:, 1] - u2[:, 2])\nnp.trapz(duy, dx=1/N)\n\n0.4601188642377852\n\n\n\ndudye = sp.diff(ue, y)\nfloat(sp.integrate(dudye.subs(y, 0), (x, 0, 1)).n())\n\n0.4596976941318603\n\n\nIntegrate over boundary at \\(y=1\\)\n\\[\n\\int_{0}^{L_x} \\frac{\\partial u}{\\partial y}(x, y=L_y) dx\n\\]\n\nduy = (N/2)*(-3*u2[0] + 4*u2[1] - u2[2])\nnp.trapz(duy, dx=1/N) \n\n-0.15854507179346725"
  },
  {
    "objectID": "postprocessing.html#two-dimensional-wave-equation-1",
    "href": "postprocessing.html#two-dimensional-wave-equation-1",
    "title": "Postprocessing and Interpolation",
    "section": "Two-dimensional wave equation",
    "text": "Two-dimensional wave equation\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\nabla^2 u, \\quad \\Omega = (0, L_x) \\times (0, L_y), t \\in (0, T]\n\\]\nWe use an initial condition \\(u(x, y, 0) = I(x, y)\\) and \\(\\frac{\\partial u}{\\partial t}(x, y, 0)=0\\) and homogeneous Dirichlet boundary conditions \\(u(x, y, t)=0\\) for the entire boundary.\n\nFor 2D waves we have frequencies in two directions\n\\[\n\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\n\\]\n\n\nAny solution to the wave equation can be written as combinations of waves\n\\[\nu(x, y, t) = g(k_x x + k_y y - |\\boldsymbol{k}|ct)\n\\]\n\n\n\n\n\n\nNote\n\n\nTry to validate by inserting for \\(u(x, y, t)= g(k_x x + k_y y - |\\boldsymbol{k}|ct)\\) into the wave equation. Use for example a variable \\(\\alpha = k_x x + k_y y - |\\boldsymbol{k}|ct\\) and the chain rule, such that \\(\\frac{\\partial g}{\\partial t} = \\frac{\\partial g}{\\partial \\alpha} \\frac{\\partial \\alpha}{\\partial t}\\) etc."
  },
  {
    "objectID": "postprocessing.html#fourier-exponentials-are-solution-in-a-periodic-domain",
    "href": "postprocessing.html#fourier-exponentials-are-solution-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Fourier exponentials are solution in a periodic domain",
    "text": "Fourier exponentials are solution in a periodic domain\n\\[\nu(x, y, t) = e^{\\imath (\\boldsymbol{k} \\cdot \\boldsymbol{x} - \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\)."
  },
  {
    "objectID": "postprocessing.html#fourier-exponentials-are-solutions-in-a-periodic-domain",
    "href": "postprocessing.html#fourier-exponentials-are-solutions-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Fourier exponentials are solutions in a periodic domain",
    "text": "Fourier exponentials are solutions in a periodic domain\n\\[\nu(x, y, t) = e^{\\hat{\\imath} (\\boldsymbol{k} \\cdot \\boldsymbol{x} + \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\) and \\(\\hat{\\imath}=\\sqrt{-1}\\).\n\n\n\n\n\n\nNote\n\n\n\\(\\boldsymbol{x} = x \\boldsymbol{i} + y \\boldsymbol{j}\\) and \\(\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\\), such that \\(\\boldsymbol{k} \\cdot \\boldsymbol{x} = k_x x + k_y y\\).\n\n\n\n\nA mesh solution is then\n\\[\n\\begin{align}\nu^n_{ij} &= u(x_i, y_j, t_n) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y + \\omega n \\Delta t)} \\\\\n&=(e^{\\hat{\\imath} \\omega \\Delta t})^n e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)} \\\\\n&= A^n E(i, j)\n\\end{align}\n\\]\nwith amplification factor \\(A=e^{\\hat{\\imath} \\omega \\Delta t}\\) and \\(E(i, j) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)}\\)."
  },
  {
    "objectID": "postprocessing.html#discretization-of-the-wave-equation-in-2d",
    "href": "postprocessing.html#discretization-of-the-wave-equation-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "Discretization of the wave equation in 2D",
    "text": "Discretization of the wave equation in 2D\nFor all internal points we have the second order accurate \\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nor on vectorized form using \\(U^n = \\Big( u(x_i, y_j, n \\Delta t)\\Big)_{i,j=0}^{N, N}\\) and \\(D_x^{(2)}\\) and \\(D_y^{(2)}\\) as second derivative matrices for \\(x\\) and \\(y\\) directions, respectively\n\\[\n\\frac{U^{n+1}-2U^n+U^{n-1}}{\\Delta t^2} = c^2 \\left( D^{(2)}_x U^n + U^n (D^{(2)}_y)^T \\right).\n\\]"
  },
  {
    "objectID": "postprocessing.html#stability",
    "href": "postprocessing.html#stability",
    "title": "Postprocessing and Interpolation",
    "section": "Stability",
    "text": "Stability\nWe have\n\\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nTo calculate the stability of this scheme we need to insert for \\(u^n_{ij} = A^n E(i, j)\\) and compute the amplification factor \\(A\\). For stability we require\n\\[\n|A| \\le 1\n\\]\nThis is the same stability criterion as used in the 1D case."
  },
  {
    "objectID": "postprocessing.html#stability-1",
    "href": "postprocessing.html#stability-1",
    "title": "Postprocessing and Interpolation",
    "section": "Stability",
    "text": "Stability\n\\[\nA - 2 + A^{-1} =\nc^2 \\Delta t^2 \\left( \\frac{ e^{\\hat{\\imath} k_x \\Delta x} - 2 + e^{- \\hat{\\imath} k_x \\Delta x} }{\\Delta x^2} + \\frac{ e^{\\hat{\\imath} k_y \\Delta y} - 2 + e^{- \\hat{\\imath} k_y \\Delta y} }{\\Delta y^2} \\right)\n\\]\ncan be written as\n\\[\nA + A^{-1} = \\beta\n\\]\nwith\n\\[\n\\beta =  2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right)\n\\]\nwhere we have used \\(e^{\\hat{\\imath} x} + e^{-\\hat{\\imath} x} = 2\\cos x\\).\n\n\n\n\n\n\nNote\n\n\nThe derivation is more or less exactly as for the wave equation in 1D."
  },
  {
    "objectID": "postprocessing.html#from-lecture-5-we-know-that",
    "href": "postprocessing.html#from-lecture-5-we-know-that",
    "title": "Postprocessing and Interpolation",
    "section": "From lecture 5 we know that",
    "text": "From lecture 5 we know that\n\\[\nA + A^{-1} = \\beta\n\\]\nimplies that \\(|A|=1\\) when\n\\[\n-2 \\le \\beta \\le 2\n\\]"
  },
  {
    "objectID": "postprocessing.html#from-lecture-5-we-remember-that",
    "href": "postprocessing.html#from-lecture-5-we-remember-that",
    "title": "Postprocessing and Interpolation",
    "section": "From lecture 5 we remember that",
    "text": "From lecture 5 we remember that\n\\[\nA + A^{-1} = \\beta\n\\]\nimplies that \\(|A|=1\\) when\n\\[\n-2 \\le \\beta \\le 2\n\\]\nHence, for stability we need\n\\[\n-2 \\le 2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 2\n\\]\nor\n\\[\n-2 \\le c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 0\n\\]"
  },
  {
    "objectID": "postprocessing.html#stability-limit-on-time-step",
    "href": "postprocessing.html#stability-limit-on-time-step",
    "title": "Postprocessing and Interpolation",
    "section": "Stability limit on time step",
    "text": "Stability limit on time step\nSince \\(\\cos (k_x \\Delta x)\\) and \\(\\cos (k_y \\Delta y)\\) are at worst \\(-1\\) each, we get from \\[ \\small\n-2 \\le c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 0\n\\]\nthat\n\\[ \\small\n-2 \\le c^2 \\Delta t^2 \\left( \\frac{-2}{\\Delta x^2} + \\frac{-2}{\\Delta y^2} \\right)\n\\]\nwhich is simplified further into\n\\[ \\small\n\\left(\\frac{c \\Delta t}{\\Delta x} \\right)^2 + \\left(\\frac{c \\Delta t}{\\Delta y} \\right)^2 \\le 1\n\\]\nThis is the stability limit on \\(\\Delta t\\) for the 2D wave equation. If \\(\\Delta x = \\Delta y = h\\), then\n\\[ \\small\n\\frac{c \\Delta t}{h} \\le \\frac{1}{\\sqrt{2}}\n\\]"
  },
  {
    "objectID": "postprocessing.html#insert-for-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "href": "postprocessing.html#insert-for-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Insert for \\(u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation",
    "text": "Insert for \\(u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation\n\\[ \\small\n\\begin{equation}\n\\begin{split}\n\\frac{(A^{n+1} - 2A^n + A^{n-1})E(i, j)}{\\Delta t^2} &=\nc^2 A^n \\Big\\{ \\\\\n    & \\frac{ E(i+1, j) - 2E(i, j) +  E(i-1, j)}{\\Delta x^2} \\\\\n+ & \\frac{E(i, j+1) - 2E(i, j) + e(i, j-1)}{\\Delta y^2} \\Big\\}\n\\end{split}\n\\end{equation}\n\\]\nDivide by \\(A^n E(i, j)\\) and multiply by \\(\\Delta t^2\\) in order to find \\(A\\). We also use that, e.g.,\n\\[ \\small\n\\frac{E(i+1, j)}{E(i, j)} = \\frac{e^{\\hat{\\imath}((i+1)k_x \\Delta x + j k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{\\hat{\\imath} k_x \\Delta x}\n\\]\n\\[ \\small\n\\frac{E(i, j-1)}{E(i, j)} = \\frac{e^{\\hat{\\imath}(ik_x \\Delta x + (j-1) k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{- \\hat{\\imath} k_y \\Delta y}\n\\]"
  },
  {
    "objectID": "postprocessing.html#after-some-manipulations-we-get",
    "href": "postprocessing.html#after-some-manipulations-we-get",
    "title": "Postprocessing and Interpolation",
    "section": "After some manipulations we get",
    "text": "After some manipulations we get\n\\[\nA - 2 + A^{-1} =\nc^2 \\Delta t^2 \\left( \\frac{ e^{\\hat{\\imath} k_x \\Delta x} - 2 + e^{- \\hat{\\imath} k_x \\Delta x} }{\\Delta x^2} + \\frac{ e^{\\hat{\\imath} k_y \\Delta y} - 2 + e^{- \\hat{\\imath} k_y \\Delta y} }{\\Delta y^2} \\right)\n\\]\nwhich can be written as\n\\[\nA + A^{-1} = \\beta\n\\]\nwith\n\\[\n\\beta =  2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right)\n\\]\nwhere we have used \\(e^{\\hat{\\imath} x} + e^{-\\hat{\\imath} x} = 2\\cos x\\).\n\n\n\n\n\n\nNote\n\n\nThe derivation is more or less exactly as for the wave equation in 1D."
  },
  {
    "objectID": "postprocessing.html#implementation",
    "href": "postprocessing.html#implementation",
    "title": "Postprocessing and Interpolation",
    "section": "Implementation",
    "text": "Implementation\ndef D2(N):\n  D = sparse.diags([1, -2, 1], [-1, 0, 1], (N+1, N+1), 'lil')\n  D[0, :4] = 2, -5, 4, -1\n  D[-1, -4:] = -1, 4, -5, 2\n  return D\n\ndef solver(N, L, Nt, cfl=0.5, c=1, store_data=10, u0=lambda x, y: np.exp(-40*((x-0.6)**2+(y-0.5)**2))):\n  xij, yij = mesh2D(N, N, L, L)\n  Unp1, Un, Unm1 = np.zeros((3, N+1, N+1))\n  Unm1[:] = u0(xij, yij)\n  dx = L / N\n  D = D2(N)/dx**2\n  dt = cfl*dx/c\n  Un[:] = Unm1[:] + 0.5*(c*dt)**2*(D @ Un + Un @ D.T)\n  plotdata = {0: Unm1.copy()}\n  for n in range(1, Nt):\n    Unp1[:] = 2*Un - Unm1 + (c*dt)**2*(D @ Un + Un @ D.T)\n    # Set boundary conditions\n    # Swap solutions\n    # Store plotdata\n  return xij, yij, plotdata"
  },
  {
    "objectID": "postprocessing.html#test-solver",
    "href": "postprocessing.html#test-solver",
    "title": "Postprocessing and Interpolation",
    "section": "Test solver",
    "text": "Test solver\nCFL = 0.5\nxij, yij, data = solver(40, 1, 501, cfl=CFL, store_data=5)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nframes = []\nfor n, val in data.items():\n  frame = ax.plot_wireframe(xij, yij, val, rstride=2, cstride=2);\n  frames.append([frame])\n\nani = animation.ArtistAnimation(fig, frames, interval=400, blit=True,  repeat_delay=1000)"
  },
  {
    "objectID": "postprocessing.html#test-solver-with-slightly-higher-than-allowed-cfl",
    "href": "postprocessing.html#test-solver-with-slightly-higher-than-allowed-cfl",
    "title": "Postprocessing and Interpolation",
    "section": "Test solver with slightly higher than allowed CFL",
    "text": "Test solver with slightly higher than allowed CFL\nCFL = 0.71\nxij, yij, data = solver(40, 1, 171, cfl=CFL, store_data=5)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nframes = []\nfor n, val in data.items():\n  frame = ax.plot_wireframe(xij, yij, val, rstride=2, cstride=2);\n  frames.append([frame])\n\nani = animation.ArtistAnimation(fig, frames, interval=400, blit=True,  repeat_delay=1000)"
  },
  {
    "objectID": "postprocessing.html#stability-considerations-make-use-of-fourier-exponentials-as-solutions-in-a-periodic-domain",
    "href": "postprocessing.html#stability-considerations-make-use-of-fourier-exponentials-as-solutions-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Stability considerations make use of Fourier exponentials as solutions in a periodic domain",
    "text": "Stability considerations make use of Fourier exponentials as solutions in a periodic domain\n\\[\nu(x, y, t) = e^{\\hat{\\imath} (\\boldsymbol{k} \\cdot \\boldsymbol{x} + \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\) and \\(\\hat{\\imath}=\\sqrt{-1}\\).\n\n\n\n\n\n\nNote\n\n\n\\(\\boldsymbol{x} = x \\boldsymbol{i} + y \\boldsymbol{j}\\) and \\(\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\\), such that \\(\\boldsymbol{k} \\cdot \\boldsymbol{x} = k_x x + k_y y\\).\n\n\n\n\nA mesh solution is then\n\\[\n\\begin{align}\nu^n_{ij} &= u(x_i, y_j, t_n) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y + \\omega n \\Delta t)} \\\\\n&=(e^{\\hat{\\imath} \\omega \\Delta t})^n e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)} \\\\\n&= A^n E(i, j)\n\\end{align}\n\\]\nwith amplification factor \\(A=e^{\\hat{\\imath} \\omega \\Delta t}\\) and \\(E(i, j) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)}\\)."
  },
  {
    "objectID": "postprocessing.html#stability-of-discretization",
    "href": "postprocessing.html#stability-of-discretization",
    "title": "Postprocessing and Interpolation",
    "section": "Stability of discretization",
    "text": "Stability of discretization\nWe have\n\\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nTo calculate the stability of this scheme we need to insert for \\(u^n_{ij} = A^n E(i, j)\\) and compute the amplification factor \\(A\\). For stability we require\n\\[\n|A| \\le 1\n\\]\nThis is the same stability criterion as used in the 1D case."
  },
  {
    "objectID": "postprocessing.html#how-to-specify-initial-conditions",
    "href": "postprocessing.html#how-to-specify-initial-conditions",
    "title": "Postprocessing and Interpolation",
    "section": "How to specify initial conditions",
    "text": "How to specify initial conditions\nThe initial condition \\(\\frac{\\partial u}{\\partial t}(x, y, 0) = 0\\) can be implemented like we did in lecture 5 for the wave equation with one spatial dimension. We use a ghost node:\n\\[\n\\frac{\\partial u}{\\partial t}(x, y, t=0) = 0 = \\frac{U^1-U^{-1}}{2 \\Delta t} \\rightarrow U^1 = U^{-1},\n\\]\nAnd we use the PDE for \\(n=0\\)\n\\[\n\\frac{U^{1}-2U^0+U^{-1}}{\\Delta t^2} = c^2 \\left( D^{(2)}_x U^0 + U^0 (D^{(2)}_y)^T \\right),\n\\]\nsuch that\n\\[\nU^1 = U^0 + \\frac{c^2 \\Delta t^2}{2} \\left( D^{(2)}_x U^0 + U^0 (D^{(2)}_y)^T \\right).\n\\]"
  },
  {
    "objectID": "postprocessing.html#ths-solution-algorithm-for-the-2d-wave-equation",
    "href": "postprocessing.html#ths-solution-algorithm-for-the-2d-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Ths solution algorithm for the 2D wave equation",
    "text": "Ths solution algorithm for the 2D wave equation\n\nSpecify \\(U^0\\) and \\(U^1\\) from initial conditions\nfor n in (1, 2, …, \\(N_t-1\\)) compute\n\n\\(U^{n+1} = 2U^n - U^{n-1} + (c\\Delta t)^2 \\left( D^{(2)}_x U^n + U^n (D^{(2)}_y)^T \\right)\\)\nApply boundary conditions to \\(U^{n+1}\\)\nSwap \\(U^{n-1} \\leftarrow U^n\\) and \\(U^n \\leftarrow U^{n+1}\\) if using only three solution vectors"
  },
  {
    "objectID": "postprocessing.html#insert-for-small-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "href": "postprocessing.html#insert-for-small-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Insert for \\(\\small u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation",
    "text": "Insert for \\(\\small u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation\n\\[ \\small\n\\begin{equation}\n\\begin{split}\n\\frac{(A^{n+1} - 2A^n + A^{n-1})E(i, j)}{\\Delta t^2} &=\nc^2 A^n \\Big\\{ \\\\\n    & \\frac{ E(i+1, j) - 2E(i, j) +  E(i-1, j)}{\\Delta x^2} \\\\\n+ & \\frac{E(i, j+1) - 2E(i, j) + E(i, j-1)}{\\Delta y^2} \\Big\\}\n\\end{split}\n\\end{equation}\n\\]\nDivide by \\(A^n E(i, j)\\) and multiply by \\(\\Delta t^2\\) in order to find \\(A\\). We also use that, e.g.,\n\\[ \\small\n\\frac{E(i+1, j)}{E(i, j)} = \\frac{e^{\\hat{\\imath}((i+1)k_x \\Delta x + j k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{\\hat{\\imath} k_x \\Delta x}\n\\]\n\\[ \\small\n\\frac{E(i, j-1)}{E(i, j)} = \\frac{e^{\\hat{\\imath}(ik_x \\Delta x + (j-1) k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{- \\hat{\\imath} k_y \\Delta y}\n\\]"
  }
]