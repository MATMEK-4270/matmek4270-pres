[
  {
    "objectID": "vibration.html#a-simple-vibration-problem",
    "href": "vibration.html#a-simple-vibration-problem",
    "title": "A simple vibration problem",
    "section": "A simple vibration problem",
    "text": "A simple vibration problem\n\n\n\nThe vibration equation is given as\n\\[\nu^{\\prime\\prime}(t) + \\omega^2u(t) = 0,\\quad u(0)=I,\\ u^{\\prime}(0)=0,\\ t\\in (0,T]\n\\]\nand the exact solution is:\n\\[\nu(t) = I\\cos (\\omega t)\n\\]\n\n\\(u(t)\\) oscillates with constant amplitude \\(I\\) and (angular) frequency \\(\\omega\\).\nPeriod: \\(P=2\\pi/\\omega\\). The period is the time between two neighboring peaks in the cosine function."
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-1-and-2",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-1-and-2",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 1 and 2",
    "text": "A centered finite difference scheme; step 1 and 2\n\nStrategy: follow the four steps of the finite difference method.\nStep 1: Introduce a time mesh, here uniform on \\([0,T]\\): \\[\nt_n=n\\Delta t, \\quad n=0, 1, \\ldots, N_t\n\\]\n\n\n\nStep 2: Let the ODE be satisfied at each mesh point minus 2 boundary conditions:\n\n\\[\nu^{\\prime\\prime}(t_n) + \\omega^2u(t_n) = 0,\\quad n=2,\\ldots,N_t\n\\]"
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-3",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-3",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 3",
    "text": "A centered finite difference scheme; step 3\nStep 3: Approximate derivative(s) by finite difference approximation(s). Very common (standard!) formula for \\(u^{\\prime\\prime}\\):\n\\[\nu^{\\prime\\prime}(t_n) \\approx \\frac{u^{n+1}-2u^n + u^{n-1}}{\\Delta t^2}\n\\]\n\nInsert into vibration ODE:\n\\[\n\\frac{u^{n+1}-2u^n + u^{n-1}}{\\Delta t^2} = -\\omega^2 u^n\n\\]"
  },
  {
    "objectID": "vibration.html#a-centered-finite-difference-scheme-step-4",
    "href": "vibration.html#a-centered-finite-difference-scheme-step-4",
    "title": "A simple vibration problem",
    "section": "A centered finite difference scheme; step 4",
    "text": "A centered finite difference scheme; step 4\nStep 4: Formulate the computational algorithm. Assume \\(u^{n-1}\\) and \\(u^n\\) are known, solve for unknown \\(u^{n+1}\\):\n\\[\nu^{n+1} = 2u^n - u^{n-1} - \\Delta t^2\\omega^2 u^n\n\\]\nNick names for this scheme: Störmer’s method or Verlet integration.\n\nThe scheme is a recurrence relation. That is, \\(u^{n+1}\\) is an explicit function of one or more of the solutions at previous time steps \\(u^n, u^{n-1}, \\ldots\\). We will later see implicit schemes where the equation for \\(u^{n+1}\\) depends also on \\(u^{n+2}, u^{n+3}\\) etc."
  },
  {
    "objectID": "vibration.html#computing-the-first-step---option-1",
    "href": "vibration.html#computing-the-first-step---option-1",
    "title": "A simple vibration problem",
    "section": "Computing the first step - option 1",
    "text": "Computing the first step - option 1\n\nThe two initial conditions require that we fix both \\(u^0\\) and \\(u^1\\). How to fix \\(u^1\\)?\n\n\n\nWe cannot use the difference equation \\(u^{1} = 2u^0 - u^{-1} - \\Delta t^2\\omega^2 u^0\\) because \\(u^{-1}\\) is unknown and outside the mesh!\nAnd: we have not used the initial condition \\(u^{\\prime}(0)=0\\)!\n\n\n\nOption 1: Use a forward difference\n\\[\n  \\begin{align*}\n  u^{\\prime}(0) &=  \\frac{u^1-u^0}{\\Delta t}=0  &\\longrightarrow u^1=u^0=I \\\\\n  u^{\\prime}(0) &= \\frac{-u^2+4u^1-3u^0}{2 \\Delta t}=0 \\quad &\\longrightarrow u^1=\\frac{u^2+3u^0}{4}\n  \\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\nFirst is merely first order accurate, second is second order, but implicit (depends on the unknown \\(u^2\\).)"
  },
  {
    "objectID": "vibration.html#computing-the-first-step---option-2",
    "href": "vibration.html#computing-the-first-step---option-2",
    "title": "A simple vibration problem",
    "section": "Computing the first step - option 2",
    "text": "Computing the first step - option 2\nUse the discrete ODE at \\(t=0\\) together with a central difference at \\(t=0\\) and a ghost cell \\(u^{-1}\\). The central difference is\n\\[\n\\frac{u^1-u^{-1}}{2\\Delta t} = 0\\quad\\Rightarrow\\quad u^{-1} = u^1\n\\]\nThe vibration scheme for \\(n=0\\) is\n\\[\nu^{1} = 2u^0 - u^{-1} - \\Delta t^2\\omega^2 u^0\n\\]\nUse \\(u^{-1}=u^1\\) to get\n\\[\nu^1 = u^0 - \\frac{1}{2} \\Delta t^2 \\omega^2 u^0\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit (does not depend on unknown \\(u^2\\))."
  },
  {
    "objectID": "vibration.html#the-computational-algorithm",
    "href": "vibration.html#the-computational-algorithm",
    "title": "A simple vibration problem",
    "section": "The computational algorithm",
    "text": "The computational algorithm\n\n\\(u^0=I\\)\ncompute \\(u^1 = u^0 - \\frac{1}{2} \\Delta t^2 \\omega^2 u^0\\)\nfor \\(n=1, 2, \\ldots, N_t-1\\):\n\ncompute \\(u^{n+1}\\)\n\n\n\nMore precisely expressed in Python:\n\nT = 1\nNt = 10\nI = 1\nw = 4\nt = np.linspace(0, T, Nt+1)  # mesh points in time\ndt = t[1] - t[0]          # constant time step.\nu = np.zeros(Nt+1)           # solution\n\nu[0] = I\nu[1] = u[0] - 0.5*dt**2*w**2*u[0]\nfor n in range(1, Nt):\n    u[n+1] = 2*u[n] - u[n-1] - dt**2*w**2*u[n]\n\nThe code is difficult to vectorize, so we should use Numba or Cython for speed."
  },
  {
    "objectID": "vibration.html#computing-uprime",
    "href": "vibration.html#computing-uprime",
    "title": "A simple vibration problem",
    "section": "Computing \\(u^{\\prime}\\)",
    "text": "Computing \\(u^{\\prime}\\)\n\\(u\\) is often displacement/position, \\(u^{\\prime}\\) is velocity and can be computed by\n\\[\nu^{\\prime}(t_n) \\approx \\frac{u^{n+1}-u^{n-1}}{2\\Delta t}\n\\]\n\n\n\n\n\n\nNote\n\n\nFor \\(u^{\\prime}(t_0)\\) and \\(u^{\\prime}(t_{N_t})\\) it is possible to use forward or backwards differences, respectively. However, we already know from initial conditions that \\(u^{\\prime}(t_0) = 0\\) so no need to use finite difference there.\n\n\n\n\nWith vectorization:\n\ndu = np.zeros(Nt+1)\ndu[1:-1] = (u[2:] - u[:-2]) / (2*dt)  # second order accurate\ndu[0] = 0                             # exact from initial condition\ndu[-1] = (u[-1]-u[-2]) / dt           # first order accurate"
  },
  {
    "objectID": "vibration.html#implementation",
    "href": "vibration.html#implementation",
    "title": "A simple vibration problem",
    "section": "Implementation",
    "text": "Implementation\n\ndef solver(I, w, dt, T):\n    \"\"\"\n    Solve u'' + w**2*u = 0 for t in (0,T], u(0)=I and u'(0)=0,\n    by a central finite difference method with time step dt.\n    \"\"\"\n    Nt = int(round(T/dt))\n    u = np.zeros(Nt+1)\n    t = np.linspace(0, Nt*dt, Nt+1)\n\n    u[0] = I\n    u[1] = u[0] - 0.5*dt**2*w**2*u[0]\n    for n in range(1, Nt):\n        u[n+1] = 2*u[n] - u[n-1] - dt**2*w**2*u[n]\n    return u, t\n \ndef u_exact(t, I, w):\n    return I*np.cos(w*t)"
  },
  {
    "objectID": "vibration.html#visualization",
    "href": "vibration.html#visualization",
    "title": "A simple vibration problem",
    "section": "Visualization",
    "text": "Visualization\n\ndef visualize(u, t, I, w):\n    plt.plot(t, u, 'b-o')\n    t_fine = np.linspace(0, t[-1], 1001)  # very fine mesh for u_e\n    u_e = u_exact(t_fine, I, w)\n    plt.plot(t_fine, u_e, 'r--')\n    plt.legend(['numerical', 'exact'], loc='upper left')\n    plt.xlabel('t')\n    plt.ylabel('u(t)')\n    dt = t[1] - t[0]\n    plt.title('dt=%g' % dt)\n    umin = 1.2*u.min();  umax = -umin\n    plt.axis([t[0], t[-1], umin, umax])"
  },
  {
    "objectID": "vibration.html#main-program",
    "href": "vibration.html#main-program",
    "title": "A simple vibration problem",
    "section": "Main program",
    "text": "Main program\n\nI = 1\nw = 2*np.pi\nnum_periods = 6\nP = 2*np.pi/w    #  one period\nT = P*num_periods\ndt = 1/w\nu, t = solver(I, w, dt, T)\nvisualize(u, t, I, w)"
  },
  {
    "objectID": "vibration.html#various-timestep",
    "href": "vibration.html#various-timestep",
    "title": "A simple vibration problem",
    "section": "Various timestep",
    "text": "Various timestep\n\n\n\n\n\n\n\n\n\nWe see that \\(\\Delta t = 2/\\omega\\) is a limit. Using longer \\(\\Delta t\\) leads to growth. Why?"
  },
  {
    "objectID": "vibration.html#mathematical-analysis",
    "href": "vibration.html#mathematical-analysis",
    "title": "A simple vibration problem",
    "section": "Mathematical analysis",
    "text": "Mathematical analysis\nThe exact solution to the continuous vibration equation is \\(u_e(t) = I \\cos (\\omega t)\\)\nThe key to study the numerical solution is knowing that linear difference equations like\n\\[\nu^{n+1} = (2-\\Delta t^2\\omega^2) u^n - u^{n-1}\n\\]\nadmit discrete (numerical) solutions of the form\n\\[\nu^{n+1} = A u^n  \\quad \\text{or} \\quad u^n = A^n I\n\\]\nwhere \\(I\\) is an initial condition.\n\nSo the recurrence relation (the difference equation) have solutions \\(u^n\\) that may be computed recursively\n\ndef u(n, I=2., A=0.5):\n  if n &gt; 0:\n    return A * u(n-1, I, A)\n  return I\nprint(u(0), u(1), u(2), u(3))\n\n2.0 1.0 0.5 0.25"
  },
  {
    "objectID": "vibration.html#exact-discrete-solution",
    "href": "vibration.html#exact-discrete-solution",
    "title": "A simple vibration problem",
    "section": "Exact discrete solution",
    "text": "Exact discrete solution\nWe now have (at least) two possibilities\n\nAssume that \\(A=e^{i \\tilde{\\omega} \\Delta t}\\) and solve for the numerical frequency \\(\\tilde{\\omega}\\)\nAssume nothing and compute with a function A(\\(\\Delta t, \\omega\\)) (like for the exponential decay)\n\n\nWe follow Langtangen’s approach (1) first. Note that since\n\\[\ne^{i \\tilde{\\omega} \\Delta t} = \\cos (\\tilde{\\omega} \\Delta t ) + i \\sin(\\tilde{\\omega} \\Delta t)\n\\]\nwe can work with a complex A and let the real part represent the physical solution.\nThe exact discrete solution is then\n\\[\nu(t_n) = I \\cos (\\tilde{\\omega} t_n)\n\\]\nand we can study the error in \\(\\tilde{\\omega}\\) compared to the true \\(\\omega\\)."
  },
  {
    "objectID": "vibration.html#find-the-error-in-tildeomega",
    "href": "vibration.html#find-the-error-in-tildeomega",
    "title": "A simple vibration problem",
    "section": "Find the error in \\(\\tilde\\omega\\)",
    "text": "Find the error in \\(\\tilde\\omega\\)\nInsert the numerical solution \\(u^n = I \\cos (\\tilde{\\omega} t_n)\\) into the discrete equation\n\\[\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0\n\\]\n\nQuite messy, but Wolfram Alpha (or another long derivation in the FD for PDEs book) will give you\n\\[\n\\begin{align}\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} &= \\frac{I}{\\Delta t^2} (\\cos (\\tilde{\\omega} t_{n+1}) - 2 \\cos (\\tilde{\\omega} t_n) + \\cos (\\tilde{\\omega} t_{n-1})) \\\\\n&= \\frac{2 I}{\\Delta t^2} (\\cos (\\tilde{\\omega} \\Delta t) - 1) \\cos (\\tilde{\\omega} n \\Delta t) \\\\\n&= -\\frac{4}{\\Delta t^2} \\sin^2 (\\tilde{\\omega} \\Delta t) \\cos (\\tilde{\\omega} n \\Delta t)\n\\end{align}\n\\]"
  },
  {
    "objectID": "vibration.html#insert-into-discrete-equation",
    "href": "vibration.html#insert-into-discrete-equation",
    "title": "A simple vibration problem",
    "section": "Insert into discrete equation",
    "text": "Insert into discrete equation\n\\[\n\\frac{u^{n+1} - 2u^n + u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0\n\\]\nWe get\n\\[\n-\\frac{4}{\\Delta t^2} \\sin^2 (\\tilde{\\omega} \\Delta t) { \\cos (\\tilde{\\omega} n \\Delta t)} + { \\omega^2 { \\cos (\\tilde{\\omega} n \\Delta t)}} = 0\n\\]\nand thus\n\\[\n\\omega^2 = \\frac{4}{\\Delta t^2} \\sin^2 \\left( \\frac{\\tilde{\\omega} \\Delta t}{2} \\right)\n\\]\nSolve for \\(\\tilde{\\omega}\\) by taking the root and using \\(\\sin^{-1}\\)"
  },
  {
    "objectID": "vibration.html#numerical-frequency",
    "href": "vibration.html#numerical-frequency",
    "title": "A simple vibration problem",
    "section": "Numerical frequency",
    "text": "Numerical frequency\n\\[\n\\tilde{\\omega} = \\pm \\frac{2}{\\Delta t} \\sin^{-1} \\left( \\frac{\\omega  \\Delta t }{2} \\right)\n\\]\n\n\nThere is always a frequency error because \\(\\tilde{\\omega} \\neq \\omega\\).\nThe dimensionless number \\(p=\\omega\\Delta t\\) is the key parameter (i.e., no of time intervals per period is important, not \\(\\Delta t\\) itself. Remember \\(P=2\\pi/w\\) and thus \\(p=2\\pi / P\\))\nHow good is the approximation \\(\\tilde{\\omega}\\) to \\(\\omega\\)?\nDoes it possibly lead to growth? \\(|A|&gt;1\\)."
  },
  {
    "objectID": "vibration.html#polynomial-approximation-of-the-frequency-error",
    "href": "vibration.html#polynomial-approximation-of-the-frequency-error",
    "title": "A simple vibration problem",
    "section": "Polynomial approximation of the frequency error",
    "text": "Polynomial approximation of the frequency error\nHow good is the approximation \\(\\tilde{\\omega} = \\pm \\frac{2}{\\Delta t} \\sin^{-1} \\left( \\frac{\\omega  \\Delta t }{2} \\right)\\) ?\n\nWe can easily use a Taylor series expansion for small \\(h=\\Delta t\\)\n\nimport sympy as sp \nh, w = sp.symbols('h,w')\nw_tilde = sp.asin(w*h/2).series(h, 0, 4)*2/h\nsp.simplify(w_tilde)\n\n\\(\\displaystyle w + \\frac{h^{2} w^{3}}{24} + O\\left(h^{3}\\right)\\)\n\n\n\n\nSo the numerical frequency i always too large (to fast oscillations): \\[\n\\tilde\\omega = \\omega\\left( 1 + \\frac{1}{24}\\omega^2\\Delta t^2\\right) + {\\cal O}(\\Delta t^3)\n\\]"
  },
  {
    "objectID": "vibration.html#simple-improvement-of-previous-solver",
    "href": "vibration.html#simple-improvement-of-previous-solver",
    "title": "A simple vibration problem",
    "section": "Simple improvement of previous solver",
    "text": "Simple improvement of previous solver\n\n\n\n\n\n\nNote\n\n\nWhat happens if we simply use the frequency \\(\\omega(1-\\omega^2 \\Delta t^2 /24)\\) instead of just \\(\\omega\\)?\n\n\n\n\nThe leading order numerical error disappears and we get\n\\[\n\\tilde\\omega = \\omega\\left( 1 - \\left(\\frac{1}{24}\\omega^2\\Delta t^2\\right)^2\\right) + \\cdots\n\\]\n\n\n\n\n\n\nNote\n\n\nDirty trick, and only usable when you can compute the numerical error exactly. But fourth order…"
  },
  {
    "objectID": "vibration.html#how-about-the-global-error",
    "href": "vibration.html#how-about-the-global-error",
    "title": "A simple vibration problem",
    "section": "How about the global error?",
    "text": "How about the global error?\n\\[\nu^n = I\\cos\\left(\\tilde\\omega n\\Delta t\\right),\\quad\n\\tilde\\omega = \\frac{2}{\\Delta t}\\sin^{-1}\\left(\\frac{\\omega\\Delta t}{2}\\right)\n\\]\nThe error mesh function,\n\\[\ne^n = u_{e}(t_n) - u^n =\nI\\cos\\left(\\omega n\\Delta t\\right)\n- I\\cos\\left(\\tilde\\omega n\\Delta t\\right)\n\\]\nis ideal for verification and further analysis!\n\n\\[\n\\begin{align*}\ne^n &= I\\cos\\left(\\omega n\\Delta t\\right)\n- I\\cos\\left(\\tilde\\omega n\\Delta t\\right) \\\\\n&= -2I\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega - \\tilde\\omega\\right)\\right)\n\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega + \\tilde\\omega\\right)\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "vibration.html#convergence-of-the-numerical-scheme",
    "href": "vibration.html#convergence-of-the-numerical-scheme",
    "title": "A simple vibration problem",
    "section": "Convergence of the numerical scheme",
    "text": "Convergence of the numerical scheme\nWe can easily show convergence (i.e., \\(e^n\\rightarrow 0 \\hbox{ as }\\Delta t\\rightarrow 0\\)) from what we know about sines in the error\n\\[\ne^n = -2I\\sin\\left(n \\Delta t\\frac{1}{2}\\left( \\omega - \\tilde\\omega\\right)\\right)\n\\sin\\left(n \\Delta t \\frac{1}{2}\\left( \\omega + \\tilde\\omega\\right)\\right)\n\\] and the following limit \\[\n\\lim_{\\Delta t\\rightarrow 0}\n\\tilde\\omega = \\lim_{\\Delta t\\rightarrow 0}\n\\frac{2}{\\Delta t}\\sin^{-1}\\left(\\frac{\\omega\\Delta t}{2}\\right)\n= \\omega\n\\]\n\nThe limit can be computed using L’Hopital’s rule or simply by asking sympy or WolframAlpha. Sympy is easier:\n\nsp.limit((2/h)*sp.asin(w*h/2), h, 0, dir='+')\n\n\\(\\displaystyle w\\)"
  },
  {
    "objectID": "vibration.html#how-about-stability",
    "href": "vibration.html#how-about-stability",
    "title": "A simple vibration problem",
    "section": "How about stability?",
    "text": "How about stability?\n\n\nSolutions are oscillatory, so not a problem that \\(A&lt;0\\), like for the exponential decay\nSolutions should have constant amplitudes (constant \\(I\\)), but we will have growth if \\[\n|A| &gt; 1\n\\]\nConstant amplitude requires \\[\n|A| = |e^{i \\tilde{\\omega} \\Delta t}| = 1\n\\] Is this always satisfied?"
  },
  {
    "objectID": "vibration.html#stability-in-growth",
    "href": "vibration.html#stability-in-growth",
    "title": "A simple vibration problem",
    "section": "Stability in growth?",
    "text": "Stability in growth?\nConsider \\[\n|A| = |e^{i y}| \\quad \\text{where} \\quad y= \\pm 2 \\sin^{-1}\\left(\\frac{\\omega \\Delta t}{2}\\right)\n\\]\nIs \\(|e^{iy}|=1\\) for all \\(y\\)?\n\n\n\nNo! For \\(\\Im(y) &lt; 0\\) we have that \\(|e^{iy}| &gt; 1\\).\n\n\ny = -0.001j\nabs(sp.exp(1j*y)) \n\n\\(\\displaystyle 1.00100050016671\\)\n\n\n\n\n\n\nHow can we get negative \\(\\Im(y)\\)? Can \\(\\Im(\\sin^{-1}(x)) &lt; 0\\) for some \\(x\\)?\n\n\nYes! We can easily check that if \\(|x|&gt;1\\) then \\(\\sin^{-1}(x)\\) has a negative imaginary part:\n\nsp.asin(1.01)\n\n\\(\\displaystyle 1.5707963267949 - 0.141303769485649 i\\)\n\n\n\n\nHence if \\(\\left| \\frac{\\omega \\Delta t}{2} \\right| &gt; 1\\) then we will have growth! For stability \\(\\longrightarrow \\Delta t \\le 2 / w\\) !"
  },
  {
    "objectID": "vibration.html#stability-limit",
    "href": "vibration.html#stability-limit",
    "title": "A simple vibration problem",
    "section": "Stability limit",
    "text": "Stability limit\nSummary: we get\n\\[\n\\left|\\sin^{-1}\\left(\\frac{\\omega \\Delta t}{2} \\right) \\right| &gt; 1\n\\]\nif\n\\[\n\\left| \\frac{\\omega \\Delta t}{2} \\right| &gt; 1\n\\]\nThis happens for\n\\[\n\\Delta t &gt; \\frac{2}{\\omega}\n\\]"
  },
  {
    "objectID": "vibration.html#remember-the-initial-plots",
    "href": "vibration.html#remember-the-initial-plots",
    "title": "A simple vibration problem",
    "section": "Remember the initial plots",
    "text": "Remember the initial plots\n\n\n\n\n\n\n\n\n\nWe have growth for \\(\\Delta t &gt; 2/\\omega\\)."
  },
  {
    "objectID": "vibration.html#about-the-stability-limit",
    "href": "vibration.html#about-the-stability-limit",
    "title": "A simple vibration problem",
    "section": "About the stability limit",
    "text": "About the stability limit\n\n\n\n\n\n\n\n\n\nFor \\(\\Delta t = 2/\\omega\\) there is exactly one timestep between a minimum and a maximum point for the numerical simulation (zigzag pattern). This is absolutely the smallest number of points that can possibly resolve (poorly) a wave of this frequency! So it really does not make sense physically to use larger time steps!"
  },
  {
    "objectID": "vibration.html#alternative-derivation-of-stability-limit",
    "href": "vibration.html#alternative-derivation-of-stability-limit",
    "title": "A simple vibration problem",
    "section": "Alternative derivation of stability limit",
    "text": "Alternative derivation of stability limit\nWe have the difference equation\n\\[\nu^{n+1} = (2-\\Delta t^2\\omega^2) u^n - u^{n-1}\n\\]\nand a numerical solution of the form\n\\[\nu^{n} = A^n I\n\\]\n\nInsert for the numerical solution in the difference equation:\n\\[\nA^{n+1}I = (2-\\Delta t^2 \\omega^2) A^n I - A^{n-1}I\n\\]\n\n\nDivide by \\(I A^{n-1}\\) and rearrange\n\\[\nA^2 - (2-\\Delta t^2 \\omega^2)A + 1 = 0\n\\]"
  },
  {
    "objectID": "vibration.html#alternative-derivation-ctd",
    "href": "vibration.html#alternative-derivation-ctd",
    "title": "A simple vibration problem",
    "section": "Alternative derivation ct’d",
    "text": "Alternative derivation ct’d\nSet \\(p=\\Delta t \\omega\\) and solve second order equation\n\\[\nA = 1 - \\frac{p^2}{2} \\pm \\frac{p}{2}\\sqrt{p^2-4}\n\\]\n\nWe still want \\(|A|=1\\) for constant amplitude and stability. However try \\(p &gt; 2\\) in the above equation (using the minus in front of the last term) and you get\n\\[\nA &lt; -1\n\\]\nCheck:\n\np = sp.Symbol('p') \nf = 1 - p**2/2 - p/2*sp.sqrt(p**2-4)\nf.subs(p, 2.01).n() \n\n\\(\\displaystyle -1.22130109316473\\)"
  },
  {
    "objectID": "vibration.html#alternative-derivation-ctd-1",
    "href": "vibration.html#alternative-derivation-ctd-1",
    "title": "A simple vibration problem",
    "section": "Alternative derivation ct’d",
    "text": "Alternative derivation ct’d\nSet \\(p=\\Delta t \\omega\\) and solve second order equation\n\\[\nA = 1 - \\frac{p^2}{2} \\pm \\frac{p}{2}\\sqrt{p^2-4}\n\\]\nWe still want \\(|A|=1\\) for constant amplitude and stability. However try \\(p &gt; 2\\) in the above equation (using the minus in front of the last term) and you get\n\\[\nA &lt; -1\n\\]\nSo we have growth if \\(p &gt; 2\\), which is the same as \\(\\Delta t \\omega &gt; 2\\) or simply\n\\[\n\\Delta t &gt; \\frac{2}{\\omega}\n\\]\nwhich is the same result as we got using the numerical frequency \\(\\tilde{\\omega}\\)!"
  },
  {
    "objectID": "vibration.html#digression",
    "href": "vibration.html#digression",
    "title": "A simple vibration problem",
    "section": "Digression",
    "text": "Digression\n\n\n\n\n\n\nDigression\n\n\nThis alternative analysis is no different from what we did with the exponential decay. Only the exponential decay was so easy that we did not actually derive the generic A!\n\n\n\n\nConsider the difference equation for exponential decay\n\\[\n\\frac{u^{n+1}-u^{n}}{\\triangle t} = -(1-\\theta)au^{n} - \\theta a u^{n+1}\n\\]\nand assume again that \\(u^n = A^n I\\). Insert this into the above\n\\[\n\\frac{A^{n+1}I-A^{n}I}{\\triangle t} = -(1-\\theta)aA^{n}I - \\theta a A^{n+1}I\n\\]\nDivide by \\(A^n I\\) and rearrange to get the well-known \\(A = \\frac{1-(1-\\theta)\\Delta t a}{1+ \\theta \\Delta t a}\\)"
  },
  {
    "objectID": "vibration.html#key-observations",
    "href": "vibration.html#key-observations",
    "title": "A simple vibration problem",
    "section": "Key observations",
    "text": "Key observations\nWe can draw three important conclusions:\n\n\nThe key parameter in the formulas is \\(p=\\omega\\Delta t\\) or \\(\\tilde{p} = \\tilde{\\omega} \\Delta t\\) (dimensionless)\n\nPeriod of oscillations: \\(P=2\\pi/\\omega\\) vs numerical \\(\\tilde{P}=2 \\pi / \\tilde{\\omega}\\)\nNumber of time steps per period: \\(N_P=P/\\Delta t\\) vs \\(\\tilde{N}_P =\\tilde{P}/\\Delta t\\)\nAt stability limit \\(\\omega = 2/\\Delta t\\) and \\(\\tilde{\\omega}=\\pi/\\Delta t\\) \\(\\Rightarrow P = \\pi \\Delta t\\) and \\(\\tilde{P} = 2 \\Delta t\\). We get \\(\\tilde{N}_P = \\tilde{P}/\\Delta t = 2\\) and the critical parameter is really the number of time steps per numerical period.\n\nFor \\(p\\leq 2\\) the amplitude of \\(u^n\\) is constant (stable solution)\n\\(u^n\\) has a relative frequency error \\(\\tilde\\omega/\\omega \\approx 1 + \\frac{1}{24}p^2\\), making numerical peaks occur too early. This is also called a phase error, or a dispersive error."
  },
  {
    "objectID": "vibration.html#convergence-rates",
    "href": "vibration.html#convergence-rates",
    "title": "A simple vibration problem",
    "section": "Convergence rates",
    "text": "Convergence rates\nLets compute the convergence rate for our solver. However, let it also be possible to choose the numerical frequency \\(\\omega(1-\\omega^2\\Delta t^2/24)\\)\n\ndef solver_adjust(I, w, dt, T, adjust_w=False):\n    Nt = int(round(T/dt))\n    u = np.zeros(Nt+1)\n    t = np.linspace(0, Nt*dt, Nt+1)\n    w_adj = w*(1 - w**2*dt**2/24.) if adjust_w else w\n    u[0] = I\n    u[1] = u[0] - 0.5*dt**2*w_adj**2*u[0]\n    for n in range(1, Nt):\n        u[n+1] = 2*u[n] - u[n-1] - dt**2*w_adj**2*u[n]\n    return u, t\n \ndef u_exact(t, I, w):\n    return I*np.cos(w*t)\n\ndef l2_error(dt, T, w=0.35, I=0.3, adjust_w=False):\n    u, t = solver_adjust(I, w, dt, T, adjust_w)\n    ue = u_exact(t, I, w)\n    return np.sqrt(dt*np.sum((ue-u)**2))"
  },
  {
    "objectID": "vibration.html#convergence-rates-1",
    "href": "vibration.html#convergence-rates-1",
    "title": "A simple vibration problem",
    "section": "Convergence rates",
    "text": "Convergence rates\nWe compute the order of the convergence in the same manner as lecture 2\n\\[\nr = \\frac{\\log {\\frac{E_{i-1}}{E_i}}}{\\log {\\frac{\\Delta t_{i-1}}{\\Delta t_i}}}\n\\]\n\ndef convergence_rates(m, num_periods=8, w=0.35, I=0.3, adjust_w=False):\n    P = 2*np.pi/w\n    dt = 1 / w    # Half stability limit\n    T = P*num_periods\n    dt_values, E_values = [], []\n    for i in range(m):\n        E = l2_error(dt, T, w, I, adjust_w)\n        dt_values.append(dt)\n        E_values.append(E)\n        dt = dt/2.\n    # Compute m-1 orders that should all be the same\n    r = [np.log(E_values[i-1]/E_values[i])/\n         np.log(dt_values[i-1]/dt_values[i])\n         for i in range(1, m, 1)]\n    return r, E_values, dt_values"
  },
  {
    "objectID": "vibration.html#try-it",
    "href": "vibration.html#try-it",
    "title": "A simple vibration problem",
    "section": "Try it!",
    "text": "Try it!\nPrint the computed convergence rates\n\nconvergence_rates(5, w=0.5, I=1)[0]\n\n[1.9487030166752326, 2.022988970233581, 2.0050238810553545, 2.0013403129395835]\n\n\nAdjusted solver:\n\nconvergence_rates(5, w=0.5, I=1, adjust_w=True)[0]\n\n[4.133074797577663, 4.035054365992913, 4.008071155725109, 4.0020125447469646]"
  },
  {
    "objectID": "vibration.html#plot-convergence-regular-solver",
    "href": "vibration.html#plot-convergence-regular-solver",
    "title": "A simple vibration problem",
    "section": "plot convergence regular solver",
    "text": "plot convergence regular solver\n\nfrom plotslopes import slope_marker\nr, E, dt = convergence_rates(5)\nplt.loglog(dt, E, dt, 10*np.array(dt), dt, np.array(dt)**2)\nplt.title('Convergence of finite difference method')\nplt.legend(['Error', '$\\\\mathcal{O}(\\\\Delta t)$', '$\\\\mathcal{O}(\\\\Delta t^2)$'])\nslope_marker((dt[1], E[1]), (2,1))\nslope_marker((dt[1], 10*dt[1]), (1,1))\nslope_marker((dt[1], dt[1]**2), (2,1))"
  },
  {
    "objectID": "vibration.html#plot-convergence-adjusted-solver",
    "href": "vibration.html#plot-convergence-adjusted-solver",
    "title": "A simple vibration problem",
    "section": "plot convergence adjusted solver",
    "text": "plot convergence adjusted solver\n\nfrom plotslopes import slope_marker\nr, E, dt = convergence_rates(5, adjust_w=True)\nplt.loglog(dt, E, dt, np.array(dt)**4)\nplt.title('Convergence of finite difference method')\nplt.legend(['Error', '$\\\\mathcal{O}(\\\\Delta t^4)$'])\nslope_marker((dt[1], E[1]), (4,1))\nslope_marker((dt[1], dt[1]**4), (4,1))"
  },
  {
    "objectID": "vibration.html#add-tests",
    "href": "vibration.html#add-tests",
    "title": "A simple vibration problem",
    "section": "Add tests",
    "text": "Add tests\nThe exact solution will not equal the numerical, but the order of the error is something we can test for.\n\ndef test_order(m, w=1, I=1, adjust_w=False):\n    r, E, dt = convergence_rates(m, w=w, I=I, adjust_w=adjust_w)\n    true_order = 4 if adjust_w else 2\n    error = abs(r[-1]-true_order)\n    try:\n      assert error &lt; 0.005\n      print(f'Test passed!!')\n    except AssertionError:\n      print(f'Test failed!! orders = {r}')\n\n\nRun test for \\(m=4\\) levels, \\(w=0.5, I=1\\) and adjust_w=False\n\ntest_order(4, w=0.5, I=1, adjust_w=False)\n\nTest failed!! orders = [1.9487030166752326, 2.022988970233581, 2.0050238810553545]\n\n\n\n\nTest fails. Try one more level\n\ntest_order(5, w=0.5, I=1, adjust_w=False)\n\nTest passed!!\n\n\n\ntest_order(5, w=0.5, I=1, adjust_w=True)\n\nTest passed!!"
  },
  {
    "objectID": "vibration.html#final-test",
    "href": "vibration.html#final-test",
    "title": "A simple vibration problem",
    "section": "Final test",
    "text": "Final test\nUse simply an assert clause and do not catch the error.\n\ndef test_order(m, w=1, I=1, adjust_w=False):\n    r, E, dt = convergence_rates(m, w=w, I=I, adjust_w=adjust_w)\n    true_order = 4 if adjust_w else 2\n    error = abs(r[-1]-true_order)\n    assert error &lt; 0.005, r\n\ntest_order(5, w=0.5, I=1, adjust_w=True)\n\nThis test will work with pytest."
  },
  {
    "objectID": "analysis.html#recap---finite-differencing-of-exponential-decay",
    "href": "analysis.html#recap---finite-differencing-of-exponential-decay",
    "title": "Analysis of exponential decay models",
    "section": "Recap - Finite differencing of exponential decay",
    "text": "Recap - Finite differencing of exponential decay\n\n\n\n\n\n\n\n\n\nThe ordinary differential equation\n\n\n\\[\nu'(t) = -au(t),\\quad u(0)=I, \\quad y \\in (0, T]\n\\] where \\(a&gt;0\\) is a constant.\n\n\n\nSolve the ODE by finite difference methods:\n\nDiscretize in time:\n\\[0 = t_0 &lt; t_1 &lt; t_2 &lt; \\cdots &lt; t_{N_t-1} &lt; t_{N_t} = T\\]\nSatisfy the ODE at \\(N_t\\) discrete time steps:\n\\[\n\\begin{align}\nu'(t_n) &= -a u(t_n), \\quad &n\\in [1, \\ldots, N_t], \\text{ or} \\\\\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) &= -a u(t_{n+\\scriptstyle\\frac{1}{2}}), \\quad &n\\in [0, \\ldots, N_t-1]\n\\end{align}\n\\]"
  },
  {
    "objectID": "analysis.html#finite-difference-algorithms",
    "href": "analysis.html#finite-difference-algorithms",
    "title": "Analysis of exponential decay models",
    "section": "Finite difference algorithms",
    "text": "Finite difference algorithms\n\nDiscretization by a generic \\(\\theta\\)-rule\n\n\\[\n\\frac{u^{n+1}-u^{n}}{\\triangle t} = -(1-\\theta)au^{n} - \\theta a u^{n+1}\n\\]\n\\[\n\\begin{cases}\n  \\theta = 0 \\quad &\\text{Forward Euler} \\\\\n  \\theta = 1 \\quad &\\text{Backward Euler} \\\\\n  \\theta = 1/2 \\quad &\\text{Crank-Nicolson}\n  \\end{cases}\n\\]\nNote \\(u^n = u(t_n)\\)\n\nSolve recursively: Set \\(u^0 = I\\) and then\n\n\\[\nu^{n+1} = \\frac{1-(1-\\theta)a \\triangle t}{1+\\theta a \\triangle t}u^{n} \\quad \\text{for } n=0, 1, \\ldots\n\\]"
  },
  {
    "objectID": "analysis.html#analysis-of-finite-difference-equations",
    "href": "analysis.html#analysis-of-finite-difference-equations",
    "title": "Analysis of exponential decay models",
    "section": "Analysis of finite difference equations",
    "text": "Analysis of finite difference equations\nModel: \\[\nu'(t) = -au(t),\\quad u(0)=I\n\\]\nMethod:\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n\n\\]\n\n\n\n\n\n\nProblem setting\n\n\nHow good is this method? Is it safe to use it?"
  },
  {
    "objectID": "analysis.html#solver",
    "href": "analysis.html#solver",
    "title": "Analysis of exponential decay models",
    "section": "Solver",
    "text": "Solver\nWe already have a solver that we can use to experiment with. Lets run it for a range of different timesteps.\n\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n    return u, t"
  },
  {
    "objectID": "analysis.html#encouraging-numerical-solutions---backwards-euler",
    "href": "analysis.html#encouraging-numerical-solutions---backwards-euler",
    "title": "Analysis of exponential decay models",
    "section": "Encouraging numerical solutions - Backwards Euler",
    "text": "Encouraging numerical solutions - Backwards Euler\n\\(I=1\\), \\(a=2\\), \\(\\theta =1\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#discouraging-numerical-solutions---crank-nicolson",
    "href": "analysis.html#discouraging-numerical-solutions---crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Discouraging numerical solutions - Crank-Nicolson",
    "text": "Discouraging numerical solutions - Crank-Nicolson\n\\(I=1\\), \\(a=2\\), \\(\\theta=0.5\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#discouraging-numerical-solutions---forward-euler",
    "href": "analysis.html#discouraging-numerical-solutions---forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "Discouraging numerical solutions - Forward Euler",
    "text": "Discouraging numerical solutions - Forward Euler\n\\(I=1\\), \\(a=2\\), \\(\\theta=0\\), \\(\\Delta t=1.25, 0.75, 0.5, 0.1\\)."
  },
  {
    "objectID": "analysis.html#summary-of-observations",
    "href": "analysis.html#summary-of-observations",
    "title": "Analysis of exponential decay models",
    "section": "Summary of observations",
    "text": "Summary of observations\nThe characteristics of the displayed curves can be summarized as follows:\n\nThe Backward Euler scheme always gives a monotone solution, lying above the exact solution.\nThe Crank-Nicolson scheme gives the most accurate results, but for \\(\\Delta t=1.25\\) the solution oscillates.\nThe Forward Euler scheme gives a growing, oscillating solution for \\(\\Delta t=1.25\\); a decaying, oscillating solution for \\(\\Delta t=0.75\\); a strange solution \\(u^n=0\\) for \\(n\\geq 1\\) when \\(\\Delta t=0.5\\); and a solution seemingly as accurate as the one by the Backward Euler scheme for \\(\\Delta t = 0.1\\), but the curve lies below the exact solution.\nSmall enough \\(\\Delta t\\) gives stable and accurate solution for all methods!"
  },
  {
    "objectID": "analysis.html#problem-setting-1",
    "href": "analysis.html#problem-setting-1",
    "title": "Analysis of exponential decay models",
    "section": "Problem setting",
    "text": "Problem setting\n\n\n\n\n\n\nWe ask the question\n\n\n\nUnder what circumstances, i.e., values of the input data \\(I\\), \\(a\\), and \\(\\Delta t\\) will the Forward Euler and Crank-Nicolson schemes result in undesired oscillatory solutions?\n\nTechniques of investigation:\n\nNumerical experiments\nMathematical analysis\n\nAnother question to be raised is\n\nHow does \\(\\Delta t\\) impact the error in the numerical solution?"
  },
  {
    "objectID": "analysis.html#exact-numerical-solution",
    "href": "analysis.html#exact-numerical-solution",
    "title": "Analysis of exponential decay models",
    "section": "Exact numerical solution",
    "text": "Exact numerical solution\nFor the simple exponential decay problem we are lucky enough to have an exact numerical solution\n\\[\nu^{n} = IA^n,\\quad A = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}\n\\]\nSuch a formula for the exact discrete solution is unusual to obtain in practice, but very handy for our analysis here.\n\n\n\n\n\n\nNote\n\n\nAn exact dicrete solution fulfills a discrete equation (without round-off errors), whereas an exact solution fulfills the original mathematical equation."
  },
  {
    "objectID": "analysis.html#stability",
    "href": "analysis.html#stability",
    "title": "Analysis of exponential decay models",
    "section": "Stability",
    "text": "Stability\nSince \\(u^n=I A^n\\),\n\n\\(A &lt; 0\\) gives a factor \\((-1)^n\\) and oscillatory solutions\n\\(|A|&gt;1\\) gives growing solutions\nRecall: the exact solution is monotone and decaying\nIf these qualitative properties are not met, we say that the numerical solution is unstable\n\n\nFor stability we need\n\\[\nA &gt; 0 \\quad \\text{ and } \\quad |A| \\le 1\n\\]"
  },
  {
    "objectID": "analysis.html#computation-of-stability-in-this-problem",
    "href": "analysis.html#computation-of-stability-in-this-problem",
    "title": "Analysis of exponential decay models",
    "section": "Computation of stability in this problem",
    "text": "Computation of stability in this problem\n\\(A &lt; 0\\) if\n\\[\n\\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t} &lt; 0\n\\]\nTo avoid oscillatory solutions we must have \\(A&gt; 0\\), which happens for\n\n\\[\n\\Delta t &lt; \\frac{1}{(1-\\theta)a}, \\quad \\text{for} \\, \\theta &lt; 1\n\\]\n\nAlways fulfilled for Backward Euler (\\(\\theta=1 \\rightarrow 1 &lt; 1+a \\Delta t\\) always true)\n\\(\\Delta t \\leq 1/a\\) for Forward Euler (\\(\\theta=0\\))\n\\(\\Delta t \\leq 2/a\\) for Crank-Nicolson (\\(\\theta = 0.5\\))\n\nWe get oscillatory solutions for FE when \\(\\Delta t \\le 1/a\\) and for CN when \\(\\Delta t \\le 2/a\\)"
  },
  {
    "objectID": "analysis.html#computation-of-stability-in-this-problem-1",
    "href": "analysis.html#computation-of-stability-in-this-problem-1",
    "title": "Analysis of exponential decay models",
    "section": "Computation of stability in this problem",
    "text": "Computation of stability in this problem\n\\(|A|\\leq 1\\) means \\(-1\\leq A\\leq 1\\)\n\\[\n-1\\leq\\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t} \\leq 1\n\\]\n\n\\(-1\\) is the critical limit (because \\(A\\le 1\\) is always satisfied).\n\\(-1 &lt; A\\) is always fulfilled for Backward Euler (\\(\\theta=1\\)) and Crank-Nicolson (\\(\\theta=0.5\\)).\nFor forward Euler or simply \\(\\theta &lt; 0.5\\) we have \\[\n\\Delta t \\leq \\frac{2}{(1-2\\theta)a},\\quad\n\\] and thus \\(\\Delta t \\leq 2/a\\) for stability of the forward Euler (\\(\\theta=0\\)) method"
  },
  {
    "objectID": "analysis.html#explanation-of-problems-with-forward-euler",
    "href": "analysis.html#explanation-of-problems-with-forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "Explanation of problems with forward Euler",
    "text": "Explanation of problems with forward Euler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\Delta t= 2\\cdot 1.25=2.5\\) and \\(A=-1.5\\): oscillations and growth\n\\(a\\Delta t = 2\\cdot 0.75=1.5\\) and \\(A=-0.5\\): oscillations and decay\n\\(\\Delta t=0.5\\) and \\(A=0\\): \\(u^n=0\\) for \\(n&gt;0\\)\nSmaller \\(\\Delta t\\): qualitatively correct solution"
  },
  {
    "objectID": "analysis.html#explanation-of-problems-with-crank-nicolson",
    "href": "analysis.html#explanation-of-problems-with-crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Explanation of problems with Crank-Nicolson",
    "text": "Explanation of problems with Crank-Nicolson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Delta t=1.25\\) and \\(A=-0.25\\): oscillatory solution\n\nNever any growing solution"
  },
  {
    "objectID": "analysis.html#summary-of-stability",
    "href": "analysis.html#summary-of-stability",
    "title": "Analysis of exponential decay models",
    "section": "Summary of stability",
    "text": "Summary of stability\n\nForward Euler is conditionally stable\n\n\\(\\Delta t &lt; 2/a\\) for avoiding growth\n\\(\\Delta t\\leq 1/a\\) for avoiding oscillations\n\nThe Crank-Nicolson is unconditionally stable wrt growth and conditionally stable wrt oscillations\n\n\\(\\Delta t &lt; 2/a\\) for avoiding oscillations\n\nBackward Euler is unconditionally stable"
  },
  {
    "objectID": "analysis.html#comparing-amplification-factors",
    "href": "analysis.html#comparing-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Comparing amplification factors",
    "text": "Comparing amplification factors\n\\(u^{n+1}\\) is an amplification \\(A\\) of \\(u^n\\):\n\\[\nu^{n+1} = Au^n,\\quad A = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}\n\\]\nThe exact solution is also an amplification:\n\\[\n\\begin{align}\nu(t_{n+1}) &= e^{-a(t_n+\\Delta t)} \\\\\nu(t_{n+1}) &= e^{-a \\Delta t} e^{-a t_n} \\\\\nu(t_{n+1}) &= A_e u(t_n), \\quad A_e = e^{-a\\Delta t}\n\\end{align}\n\\]\nA possible measure of accuracy: \\(A_e - A\\)"
  },
  {
    "objectID": "analysis.html#plotting-amplification-factors",
    "href": "analysis.html#plotting-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Plotting amplification factors",
    "text": "Plotting amplification factors"
  },
  {
    "objectID": "analysis.html#padelta-t-is-the-important-parameter-for-numerical-performance",
    "href": "analysis.html#padelta-t-is-the-important-parameter-for-numerical-performance",
    "title": "Analysis of exponential decay models",
    "section": "\\(p=a\\Delta t\\) is the important parameter for numerical performance",
    "text": "\\(p=a\\Delta t\\) is the important parameter for numerical performance\n\n\\(p=a\\Delta t\\) is a dimensionless parameter\nall expressions for stability and accuracy involve \\(p\\)\nNote that \\(\\Delta t\\) alone is not so important, it is the combination with \\(a\\) through \\(p=a\\Delta t\\) that matters\n\n\n\n\n\n\n\nAnother evidence why \\(p=a\\Delta t\\) is key\n\n\nIf we scale the model by \\(\\bar t=at\\), \\(\\bar u=u/I\\), we get \\(d\\bar u/d\\bar t = -\\bar u\\), \\(\\bar u(0)=1\\) (no physical parameters!). The analysis show that \\(\\Delta \\bar t\\) is key, corresponding to \\(a\\Delta t\\) in the unscaled model."
  },
  {
    "objectID": "analysis.html#series-expansion-of-amplification-factors",
    "href": "analysis.html#series-expansion-of-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Series expansion of amplification factors",
    "text": "Series expansion of amplification factors\nTo investigate \\(A_e - A\\) mathematically, we can Taylor expand the expression, using \\(p=a\\Delta t\\) as variable.\n\nfrom sympy import *\n# Create p as a mathematical symbol with name 'p'\np = Symbol('p', positive=True)\n# Create a mathematical expression with p\nA_e = exp(-p)\n# Find the first 6 terms of the Taylor series of A_e\nA_e.series(p, 0, 6)\n\n\\(\\displaystyle 1 - p + \\frac{p^{2}}{2} - \\frac{p^{3}}{6} + \\frac{p^{4}}{24} - \\frac{p^{5}}{120} + O\\left(p^{6}\\right)\\)\n\n\nThis is the Taylor expansion of the exact amplification factor. How does it compare with the numerical amplification factors?"
  },
  {
    "objectID": "analysis.html#numerical-amplification-factors",
    "href": "analysis.html#numerical-amplification-factors",
    "title": "Analysis of exponential decay models",
    "section": "Numerical amplification factors",
    "text": "Numerical amplification factors\nCompute the Taylor expansions of \\(A_e - A\\)\n\nfrom IPython.display import display\ntheta = Symbol('theta', positive=True)\nA = (1-(1-theta)*p)/(1+theta*p)\nFE = A_e.series(p, 0, 4) - A.subs(theta, 0).series(p, 0, 4)\nBE = A_e.series(p, 0, 4) - A.subs(theta, 1).series(p, 0, 4)\nhalf = Rational(1, 2)  # exact fraction 1/2\nCN = A_e.series(p, 0, 4) - A.subs(theta, half).series(p, 0, 4)\ndisplay(FE)\ndisplay(BE)\ndisplay(CN)\n\n\\(\\displaystyle \\frac{p^{2}}{2} - \\frac{p^{3}}{6} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle - \\frac{p^{2}}{2} + \\frac{5 p^{3}}{6} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle \\frac{p^{3}}{12} + O\\left(p^{4}\\right)\\)\n\n\n\nForward/backward Euler have leading error \\(p^2\\), or more commonly \\(\\Delta t^2\\)\nCrank-Nicolson has leading error \\(p^3\\), or \\(\\Delta t^3\\)"
  },
  {
    "objectID": "analysis.html#the-trueglobal-error-at-a-point",
    "href": "analysis.html#the-trueglobal-error-at-a-point",
    "title": "Analysis of exponential decay models",
    "section": "The true/global error at a point",
    "text": "The true/global error at a point\n\nThe error in \\(A\\) reflects the local (amplification) error when going from one time step to the next\nWhat is the global (true) error at \\(t_n\\)?\n\n\\[\ne^n = u_e(t_n) - u^n = Ie^{-at_n} - IA^n\n\\]\n\nTaylor series expansions of \\(e^n\\) simplify the expression"
  },
  {
    "objectID": "analysis.html#computing-the-global-error-at-a-point",
    "href": "analysis.html#computing-the-global-error-at-a-point",
    "title": "Analysis of exponential decay models",
    "section": "Computing the global error at a point",
    "text": "Computing the global error at a point\n\nn = Symbol('n', integer=True, positive=True)\nu_e = exp(-p*n)   # I=1\nu_n = A**n        # I=1\nFE = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)\nBE = u_e.series(p, 0, 4) - u_n.subs(theta, 1).series(p, 0, 4)\nCN = u_e.series(p, 0, 4) - u_n.subs(theta, half).series(p, 0, 4)\ndisplay(simplify(FE))\ndisplay(simplify(BE))\ndisplay(simplify(CN))\n\n\\(\\displaystyle \\frac{n p^{2}}{2} + \\frac{n p^{3}}{3} - \\frac{n^{2} p^{3}}{2} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle - \\frac{n p^{2}}{2} + \\frac{n p^{3}}{3} + \\frac{n^{2} p^{3}}{2} + O\\left(p^{4}\\right)\\)\n\n\n\\(\\displaystyle \\frac{n p^{3}}{12} + O\\left(p^{4}\\right)\\)\n\n\nSubstitute \\(n\\) by \\(t/\\Delta t\\) and \\(p\\) by \\(a \\Delta t\\):\n\nForward and Backward Euler: leading order term \\(\\frac{1}{2} ta^2\\Delta t\\)\nCrank-Nicolson: leading order term \\(\\frac{1}{12}ta^3\\Delta t^2\\)"
  },
  {
    "objectID": "analysis.html#convergence",
    "href": "analysis.html#convergence",
    "title": "Analysis of exponential decay models",
    "section": "Convergence",
    "text": "Convergence\nThe numerical scheme is convergent if the global error \\(e^n\\rightarrow 0\\) as \\(\\Delta t\\rightarrow 0\\). If the error has a leading order term \\((\\Delta t)^r\\), the convergence rate is of order \\(r\\)."
  },
  {
    "objectID": "analysis.html#integrated-errors",
    "href": "analysis.html#integrated-errors",
    "title": "Analysis of exponential decay models",
    "section": "Integrated errors",
    "text": "Integrated errors\nThe \\(\\ell^2\\) norm of the numerical error is computed as\n\\[\n||e^n||_{\\ell^2} = \\sqrt{\\Delta t\\sum_{n=0}^{N_t} ({u_{e}}(t_n) - u^n)^2}\n\\]\nWe can compute this using Sympy. Forward/Backward Euler has \\(e^n \\sim np^2/2\\)\n\nh, N, a, T = symbols('h,N,a,T') # h represents Delta t\nsimplify(sqrt(h * summation((n*p**2/2)**2, (n, 0, N))).subs(p, a*h).subs(N, T/h))\n\n\\(\\displaystyle \\frac{\\sqrt{6} a^{2} h^{2} \\sqrt{T \\left(\\frac{2 T^{2}}{h^{2}} + \\frac{3 T}{h} + 1\\right)}}{12}\\)\n\n\nIf we keep only the leading term in the parenthesis, we get the first order \\[\n||e^n||_{\\ell^2} \\approx \\frac{1}{2}\\sqrt{\\frac{T^3}{3}} a^2\\Delta t\n\\]"
  },
  {
    "objectID": "analysis.html#crank-nicolson",
    "href": "analysis.html#crank-nicolson",
    "title": "Analysis of exponential decay models",
    "section": "Crank-Nicolson",
    "text": "Crank-Nicolson\nFor Crank-Nicolson the pointwise error is \\(e^n \\sim n p^3 / 12\\). We get\n\nsimplify(sqrt(h * summation((n*p**3/12)**2, (n, 0, N))).subs(p, a*h).subs(N, T/h))\n\n\\(\\displaystyle \\frac{\\sqrt{6} a^{3} h^{3} \\sqrt{T \\left(\\frac{2 T^{2}}{h^{2}} + \\frac{3 T}{h} + 1\\right)}}{72}\\)\n\n\nwhich is simplified to the second order accurate\n\\[\n||e^n||_{\\ell^2} \\approx \\frac{1}{12}\\sqrt{\\frac{T^3}{3}}a^3\\Delta t^2\n\\]\n\n\n\n\n\n\nSummary of errors\n\n\nAnalysis of both the pointwise and the time-integrated true errors:\n\n1st order for Forward and Backward Euler\n2nd order for Crank-Nicolson"
  },
  {
    "objectID": "analysis.html#truncation-error",
    "href": "analysis.html#truncation-error",
    "title": "Analysis of exponential decay models",
    "section": "Truncation error",
    "text": "Truncation error\n\nHow good is the discrete equation?\nPossible answer: see how well \\(u_{e}\\) fits the discrete equation\n\nConsider the forward difference equation \\[\n\\frac{u^{n+1}-u^n}{\\Delta t} = -au^n\n\\]\nInsert \\(u_{e}\\) to obtain a truncation error \\(R^n\\)\n\\[\n\\frac{u_{e}(t_{n+1})-u_{e}(t_n)}{\\Delta t} + au_{e}(t_n) = R^n \\neq 0\n\\]"
  },
  {
    "objectID": "analysis.html#computation-of-the-truncation-error",
    "href": "analysis.html#computation-of-the-truncation-error",
    "title": "Analysis of exponential decay models",
    "section": "Computation of the truncation error",
    "text": "Computation of the truncation error\n\nThe residual \\(R^n\\) is the truncation error. How does \\(R^n\\) vary with \\(\\Delta t\\)?\n\nTool: Taylor expand \\(u_{e}\\) around the point where the ODE is sampled (here \\(t_n\\))\n\\[\nu_{e}(t_{n+1}) = u_{e}(t_n) + u_{e}'(t_n)\\Delta t + \\frac{1}{2}u_{e}''(t_n)\n\\Delta t^2 + \\cdots\n\\]\nInserting this Taylor series for \\(u_{e}\\) in the forward difference equation\n\\[\nR^n = \\frac{u_{e}(t_{n+1})-u_{e}(t_n)}{\\Delta t} + au_{e}(t_n)\n\\]\nto get\n\\[\nR^n = u_{e}'(t_n) + \\frac{1}{2}u_{e}''(t_n)\\Delta t + \\ldots + au_{e}(t_n)\n\\]"
  },
  {
    "objectID": "analysis.html#the-truncation-error-forward-euler",
    "href": "analysis.html#the-truncation-error-forward-euler",
    "title": "Analysis of exponential decay models",
    "section": "The truncation error forward Euler",
    "text": "The truncation error forward Euler\nWe have \\[\nR^n = u_{e}'(t_n) + \\frac{1}{2}u_{e}''(t_n)\\Delta t + \\ldots + au_{e}(t_n)\n\\]\nSince \\(u_{e}\\) solves the ODE \\(u_{e}'(t_n)=-au_{e}(t_n)\\), we get that \\(u_{e}'(t_n)\\) and \\(au_{e}(t_n)\\) cancel out. We are left with leading term\n\\[\nR^n \\approx \\frac{1}{2}u_{e}''(t_n)\\Delta t\n\\]\nThis is a mathematical expression for the truncation error."
  },
  {
    "objectID": "analysis.html#the-truncation-error-for-other-schemes",
    "href": "analysis.html#the-truncation-error-for-other-schemes",
    "title": "Analysis of exponential decay models",
    "section": "The truncation error for other schemes",
    "text": "The truncation error for other schemes\nBackward Euler:\n\\[\nR^n \\approx -\\frac{1}{2}u_{e}''(t_n)\\Delta t\n\\]\nCrank-Nicolson:\n\\[\nR^{n+\\scriptstyle\\frac{1}{2}} \\approx \\frac{1}{24}u_{e}'''(t_{n+\\scriptstyle\\frac{1}{2}})\\Delta t^2\n\\]"
  },
  {
    "objectID": "analysis.html#consistency-stability-and-convergence",
    "href": "analysis.html#consistency-stability-and-convergence",
    "title": "Analysis of exponential decay models",
    "section": "Consistency, stability, and convergence",
    "text": "Consistency, stability, and convergence\n\nTruncation error measures the residual in the difference equations. The scheme is consistent if the truncation error goes to 0 as \\(\\Delta t\\rightarrow 0\\). Importance: the difference equations approaches the differential equation as \\(\\Delta t\\rightarrow 0\\).\nStability means that the numerical solution exhibits the same qualitative properties as the exact solution. Here: monotone, decaying function.\nConvergence implies that the true (global) error \\(e^n =u_{e}(t_n)-u^n\\rightarrow 0\\) as \\(\\Delta t\\rightarrow 0\\). This is really what we want!\n\nThe Lax equivalence theorem for linear differential equations: consistency + stability is equivalent with convergence.\n(Consistency and stability is in most problems much easier to establish than convergence.)"
  },
  {
    "objectID": "analysis.html#numerical-computation-of-convergence-rate",
    "href": "analysis.html#numerical-computation-of-convergence-rate",
    "title": "Analysis of exponential decay models",
    "section": "Numerical computation of convergence rate",
    "text": "Numerical computation of convergence rate\nWe assume that the \\(\\ell^2\\) error norm on the mesh with level \\(i\\) can be written as\n\\[\nE_i = C (\\Delta t_i)^r\n\\]\nwhere \\(C\\) is a constant. This way, if we have the error on two levels, then we can compute\n\\[\n\\frac{E_{i-1}}{E_i} = \\frac{ (\\Delta t_{i-1})^r}{(\\Delta t_{i})^r} = \\left( \\frac{\\Delta t_{i-1}}{ \\Delta t_i} \\right)^r\n\\]\nand isolate \\(r\\) by computing\n\\[\nr = \\frac{\\log {\\frac{E_{i-1}}{E_i}}}{\\log {\\frac{\\Delta t_{i-1}}{\\Delta t_i}}}\n\\]"
  },
  {
    "objectID": "analysis.html#function-for-convergence-rate",
    "href": "analysis.html#function-for-convergence-rate",
    "title": "Analysis of exponential decay models",
    "section": "Function for convergence rate",
    "text": "Function for convergence rate\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\n\ndef l2_error(I, a, theta, dt): \n    u, t = solver(I, a, T, dt, theta)\n    en = u_exact(t, I, a) - u\n    return np.sqrt(dt*np.sum(en**2)) \n\ndef convergence_rates(m, I=1, a=2, T=8, theta=1, dt=1.):\n    dt_values, E_values = [], []\n    for i in range(m):\n        E = l2_error(I, a, theta, dt)\n        dt_values.append(dt)\n        E_values.append(E)\n        dt = dt/2\n    # Compute m-1 orders that should all be the same\n    r = [np.log(E_values[i-1]/E_values[i])/\n         np.log(dt_values[i-1]/dt_values[i])\n         for i in range(1, m, 1)]\n    return r"
  },
  {
    "objectID": "analysis.html#test-convergence-rates",
    "href": "analysis.html#test-convergence-rates",
    "title": "Analysis of exponential decay models",
    "section": "Test convergence rates",
    "text": "Test convergence rates\nBackward Euler:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 1.\nconvergence_rates(4, I, a, T, theta, dt)\n\n[0.9619265651066382, 0.98003334385805, 0.9897576131285538]\n\n\nForward Euler:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 0.\nconvergence_rates(4, I, a, T, theta, dt)\n\n[1.0472640894307232, 1.0222599097461846, 1.0108154242259877]\n\n\nCrank-Nicolson:\n\nI, a, T, dt, theta = 1., 2., 8., 0.1, 0.5\nconvergence_rates(4, I, a, T, theta, dt)\n\n[2.0037335266421343, 2.0009433957768175, 2.000236481071457]\n\n\nAll in good agreement with theory:-)"
  },
  {
    "objectID": "intro.html#hans-petter-langtangen-1962-2016",
    "href": "intro.html#hans-petter-langtangen-1962-2016",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Hans Petter Langtangen 1962-2016",
    "text": "Hans Petter Langtangen 1962-2016\n\n\n\n\n\n\n\n\n\n\n2011-2015 Editor-In-Chief SIAM J of Scientific Computing\nAuthor of 13 published books on scientific computing\nProfessor of Mechanics, University of Oslo 1998\nDeveloped INF5620 (which became IN5270 and now MAT-MEK4270)\nMemorial page"
  },
  {
    "objectID": "intro.html#a-little-bit-about-myself",
    "href": "intro.html#a-little-bit-about-myself",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A little bit about myself",
    "text": "A little bit about myself\n  \n\nProfessor of mechanics (2019-)\nPhD (Chalmers University of Technology) in mathematical modelling of turbulent combustion\nNorwegian Defence Research Establishment (2007-2012)\nComputational Fluid Dynamics\nHigh Performance Computing\nSpectral methods"
  },
  {
    "objectID": "intro.html#principal-developer-of-shenfun",
    "href": "intro.html#principal-developer-of-shenfun",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Principal developer of Shenfun",
    "text": "Principal developer of Shenfun\nHigh performance computing platform for solving PDEs by the spectral Galerkin method. Written in Python (Cython). https://github.com/spectralDNS/shenfun"
  },
  {
    "objectID": "intro.html#mat-mek4270-in-a-nutshell",
    "href": "intro.html#mat-mek4270-in-a-nutshell",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "MAT-MEK4270 in a nutshell",
    "text": "MAT-MEK4270 in a nutshell\n\nNumerical methods for partial differential equations (PDEs)\nHow to solve the equations, not why\nHow do we solve a PDE in practice?\nHow do we trust the answer?\nIs the numerical scheme stable? accurate? consistent?\nFocus on programming (github, python, testing code)\nIN5670 -&gt; IN5270 -&gt; MAT-MEK4270 - Lots of old material"
  },
  {
    "objectID": "intro.html#syllabus",
    "href": "intro.html#syllabus",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Syllabus",
    "text": "Syllabus\n\n\n\n\n\n\nImportant stuff\n\n\n\nLecture notes\nPresentations (including this one)\nGithub organization MATMEK-4270\n\n\n\n\n\n\n\n\n\n\nAlso important stuff, but less so as I will try to put all really important stuff in the lecture notes\n\n\n\nLangtangen, Finite Difference Computing with exponential decay - Chapters 1 and 2.\nLangtangen and Linge, Finite Difference Computing with PDEs - Parts of chapters 1 and 2.\nLangtangen and Mardal, Introduction to Numerical Methods for Variational Problems"
  },
  {
    "objectID": "intro.html#two-major-approaches",
    "href": "intro.html#two-major-approaches",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Two major approaches",
    "text": "Two major approaches\n\n\nFinite differences\n\\[\n\\frac{du(t)}{dt} \\approx \\frac{u(t+\\Delta t) - u(t)}{\\Delta t}\n\\]\n\nApproximate in points\nUniform grid\nTaylor expansions\n\n\nVariational methods\n\\[\n\\int_{\\Omega} u'' v d\\Omega = -\\int_{\\Omega} u' v' d\\Omega + \\int_{\\Gamma} u'v d\\Gamma\n\\]\n\nApproximate weakly\nFinite element method\nLeast squares method\nGalerkin method\n\n\n\nWe will use both approaches to first consider function approximations and then the approximation of equations."
  },
  {
    "objectID": "intro.html#required-software-skills",
    "href": "intro.html#required-software-skills",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Required software skills",
    "text": "Required software skills\n\nOur software platform: Python, Jupyter notebooks\nImportant Python packages: numpy, scipy, matplotlib, sympy, shenfun, …\nAnaconda Python, conda environments"
  },
  {
    "objectID": "intro.html#assumedideal-background",
    "href": "intro.html#assumedideal-background",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Assumed/ideal background",
    "text": "Assumed/ideal background\n\nIN1900: Python programming, solution of ODEs\nSome experience with finite difference methods\nSome analytical and numerical knowledge of PDEs\nMuch experience with calculus and linear algebra\nMuch experience with programming of mathematical problems\nExperience with mathematical modeling with PDEs (from physics, mechanics, geophysics, or …)"
  },
  {
    "objectID": "intro.html#exponential-decay-model",
    "href": "intro.html#exponential-decay-model",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Exponential decay model",
    "text": "Exponential decay model\n\n\n\n\n\n\nODE problem\n\n\n\\[\nu'=-au,\\quad u(0)=I,\\ t\\in (0,T]\n\\]\nwhere \\(a&gt;0\\) is a constant and \\(u(t)\\) is the time-dependent solution.\n\n\n\n\nWe study first a simple 1D ODE, because this will lead us to the building blocks that we need for solving PDEs!\nWe can more easily study the concepts of stability, accuracy, convergence and consistency.\n\n\nSee Langtangen, Finite Difference Computing - Chapter 1"
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example",
    "href": "intro.html#what-to-learn-in-the-start-up-example",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example",
    "text": "What to learn in the start-up example\n\nHow to think when constructing finite difference methods, with special focus on the Forward Euler, Backward Euler, and Crank-Nicolson (midpoint) schemes\nHow to formulate a computational algorithm and translate it into Python code\nHow to optimize the code for computational speed\nHow to plot the solutions\nHow to compute numerical errors and convergence rates\nHow to analyse the numerical solution"
  },
  {
    "objectID": "intro.html#finite-difference-methods",
    "href": "intro.html#finite-difference-methods",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Finite difference methods",
    "text": "Finite difference methods\n\nThe finite difference method is the simplest method for solving differential equations\nSatisfy the equations in discrete points, not continuously\nFast to learn, derive, and implement\nA very useful tool to know, even if you aim at using the finite element or the finite volume method"
  },
  {
    "objectID": "intro.html#the-steps-in-the-finite-difference-method",
    "href": "intro.html#the-steps-in-the-finite-difference-method",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The steps in the finite difference method",
    "text": "The steps in the finite difference method\nSolving a differential equation by a finite difference method consists of four steps:\n\ndiscretizing the domain,\nfulfilling the equation at discrete time points,\nreplacing derivatives by finite differences,\nsolve the discretized problem. (Often with a recursive algorithm in 1D)"
  },
  {
    "objectID": "intro.html#step-1-discretizing-the-domain",
    "href": "intro.html#step-1-discretizing-the-domain",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 1: Discretizing the domain",
    "text": "Step 1: Discretizing the domain\nThe time domain \\([0,T]\\) is represented by a mesh: a finite number of \\(N_t+1\\) points\n\\[\n0 = t_0 &lt; t_1 &lt; t_2 &lt; \\cdots &lt; t_{N_t-1} &lt; t_{N_t} = T\n\\]\n\n\nWe seek the solution \\(u\\) at the mesh points: \\(u(t_n)\\), \\(n=1,2,\\ldots,N_t\\).\nNote: \\(u^0\\) is known as \\(I\\).\nNotational short-form for the numerical approximation to \\(u(t_n)\\): \\(u^n\\)\nIn the differential equation: \\(u(t)\\) is the exact solution\nIn the numerical method and implementation: \\(u^n\\) is the numerical approximation"
  },
  {
    "objectID": "intro.html#step-1-discretizing-the-domain-1",
    "href": "intro.html#step-1-discretizing-the-domain-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 1: Discretizing the domain",
    "text": "Step 1: Discretizing the domain\n\\(u^n\\) is a mesh function, defined at the mesh points \\(t_n\\), \\(n=0,\\ldots,N_t\\) only."
  },
  {
    "objectID": "intro.html#what-about-a-mesh-function-between-the-mesh-points",
    "href": "intro.html#what-about-a-mesh-function-between-the-mesh-points",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What about a mesh function between the mesh points?",
    "text": "What about a mesh function between the mesh points?\nCan extend the mesh function to yield values between mesh points by linear interpolation:\n\\[\n\\begin{equation}\nu(t) \\approx u^n + \\frac{u^{n+1}-u^n}{t_{n+1}-t_n}(t - t_n)\n\\end{equation}\n\\]"
  },
  {
    "objectID": "intro.html#step-2-fulfilling-the-equation-at-discrete-time-points",
    "href": "intro.html#step-2-fulfilling-the-equation-at-discrete-time-points",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 2: Fulfilling the equation at discrete time points",
    "text": "Step 2: Fulfilling the equation at discrete time points\n\nThe ODE holds for all \\(t\\in (0,T]\\) (infinite no of points)\nIdea: let the ODE be valid at the mesh points only (finite no of points)\n\n\\[\nu'(t_n) = -au(t_n),\\quad n=1,\\ldots,N_t\n\\]"
  },
  {
    "objectID": "intro.html#step-3-replacing-derivatives-by-finite-differences",
    "href": "intro.html#step-3-replacing-derivatives-by-finite-differences",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 3: Replacing derivatives by finite differences",
    "text": "Step 3: Replacing derivatives by finite differences\nNow it is time for the finite difference approximations of derivatives:\n\\[\nu'(t_n) \\approx \\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n}\n\\]"
  },
  {
    "objectID": "intro.html#step-3-replacing-derivatives-by-finite-differences-1",
    "href": "intro.html#step-3-replacing-derivatives-by-finite-differences-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 3: Replacing derivatives by finite differences",
    "text": "Step 3: Replacing derivatives by finite differences\nInserting the finite difference approximation in\n\\[\nu'(t_n) = -au(t_n)\n\\]\ngives\n\\[\n\\begin{equation}\n\\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -au^{n},\\quad n=0,1,\\ldots,N_t-1\n\\end{equation}\n\\]\n(Known as discrete equation, or discrete problem, or finite difference method/scheme)"
  },
  {
    "objectID": "intro.html#step-4-formulating-a-recursive-algorithm",
    "href": "intro.html#step-4-formulating-a-recursive-algorithm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Step 4: Formulating a recursive algorithm",
    "text": "Step 4: Formulating a recursive algorithm\nHow can we actually compute the \\(u^n\\) values?\n\ngiven \\(u^0=I\\)\ncompute \\(u^1\\) from \\(u^0\\)\ncompute \\(u^2\\) from \\(u^1\\)\ncompute \\(u^3\\) from \\(u^2\\) (and so forth)\n\nIn general: we have \\(u^n\\) and seek \\(u^{n+1}\\)\n\n\n\n\n\n\nThe Forward Euler scheme\n\n\nSolve wrt \\(u^{n+1}\\) to get the computational formula: \\[\nu^{n+1} = u^n - a(t_{n+1} -t_n)u^n\n\\]"
  },
  {
    "objectID": "intro.html#let-us-apply-the-scheme-by-hand",
    "href": "intro.html#let-us-apply-the-scheme-by-hand",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Let us apply the scheme by hand",
    "text": "Let us apply the scheme by hand\nAssume constant time spacing: \\(\\Delta t = t_{n+1}-t_n=\\mbox{const}\\) such that \\(u^{n+1} = u^n (1- a \\Delta t)\\)\n\\[\n\\begin{align*}\nu^0 &= I,\\\\\nu^1 & = I(1-a\\Delta t),\\\\\nu^2 & = I(1-a\\Delta t)^2,\\\\\n&\\vdots\\\\\nu^{N_t} &= I(1-a\\Delta t)^{N_t}\n\\end{align*}\n\\]\nOoops - we can find the numerical solution by hand (in this simple example)! No need for a computer (yet)…"
  },
  {
    "objectID": "intro.html#a-backward-difference",
    "href": "intro.html#a-backward-difference",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A backward difference",
    "text": "A backward difference\nHere is another finite difference approximation to the derivative (backward difference):\n\\[\nu'(t_n) \\approx \\frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}}\n\\]"
  },
  {
    "objectID": "intro.html#the-backward-euler-scheme",
    "href": "intro.html#the-backward-euler-scheme",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Backward Euler scheme",
    "text": "The Backward Euler scheme\nInserting the finite difference approximation in \\(u'(t_n)=-au(t_n)\\) yields the Backward Euler (BE) scheme:\n\\[\n\\frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}} = -a u^n\n\\]\nSolve with respect to the unknown \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1}{1+ a(t_{n+1}-t_n)} u^n\n\\]\n\n\n\n\n\n\nNote\n\n\nWe use \\(u^{n+1}\\) as unknown and rename \\(u^n \\longrightarrow u^{n+1}\\) and \\(u^{n-1} \\longrightarrow u^{n}\\)"
  },
  {
    "objectID": "intro.html#a-centered-difference",
    "href": "intro.html#a-centered-difference",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "A centered difference",
    "text": "A centered difference\nCentered differences are better approximations than forward or backward differences."
  },
  {
    "objectID": "intro.html#the-crank-nicolson-scheme-ideas",
    "href": "intro.html#the-crank-nicolson-scheme-ideas",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Crank-Nicolson scheme; ideas",
    "text": "The Crank-Nicolson scheme; ideas\nIdea 1: let the ODE hold at \\(t_{n+\\scriptstyle\\frac{1}{2}}\\). With \\(N_t+1\\) points, that is \\(N_t\\) equations for \\(n=0, 1, \\ldots N_t-1\\)\n\\[\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) = -au(t_{n+\\scriptstyle\\frac{1}{2}})\n\\]\nIdea 2: approximate \\(u'(t_{n+\\scriptstyle\\frac{1}{2}})\\) by a centered difference\n\\[\nu'(t_{n+\\scriptstyle\\frac{1}{2}}) \\approx \\frac{u^{n+1}-u^n}{t_{n+1}-t_n}\n\\]\nProblem: \\(u(t_{n+\\scriptstyle\\frac{1}{2}})\\) is not defined, only \\(u^n=u(t_n)\\) and \\(u^{n+1}=u(t_{n+1})\\)\n\nSolution (linear interpolation):\n\\[\nu(t_{n+\\scriptstyle\\frac{1}{2}}) \\approx \\frac{1}{2} (u^n + u^{n+1})\n\\]"
  },
  {
    "objectID": "intro.html#the-crank-nicolson-scheme-result",
    "href": "intro.html#the-crank-nicolson-scheme-result",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The Crank-Nicolson scheme; result",
    "text": "The Crank-Nicolson scheme; result\nResult:\n\\[\n\\frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -a\\frac{1}{2} (u^n + u^{n+1})\n\\]\nSolve wrt to \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1-\\frac{1}{2} a(t_{n+1}-t_n)}{1 + \\frac{1}{2} a(t_{n+1}-t_n)}u^n\n\\] This is a Crank-Nicolson (CN) scheme or a midpoint or centered scheme."
  },
  {
    "objectID": "intro.html#the-unifying-theta-rule",
    "href": "intro.html#the-unifying-theta-rule",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The unifying \\(\\theta\\)-rule",
    "text": "The unifying \\(\\theta\\)-rule\nThe Forward Euler, Backward Euler, and Crank-Nicolson schemes can be formulated as one scheme with a varying parameter \\(\\theta\\):\n\\[\n\\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -a (\\theta u^{n+1} + (1-\\theta) u^{n})\n\\]\n\n\\(\\theta =0\\): Forward Euler\n\\(\\theta =1\\): Backward Euler\n\\(\\theta =1/2\\): Crank-Nicolson\nWe may alternatively choose any \\(\\theta\\in [0,1]\\).\n\n\\(u^n\\) is known, solve for \\(u^{n+1}\\):\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a(t_{n+1}-t_n)}{1 + \\theta a(t_{n+1}-t_n)} u^n\n\\]"
  },
  {
    "objectID": "intro.html#constant-time-step",
    "href": "intro.html#constant-time-step",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Constant time step",
    "text": "Constant time step\nVery common assumption (not important, but exclusively used for simplicity hereafter): constant time step \\(t_{n+1}-t_n\\equiv\\Delta t\\)\nSummary of schemes for constant time step \\[\n\\begin{align}\nu^{n+1} &= (1 - a\\Delta t )u^n  \\quad (\\hbox{FE}) \\\\\nu^{n+1} &= \\frac{1}{1+ a\\Delta t} u^n  \\quad (\\hbox{BE}) \\\\\nu^{n+1} &= \\frac{1-\\frac{1}{2} a\\Delta t}{1 + \\frac{1}{2} a\\Delta t} u^n \\quad (\\hbox{CN})\\\\\nu^{n+1} &= \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n \\quad (\\theta-\\hbox{rule})\n\\end{align}\n\\]"
  },
  {
    "objectID": "intro.html#implementation-1",
    "href": "intro.html#implementation-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Implementation",
    "text": "Implementation\nModel:\n\\[\nu'(t) = -au(t),\\quad t\\in (0,T], \\quad u(0)=I\n\\]\nNumerical method:\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a\\Delta t}u^n\n\\]\nfor \\(\\theta\\in [0,1]\\). Note\n\n\\(\\theta=0\\) gives Forward Euler\n\\(\\theta=1\\) gives Backward Euler\n\\(\\theta=1/2\\) gives Crank-Nicolson"
  },
  {
    "objectID": "intro.html#requirements-of-a-program",
    "href": "intro.html#requirements-of-a-program",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Requirements of a program",
    "text": "Requirements of a program\n\nCompute the numerical solution \\(u^n\\), \\(n=1,2,\\ldots,N_t\\)\nDisplay the numerical and exact solution \\(u_{e}(t)=e^{-at}\\)\nBring evidence to a correct implementation (verification)\nCompare the numerical and the exact solution in a plot\nQuantify the error \\(u_{e}(t_n) - u^n\\) using norms\nCompute the convergence rate of the numerical scheme\n(Optimize for speed)"
  },
  {
    "objectID": "intro.html#algorithm",
    "href": "intro.html#algorithm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Algorithm",
    "text": "Algorithm\n\nStore \\(u^n\\), \\(n=0,1,\\ldots,N_t\\) in an array \\(\\boldsymbol{u}\\).\nAlgorithm:\n\ninitialize \\(u^0\\)\nfor \\(n=1, 2, \\ldots, N_t\\): compute \\(u^n\\) using the \\(\\theta\\)-rule formula"
  },
  {
    "objectID": "intro.html#in-python",
    "href": "intro.html#in-python",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "In Python",
    "text": "In Python\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    for n in range(0, Nt):    # n=0,1,...,Nt-1\n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u, t\n\nu, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)\n# Write out a table of t and u values:\nfor i in range(len(t)):\n    print(f't={t[i]:6.3f} u={u[i]:g}')"
  },
  {
    "objectID": "intro.html#in-python-1",
    "href": "intro.html#in-python-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "In Python",
    "text": "In Python\n\nimport numpy as np\ndef solver(I, a, T, dt, theta):\n    \"\"\"Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt.\"\"\"\n    Nt = int(T/dt)            # no of time intervals\n    T = Nt*dt                 # adjust T to fit time step dt\n    u = np.zeros(Nt+1)           # array of u[n] values\n    t = np.linspace(0, T, Nt+1)  # time mesh\n    u[0] = I                  # assign initial condition\n    for n in range(0, Nt):    # n=0,1,...,Nt-1\n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u, t\n\nu, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)\n# Write out a table of t and u values:\nfor i in range(len(t)):\n    print(f't={t[i]:6.3f} u={u[i]:g}')\n\nt= 0.000 u=1\nt= 0.800 u=0.384615\nt= 1.600 u=0.147929\nt= 2.400 u=0.0568958\nt= 3.200 u=0.021883\nt= 4.000 u=0.00841653\nt= 4.800 u=0.00323713\nt= 5.600 u=0.00124505\nt= 6.400 u=0.000478865\nt= 7.200 u=0.000184179\nt= 8.000 u=7.0838e-05"
  },
  {
    "objectID": "intro.html#plot-the-solution",
    "href": "intro.html#plot-the-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plot the solution",
    "text": "Plot the solution\nWe will also learn about plotting. It is very important to present data in a clear and consise manner. It is very easy to generate a naked plot\n\nimport matplotlib.pyplot as plt \nI, a, T, dt, theta = 1, 2, 8, 0.8, 1\nu, t = solver(I, a, T, dt, theta)\nfig = plt.figure(figsize=(6, 4))\nax = fig.gca()\nax.plot(t, u)"
  },
  {
    "objectID": "intro.html#plot-the-solution-1",
    "href": "intro.html#plot-the-solution-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plot the solution",
    "text": "Plot the solution\nBut you should always add legends, titles, exact solution, etc. Make the plot nice:-)\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\nu, t = solver(I=I, a=a, T=T, dt=0.8, theta=1)\nte = np.linspace(0, T, 1000)\nue = u_exact(te, I, a)\nfig = plt.figure(figsize=(6, 4))\nplt.plot(t, u, 'bs-', te, ue, 'r')\nplt.title('Decay')\nplt.legend(['numerical', 'exact'])\nplt.xlabel('Time'), plt.ylabel('u(t)');"
  },
  {
    "objectID": "intro.html#plotly-is-a-very-good-alternative",
    "href": "intro.html#plotly-is-a-very-good-alternative",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Plotly is a very good alternative",
    "text": "Plotly is a very good alternative\n\nimport plotly.express as px\npfig = px.line(x=t, y=u, labels={'x': 'Time', 'y': 'u(t)'}, \n               width=600, height=400, title='Decay',\n               template=\"simple_white\")\npfig.show()"
  },
  {
    "objectID": "intro.html#verifying-the-implementation",
    "href": "intro.html#verifying-the-implementation",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Verifying the implementation",
    "text": "Verifying the implementation\n\nVerification = bring evidence that the program works\nFind suitable test problems\nMake function for each test problem\nLater: put the verification tests in a professional testing framework\n\npytest\ngithub actions"
  },
  {
    "objectID": "intro.html#comparison-with-exact-numerical-solution",
    "href": "intro.html#comparison-with-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Comparison with exact numerical solution",
    "text": "Comparison with exact numerical solution\n\n\n\n\n\n\nWhat is exact?\n\n\nThere is a difference between exact numerical solution and exact solution!\n\n\n\nRepeated use of the \\(\\theta\\)-rule gives exact numerical solution: \\[\n\\begin{align*}\nu^0 &= I,\\\\\nu^1 &= Au^0 = AI\\\\\nu^n &= A^nu^{n-1} = A^nI\n\\end{align*}\n\\]\nExact solution on the other hand:\n\\[\nu(t) = \\exp(-a t), \\quad u(t_n) = \\exp(-a t_n)\n\\]"
  },
  {
    "objectID": "intro.html#making-a-test-based-on-an-exact-numerical-solution",
    "href": "intro.html#making-a-test-based-on-an-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Making a test based on an exact numerical solution",
    "text": "Making a test based on an exact numerical solution\nThe exact discrete solution is\n\\[\nu^n = IA^n\n\\]\nTest if your solver gives\n\\[\n\\max_n |u^n - IA^n| &lt; \\epsilon\\sim 10^{-15}\n\\]\nfor a few precalculated steps.\n\n\n\n\n\n\nTip\n\n\nMake sure you understand what \\(n\\) in \\(u^n\\) and in \\(A^n\\) means! \\(n\\) is not used as a power in \\(u^n\\), but it is a power in \\(A^n\\)!"
  },
  {
    "objectID": "intro.html#run-a-few-numerical-steps-by-hand",
    "href": "intro.html#run-a-few-numerical-steps-by-hand",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Run a few numerical steps by hand",
    "text": "Run a few numerical steps by hand\nUse a calculator (\\(I=0.1\\), \\(\\theta=0.8\\), \\(\\Delta t =0.8\\)):\n\\[\nA\\equiv \\frac{1 - (1-\\theta) a\\Delta t}{1 + \\theta a \\Delta t} = 0.298245614035\n\\]\n\\[\n\\begin{align*}\nu^1 &= AI=0.0298245614035,\\\\\nu^2 &= Au^1= 0.00889504462912,\\\\\nu^3 &=Au^2= 0.00265290804728\n\\end{align*}\n\\]"
  },
  {
    "objectID": "intro.html#the-test-based-on-exact-numerical-solution",
    "href": "intro.html#the-test-based-on-exact-numerical-solution",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "The test based on exact numerical solution",
    "text": "The test based on exact numerical solution\n\ndef test_solver_three_steps(solver):\n    \"\"\"Compare three steps with known manual computations.\"\"\"\n    theta = 0.8\n    a = 2\n    I = 0.1\n    dt = 0.8\n    u_by_hand = np.array([I,\n                          0.0298245614035,\n                          0.00889504462912,\n                          0.00265290804728])\n\n    Nt = 3  # number of time steps\n    u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)\n    tol = 1E-14  # tolerance for comparing floats\n    diff = abs(u - u_by_hand).max()\n    success = diff &lt; tol\n    assert success, diff\n\ntest_solver_three_steps(solver)\n\n\n\n\n\n\n\nNote\n\n\nWe do not use the exact solution because the numerical solution will not equal the exact!"
  },
  {
    "objectID": "intro.html#quantifying-the-error",
    "href": "intro.html#quantifying-the-error",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Quantifying the error",
    "text": "Quantifying the error\nComputing the norm of the error\n\n\\(e^n = u^n - u_e(t_n)\\) is a mesh function\nUsually we want one number for the error\nUse a norm of \\(e^n\\)\n\nNorms of a function \\(f(t)\\):\n\\[\n\\begin{align}\n||f||_{L^2} &= \\left( \\int_0^T f(t)^2 dt\\right)^{1/2} \\\\\n||f||_{L^1} &= \\int_0^T |f(t)| dt \\\\\n||f||_{L^\\infty} &= \\max_{t\\in [0,T]}|f(t)|\n\\end{align}\n\\]"
  },
  {
    "objectID": "intro.html#norms-of-mesh-functions",
    "href": "intro.html#norms-of-mesh-functions",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Norms of mesh functions",
    "text": "Norms of mesh functions\n\nProblem: \\(f^n =f(t_n)\\) is a mesh function and hence not defined for all \\(t\\). How to integrate \\(f^n\\)?\nIdea: Apply a numerical integration rule, using only the mesh points of the mesh function.\n\nThe Trapezoidal rule:\n\\[\n||f^n|| = \\left(\\Delta t\\left(\\scriptstyle\\frac{1}{2}(f^0)^2 + \\scriptstyle\\frac{1}{2}(f^{N_t})^2\n+ \\sum_{n=1}^{N_t-1} (f^n)^2\\right)\\right)^{1/2}\n\\]\nCommon simplification yields the \\(\\ell^2\\) norm of a mesh function:\n\\[\n||f^n||_{\\ell^2} = \\left(\\Delta t\\sum_{n=0}^{N_t} (f^n)^2\\right)^{1/2}\n\\]"
  },
  {
    "objectID": "intro.html#norms---notice",
    "href": "intro.html#norms---notice",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Norms - notice!",
    "text": "Norms - notice!\n\nThe continuous norms use capital \\(L^2, L^1, L^\\infty{}\\)\nThe discrete norm uses lowercase \\(\\ell^2, \\ell^1, \\ell^{\\infty}\\)"
  },
  {
    "objectID": "intro.html#implementation-of-the-error-norm",
    "href": "intro.html#implementation-of-the-error-norm",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Implementation of the error norm",
    "text": "Implementation of the error norm\n\\[\nE = ||e^n||_{\\ell^2}  = \\sqrt{\\Delta t\\sum_{n=0}^{N_t} (e^n)^2}\n\\]\nPython code for the norm:\n\nu_exact = lambda t, I, a: I*np.exp(-a*t)\nI, a, T, dt, theta = 1., 2., 8., 0.2, 1\nu, t = solver(I, a, T, dt, theta)\nen = u_exact(t, I, a) - u\nE = np.sqrt(dt*np.sum(en**2))\nprint(f'Errornorm = {E}')\n\nErrornorm = 0.0637716295205199"
  },
  {
    "objectID": "intro.html#how-about-computational-speed",
    "href": "intro.html#how-about-computational-speed",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\n\nFor example, you have three arrays\n\\[\n\\boldsymbol{u} = (u_i)_{i=0}^N, \\boldsymbol{v} = (v_i)_{i=0}^N, \\boldsymbol{w} = (w_i)_{i=0}^N\n\\]\nNow compute\n\\[\nw_i = u_i \\cdot v_i, \\quad \\forall \\, i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "intro.html#how-about-computational-speed-1",
    "href": "intro.html#how-about-computational-speed-1",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\nRegular (scalar) implementation:\n\nN = 1000\nu = np.random.random(N)\nv = np.random.random(N)\nw = np.zeros(N)\n\nfor i in range(N):\n    w[i] = u[i] * v[i]\n\nVectorized:\n\nw[:] = u * v\n\nNumpy is heavily vectorized! So much so that mult, add, div, etc are vectorized by default!"
  },
  {
    "objectID": "intro.html#how-about-computational-speed-2",
    "href": "intro.html#how-about-computational-speed-2",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "How about computational speed?",
    "text": "How about computational speed?\nThe code is naive and not very efficient. It is not vectorized!\n\n\n\n\n\n\nVectorization\n\n\nVectorization refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.\n\n\n\n\n\n\n\n\n\nVectorization warning\n\n\nPretty much all the code you will see and get access to in this course will be vectorized!"
  },
  {
    "objectID": "intro.html#vectorizing-the-decay-solver",
    "href": "intro.html#vectorizing-the-decay-solver",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Vectorizing the decay solver",
    "text": "Vectorizing the decay solver\nGet rid of the for-loop!\nu[0] = I                  # assign initial condition\nfor n in range(0, Nt):    # n=0,1,...,Nt-1\n    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nHow? Difficult because it is a recursive update and not regular elementwise multiplication. But remember\n\\[\nA = (1 - (1- \\theta) a  \\Delta t)/(1 + \\theta \\Delta t a)\n\\]\n\\[\n\\begin{align*}\nu^1 & = A u^0,\\\\\nu^2 & = A u^1,\\\\\n&\\vdots\\\\\nu^{N_t} &= A u^{N_t-1}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "intro.html#vectorized-code",
    "href": "intro.html#vectorized-code",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Vectorized code",
    "text": "Vectorized code\nu[0] = I                  # assign initial condition\nfor n in range(0, Nt):    # n=0,1,...,Nt-1\n    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\nCan be implemented as\n\nu[0] = I                  # assign initial condition\nu[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\nu[:] = np.cumprod(u)     \n\n\nbecause\n\\[\nu^n = A^n u^0, \\quad \\text{since }\n\\begin{cases}\nu^1 & = A u^0,\\\\\nu^2 & = A u^1 = A^2 u^0,\\\\\n&\\vdots\\\\\nu^{N_t} &= A u^{N_t-1} = A^{N_t} u^0\n\\end{cases}\n\\]\n\nnp.cumprod([1, 2, 2, 2])\n\narray([1, 2, 4, 8])"
  },
  {
    "objectID": "intro.html#why-vectorization",
    "href": "intro.html#why-vectorization",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Why vectorization?",
    "text": "Why vectorization?\n\nPython for-loops are slow!\nPython for-loops usually requires more lines of code.\n\n\ndef f0(u, I, theta, a, dt):\n    u[0] = I                  \n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n    return u\n\ndef f1(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return u\n\nI, a, T, dt, theta = 1, 2, 8, 0.8, 1\nu, t = solver(I, a, T, dt, theta)\n\nassert np.allclose(f0(u.copy(), I, theta, a, dt), \n                   f1(u.copy(), I, theta, a, dt))\n\n\nLets try some timings!"
  },
  {
    "objectID": "intro.html#why-vectorization-timings",
    "href": "intro.html#why-vectorization-timings",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Why vectorization? Timings",
    "text": "Why vectorization? Timings\n\ndef f0(u, I, theta, a, dt):\n    u[0] = I                  \n    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)\n    u[:] = np.cumprod(u)\n\ndef f1(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nLets try some timings:\n\n%timeit -q -o -n 1000 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.32 µs ± 94 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)&gt;\n\n\n\n%timeit -q -o -n 1000 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.26 µs ± 68.3 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)&gt;\n\n\n\nHmm. Not really what’s expected. Why? Because the array u is really short! Lets try a longer array\n\nprint(f\"Length of u = {u.shape[0]}\") \n\nLength of u = 11"
  },
  {
    "objectID": "intro.html#longer-array-timings",
    "href": "intro.html#longer-array-timings",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Longer array timings",
    "text": "Longer array timings\n\ndt = dt/10\nu, t = solver(I, a, T, dt, theta) \nprint(f\"Length of u = {u.shape[0]}\")\n\nLength of u = 101\n\n\n\n%timeit -q -o -n 100 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 2.86 µs ± 1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n%timeit -q -o -n 100 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 20.6 µs ± 517 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\nEven longer array:\n\ndt = dt/10\nu, t = solver(I, a, T, dt, theta) \nprint(f\"Length of u = {u.shape[0]}\")\n\nLength of u = 1001\n\n\n\n%timeit -q -o -n 100 f0(u, I, theta, a, dt)\n\n&lt;TimeitResult : 3.86 µs ± 2.34 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n%timeit -q -o -n 100 f1(u, I, theta, a, dt)\n\n&lt;TimeitResult : 220 µs ± 2.68 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\nVectorized code takes the same time! Only overhead costs, not the actual computation."
  },
  {
    "objectID": "intro.html#what-else-is-there-numba",
    "href": "intro.html#what-else-is-there-numba",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What else is there? Numba",
    "text": "What else is there? Numba\n\nimport numba as nb\n@nb.jit\ndef f2(u,  I, theta, a, dt):\n    u[0] = I                 \n    for n in range(0, len(u)-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n\nTime it once\n\n%timeit -q -o -n 100 f2(u, I, theta, a, dt)\n\n&lt;TimeitResult : 48.1 µs ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\nSlow because the code needs to be compiled. Try again\n\n%timeit -q -o -n 100 f2(u, I, theta, a, dt)\n\n&lt;TimeitResult : 1.29 µs ± 8.08 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\nThat is even faster than the vectorized code!"
  },
  {
    "objectID": "intro.html#what-else-cython",
    "href": "intro.html#what-else-cython",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What else? Cython",
    "text": "What else? Cython\n\n%load_ext cython\n\nThe cython extension is already loaded. To reload it, use:\n  %reload_ext cython\n\n\n\n%%cython -a\n#cython: boundscheck=False, wraparound=False, cdivision=True\ncpdef void f3(double[::1] u, int  I, double theta, double a, double dt):\n    cdef int n\n    cdef int N = u.shape[0]\n    u[0] = I                 \n    for n in range(0, N-1):  \n        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    return\n\n\n\n\n\n    \n    Cython: _cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.pyx\n    \n\n\nGenerated by Cython 3.0.10\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+1: #cython: boundscheck=False, wraparound=False, cdivision=True\n  __pyx_t_7 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_7);\n  if (PyDict_SetItem(__pyx_d, __pyx_n_s_test, __pyx_t_7) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;\n+2: cpdef void f3(double[::1] u, int  I, double theta, double a, double dt):\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic void __pyx_f_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__Pyx_memviewslice __pyx_v_u, int __pyx_v_I, double __pyx_v_theta, double __pyx_v_a, double __pyx_v_dt, CYTHON_UNUSED int __pyx_skip_dispatch) {\n  int __pyx_v_n;\n  int __pyx_v_N;\n/* … */\n  /* function exit code */\n  __pyx_L0:;\n}\n\n/* Python wrapper */\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3 = {\"f3\", (PyCFunction)(void*)(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  __Pyx_memviewslice __pyx_v_u = { 0, 0, { 0 }, { 0 }, { 0 } };\n  int __pyx_v_I;\n  double __pyx_v_theta;\n  double __pyx_v_a;\n  double __pyx_v_dt;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"f3 (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_MACROS\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject **__pyx_pyargnames[] = {&__pyx_n_s_u,&__pyx_n_s_I,&__pyx_n_s_theta,&__pyx_n_s_a,&__pyx_n_s_dt,0};\n  PyObject* values[5] = {0,0,0,0,0};\n    if (__pyx_kwds) {\n      Py_ssize_t kw_args;\n      switch (__pyx_nargs) {\n        case  5: values[4] = __Pyx_Arg_FASTCALL(__pyx_args, 4);\n        CYTHON_FALLTHROUGH;\n        case  4: values[3] = __Pyx_Arg_FASTCALL(__pyx_args, 3);\n        CYTHON_FALLTHROUGH;\n        case  3: values[2] = __Pyx_Arg_FASTCALL(__pyx_args, 2);\n        CYTHON_FALLTHROUGH;\n        case  2: values[1] = __Pyx_Arg_FASTCALL(__pyx_args, 1);\n        CYTHON_FALLTHROUGH;\n        case  1: values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      kw_args = __Pyx_NumKwargs_FASTCALL(__pyx_kwds);\n      switch (__pyx_nargs) {\n        case  0:\n        if (likely((values[0] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_u)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[0]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else goto __pyx_L5_argtuple_error;\n        CYTHON_FALLTHROUGH;\n        case  1:\n        if (likely((values[1] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_I)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[1]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 1); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  2:\n        if (likely((values[2] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_theta)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[2]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 2); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  3:\n        if (likely((values[3] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_a)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[3]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 3); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n        CYTHON_FALLTHROUGH;\n        case  4:\n        if (likely((values[4] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_dt)) != 0)) {\n          (void)__Pyx_Arg_NewRef_FASTCALL(values[4]);\n          kw_args--;\n        }\n        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n        else {\n          __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, 4); __PYX_ERR(0, 2, __pyx_L3_error)\n        }\n      }\n      if (unlikely(kw_args &gt; 0)) {\n        const Py_ssize_t kwd_pos_args = __pyx_nargs;\n        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, \"f3\") &lt; 0)) __PYX_ERR(0, 2, __pyx_L3_error)\n      }\n    } else if (unlikely(__pyx_nargs != 5)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);\n      values[1] = __Pyx_Arg_FASTCALL(__pyx_args, 1);\n      values[2] = __Pyx_Arg_FASTCALL(__pyx_args, 2);\n      values[3] = __Pyx_Arg_FASTCALL(__pyx_args, 3);\n      values[4] = __Pyx_Arg_FASTCALL(__pyx_args, 4);\n    }\n    __pyx_v_u = __Pyx_PyObject_to_MemoryviewSlice_dc_double(values[0], PyBUF_WRITABLE); if (unlikely(!__pyx_v_u.memview)) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_I = __Pyx_PyInt_As_int(values[1]); if (unlikely((__pyx_v_I == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_theta = __pyx_PyFloat_AsDouble(values[2]); if (unlikely((__pyx_v_theta == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_a = __pyx_PyFloat_AsDouble(values[3]); if (unlikely((__pyx_v_a == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n    __pyx_v_dt = __pyx_PyFloat_AsDouble(values[4]); if (unlikely((__pyx_v_dt == (double)-1) && PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L3_error)\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"f3\", 1, 5, 5, __pyx_nargs); __PYX_ERR(0, 2, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  {\n    Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n      __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);\n    }\n  }\n  __PYX_XCLEAR_MEMVIEW(&__pyx_v_u, 1);\n  __Pyx_AddTraceback(\"_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.f3\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__pyx_self, __pyx_v_u, __pyx_v_I, __pyx_v_theta, __pyx_v_a, __pyx_v_dt);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  __PYX_XCLEAR_MEMVIEW(&__pyx_v_u, 1);\n  {\n    Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n      __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);\n    }\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(CYTHON_UNUSED PyObject *__pyx_self, __Pyx_memviewslice __pyx_v_u, int __pyx_v_I, double __pyx_v_theta, double __pyx_v_a, double __pyx_v_dt) {\n  PyObject *__pyx_r = NULL;\n  __Pyx_XDECREF(__pyx_r);\n  if (unlikely(!__pyx_v_u.memview)) { __Pyx_RaiseUnboundLocalError(\"u\"); __PYX_ERR(0, 2, __pyx_L1_error) }\n  __pyx_f_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_f3(__pyx_v_u, __pyx_v_I, __pyx_v_theta, __pyx_v_a, __pyx_v_dt, 0); if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2, __pyx_L1_error)\n  __pyx_t_1 = __Pyx_void_to_None(NULL); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_1);\n  __pyx_r = __pyx_t_1;\n  __pyx_t_1 = 0;\n  goto __pyx_L0;\n\n  /* function exit code */\n  __pyx_L1_error:;\n  __Pyx_XDECREF(__pyx_t_1);\n  __Pyx_AddTraceback(\"_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38.f3\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __pyx_r = NULL;\n  __pyx_L0:;\n  __Pyx_XGIVEREF(__pyx_r);\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n/* … */\n  __pyx_tuple__20 = PyTuple_Pack(5, __pyx_n_s_u, __pyx_n_s_I, __pyx_n_s_theta, __pyx_n_s_a, __pyx_n_s_dt); if (unlikely(!__pyx_tuple__20)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_tuple__20);\n  __Pyx_GIVEREF(__pyx_tuple__20);\n/* … */\n  __pyx_t_7 = __Pyx_CyFunction_New(&__pyx_mdef_54_cython_magic_56487b4d6695ae1d6d1ee138204b704501527e38_1f3, 0, __pyx_n_s_f3, NULL, __pyx_n_s_cython_magic_56487b4d6695ae1d6d, __pyx_d, ((PyObject *)__pyx_codeobj__21)); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_7);\n  if (PyDict_SetItem(__pyx_d, __pyx_n_s_f3, __pyx_t_7) &lt; 0) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;\n 3:     cdef int n\n+4:     cdef int N = u.shape[0]\n  __pyx_v_N = (__pyx_v_u.shape[0]);\n+5:     u[0] = I\n  __pyx_t_1 = 0;\n  *((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_1)) )) = __pyx_v_I;\n+6:     for n in range(0, N-1):\n  __pyx_t_2 = (__pyx_v_N - 1);\n  __pyx_t_3 = __pyx_t_2;\n  for (__pyx_t_4 = 0; __pyx_t_4 &lt; __pyx_t_3; __pyx_t_4+=1) {\n    __pyx_v_n = __pyx_t_4;\n+7:         u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]\n    __pyx_t_1 = __pyx_v_n;\n    __pyx_t_5 = (__pyx_v_n + 1);\n    *((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_5)) )) = (((1.0 - (((1.0 - __pyx_v_theta) * __pyx_v_a) * __pyx_v_dt)) / (1.0 + ((__pyx_v_theta * __pyx_v_dt) * __pyx_v_a))) * (*((double *) ( /* dim=0 */ ((char *) (((double *) __pyx_v_u.data) + __pyx_t_1)) ))));\n  }\n+8:     return\n  goto __pyx_L0;"
  },
  {
    "objectID": "intro.html#cython-timing",
    "href": "intro.html#cython-timing",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Cython timing",
    "text": "Cython timing\n\n%timeit -q -o -n 100 f3(u, I, theta, a, dt)\n\n&lt;TimeitResult : 1.28 µs ± 21.6 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)&gt;\n\n\n\n\n\n\n\n\nCython and Numba are both fast!\n\n\nCython and Numba are both as fast as pure C. Either one can be used to speed up critical routines with very little additional effort!\n\n\n\n\n\n\n\n\n\nNote\n\n\nCython is very easy to use in notebooks, but requires some additional steps to be compiled used as extension modules with regular python programs."
  },
  {
    "objectID": "finitedifferences.html#initial-or-boundary-value-problems",
    "href": "finitedifferences.html#initial-or-boundary-value-problems",
    "title": "The finite difference method",
    "section": "Initial or Boundary Value Problems",
    "text": "Initial or Boundary Value Problems\nThe second-order differential equation\n\\[\nu''(t) + au'(t) + bu(t) = f(t), \\quad t \\in [0, T]\n\\]\nis classified as either an IVP or a BVP based on boundary/initial conditions.\n\n\n\nIVP\n\nTypically specifies \\(u(0)\\) and \\(u'(0)\\)\nExplicit recurrence relations, no use of linear algebra\nCan use the finite difference or similar methods for discretization\n\n\nBVP\n\nTypically specifies \\(u(0)\\) and \\(u(T)\\) or \\(u'(0)\\) and \\(u'(T)\\)\nImplicit linear algebra methods\nCan use the finite difference or similar methods for discretization\n\n\n\nSimilar characteristics for higher-order differential equations. IVP specifies all conditions at initial time."
  },
  {
    "objectID": "finitedifferences.html#the-explicit-ivp-approach-simply-computes-un1-from-un-un-1",
    "href": "finitedifferences.html#the-explicit-ivp-approach-simply-computes-un1-from-un-un-1",
    "title": "The finite difference method",
    "section": "The explicit IVP approach simply computes \\(u^{n+1}\\) from \\(u^n, u^{n-1}\\)…",
    "text": "The explicit IVP approach simply computes \\(u^{n+1}\\) from \\(u^n, u^{n-1}\\)…\n\n\n\nFor example, the exponential decay problem with initial condition\n\\[\nu' + au = 0, t \\in (0, T], \\, u(0)=I.\n\\]\nis discretized as\n\\[\nu^{n+1} = \\frac{1 - (1-\\theta) a \\Delta t}{1 + \\theta a \\Delta t} u^n = g u^n.\n\\]\nThe solution vector \\(\\boldsymbol{u} = (u^0, u^1, \\ldots, u^{N})\\) is obtained from a recurrence algorithm\n\n\\(u^0 = I\\)\nfor n = 0, 1, … , N-1\n\nCompute \\(u^{n+1} = g u^n\\)\n\n\nEasy to understand and easy to solve. Equations are never assembled into matrix form. But may be unstable!"
  },
  {
    "objectID": "finitedifferences.html#the-bvp-approach-solves-the-difference-equations-by-assembling-matrices-and-vectors",
    "href": "finitedifferences.html#the-bvp-approach-solves-the-difference-equations-by-assembling-matrices-and-vectors",
    "title": "The finite difference method",
    "section": "The BVP approach solves the difference equations by assembling matrices and vectors",
    "text": "The BVP approach solves the difference equations by assembling matrices and vectors\nEach row in the matrix-problem represents one difference equation.\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}\n\\]\nFor the exponential decay problem the matrix problem looks like\n\\[\n\\underbrace{\n\\begin{bmatrix}\n  1  & 0 & 0 & 0 & 0  \\\\\n-g  & 1 & 0 & 0 & 0  \\\\\n0  & -g & 1 & 0 & 0  \\\\\n0  & 0 & -g & 1 & 0  \\\\\n0  & 0 & 0 & -g & 1  \n\\end{bmatrix}}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4\n\\end{bmatrix}}_{\\boldsymbol{u}} =\n\\underbrace{\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#understand-that-the-matrix-problem-is-the-same-as-the-recurrence",
    "href": "finitedifferences.html#understand-that-the-matrix-problem-is-the-same-as-the-recurrence",
    "title": "The finite difference method",
    "section": "Understand that the matrix problem is the same as the recurrence",
    "text": "Understand that the matrix problem is the same as the recurrence\n\\[\n\\underbrace{\n\\begin{bmatrix}\n1   & 0 & 0 & 0 & 0   \\\\\n-g  & 1 & 0 & 0 & 0   \\\\\n0  & -g & 1 & 0 & 0   \\\\\n0  & 0 & -g & 1 & 0   \\\\\n0  & 0 & 0 & -g & 1   \n\\end{bmatrix}}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4\n\\end{bmatrix}}_{\\boldsymbol{u}} =\n\\underbrace{\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]\n\n\nsimply means\n\n\\[\n\\begin{align}\nu^0 &= I \\\\\n-g u^0 + u^1 &= 0 \\\\\n-g u^1 + u^2 &= 0 \\\\\n\\vdots\n\\end{align}\n\\]\n\n\nStill explicit since matrix is lower triangular. But problem is assembled into matrix form."
  },
  {
    "objectID": "finitedifferences.html#solve-matrix-problem",
    "href": "finitedifferences.html#solve-matrix-problem",
    "title": "The finite difference method",
    "section": "Solve matrix problem",
    "text": "Solve matrix problem\nThe matrix problem\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}\n\\]\nis solved as\n\\[\n\\boldsymbol{u} = A^{-1} \\boldsymbol{b}\n\\]\nThere are many different ways to achieve this. We will not be very concerned with how in this course."
  },
  {
    "objectID": "finitedifferences.html#assemble-matrix-problem",
    "href": "finitedifferences.html#assemble-matrix-problem",
    "title": "The finite difference method",
    "section": "Assemble matrix problem",
    "text": "Assemble matrix problem\n\nN = 8\na = 2\nI = 1\ntheta = 0.5\ndt = 0.5\nT = N*dt\nt = np.linspace(0, N*dt, N+1)\nu = np.zeros(N+1)\ng = (1 - (1-theta) * a * dt)/(1 + theta * a * dt)\n\nAssemble\n\nfrom scipy import sparse\nA = sparse.diags([-g, 1], np.array([-1, 0]), (N+1, N+1), 'csr')\nb = np.zeros(N+1); b[0] = I"
  },
  {
    "objectID": "finitedifferences.html#solve-and-compare-with-recurrence",
    "href": "finitedifferences.html#solve-and-compare-with-recurrence",
    "title": "The finite difference method",
    "section": "Solve and compare with recurrence",
    "text": "Solve and compare with recurrence\nRecurrence:\n\nu[0] = I\nfor n in range(N):\n  u[n+1] = g * u[n] \n\nMatrix\n\num = sparse.linalg.spsolve(A, b)\nprint(u-um)\n\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\nOk, no difference. There’s really no advantage of assembling the explicit exponential decay problem.\nBut which is faster?\n\n\ndef reccsolve(u, N, I):\n  u[0] = I\n  for n in range(N):\n    u[n+1] = g * u[n] \nu = np.zeros(N+1)\n%timeit -n100 reccsolve(u, N, I) \n\n1.41 µs ± 812 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%timeit -n100 um = sparse.linalg.spsolve(A, b) \n\n18.9 µs ± 432 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "finitedifferences.html#try-to-speed-up-by-computing-the-inverse",
    "href": "finitedifferences.html#try-to-speed-up-by-computing-the-inverse",
    "title": "The finite difference method",
    "section": "Try to speed up by computing the inverse",
    "text": "Try to speed up by computing the inverse\nA unit lower triangular matrix has a unit lower triangular inverse\n\nnp.set_printoptions(precision=3, suppress=True) \nAi = np.linalg.inv(A.toarray())\nprint(Ai)\n\n[[1.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n [0.333 1.    0.    0.    0.    0.    0.    0.    0.   ]\n [0.111 0.333 1.    0.    0.    0.    0.    0.    0.   ]\n [0.037 0.111 0.333 1.    0.    0.    0.    0.    0.   ]\n [0.012 0.037 0.111 0.333 1.    0.    0.    0.    0.   ]\n [0.004 0.012 0.037 0.111 0.333 1.    0.    0.    0.   ]\n [0.001 0.004 0.012 0.037 0.111 0.333 1.    0.    0.   ]\n [0.    0.001 0.004 0.012 0.037 0.111 0.333 1.    0.   ]\n [0.    0.    0.001 0.004 0.012 0.037 0.111 0.333 1.   ]]\n\n\n\nNow solve directly using \\(A^{-1}\\)\n\nui = Ai @ b\n\nCompare with previous - still the same\n\num = sparse.linalg.spsolve(A, b)\nprint(um-ui) \n\n[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\n\n\n\nbut it seems to be very fast\n\n%timeit -n 100 ui = Ai @ b \n\n542 ns ± 43 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "finitedifferences.html#until-we-try-numba-on-the-explicit-solver",
    "href": "finitedifferences.html#until-we-try-numba-on-the-explicit-solver",
    "title": "The finite difference method",
    "section": "Until we try Numba on the explicit solver",
    "text": "Until we try Numba on the explicit solver\n\nimport numba as nb \n\n@nb.jit\ndef nbreccsolve(u, N, I, g):\n  u[0] = I\n  for n in range(N):\n    u[n+1] = g * u[n]\nnbreccsolve(u, N, I, g)\n%timeit -n 100 nbreccsolve(u, N, I, g)\n\n139 ns ± 10.1 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nBottom line - explicit marching methods are very fast! Unfortunately, they may be unstable and cannot be used for BVPs."
  },
  {
    "objectID": "finitedifferences.html#the-bv-vibration-problem",
    "href": "finitedifferences.html#the-bv-vibration-problem",
    "title": "The finite difference method",
    "section": "The BV vibration problem",
    "text": "The BV vibration problem\n\\[\nu'' + \\omega^2 u = 0,\\, t \\in (0, T) \\quad u(0) = I, u(T) = I,\n\\]\ncannot be solved with recurrence, since it is not an initial value problem.\n\nHowever, we can solve this problem using a central finite difference for all internal points \\(n=1, 2, \\ldots, N-1\\)\n\\[\n\\frac{u^{n+1}-2u^n+u^{n-1}}{\\Delta t^2} + \\omega^2 u^n = 0.\n\\]\nThis leads to an implicit linear algebra problem, because the equation for \\(u^n\\) (above) depends on \\(u^{n+1}\\)!"
  },
  {
    "objectID": "finitedifferences.html#vibration-on-matrix-form",
    "href": "finitedifferences.html#vibration-on-matrix-form",
    "title": "The finite difference method",
    "section": "Vibration on matrix form",
    "text": "Vibration on matrix form\nThe matrix problem is now, using \\(g = 2-\\omega^2 \\Delta t^2\\),\n\\[\n\\begin{bmatrix}\n  1  & 0 & 0 & 0 & 0 & 0 & 0  \\\\\n1  & -g & 1 & 0 & 0 & 0 & 0  \\\\\n0  & 1 & -g & 1 & 0 & 0 & 0  \\\\\n0  & 0 & 1 & -g & 1 & 0 & 0  \\\\\n0  & 0 & 0 & 1 & -g & 1 & 0  \\\\\n0  & 0 & 0 & 0 & 1 & -g & 1  \\\\\n0  & 0 & 0 & 0 & 0 & 0 & 1  \n\\end{bmatrix}\n\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\nu^3 \\\\\nu^4 \\\\\nu^5 \\\\\nu^6\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\nI\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe matrix contains items both over and under the main diagonal, which characterizes an implicit method. Explicit marching method leads to lower triangular matrices.\nThe first and last rows are modified in order to apply boundary conditions implicitly. More about that later."
  },
  {
    "objectID": "finitedifferences.html#assemble-and-solve-the-implicit-bvp",
    "href": "finitedifferences.html#assemble-and-solve-the-implicit-bvp",
    "title": "The finite difference method",
    "section": "Assemble and solve the implicit BVP",
    "text": "Assemble and solve the implicit BVP\n\nT, N, I, w = 3., 35, 1., 2*np.pi\ndt = T/N \ng = 2 - w**2*dt**2\nA = sparse.diags([1, -g, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nb = np.zeros(N+1)\nA[0, :3] = 1, 0, 0    # Fix first row\nA[-1, -3:] = 0, 0, 1  # Fix last row\nb[0], b[-1] = I, I\nu2 = sparse.linalg.spsolve(A.tocsr(), b)\nt = np.linspace(0, T, N+1)\nplt.plot(t, u2, 'bo', t, I*np.cos(w*t), 'r--')\nplt.legend(['Numerical', 'Exact']);"
  },
  {
    "objectID": "finitedifferences.html#taylor-expansions-can-be-used-to-design-finite-difference-methods-of-any-order",
    "href": "finitedifferences.html#taylor-expansions-can-be-used-to-design-finite-difference-methods-of-any-order",
    "title": "The finite difference method",
    "section": "Taylor expansions can be used to design finite difference methods of any order",
    "text": "Taylor expansions can be used to design finite difference methods of any order\nFor example, we can create two backward and two forward Taylor expansions starting from \\(u^n=u(t_n)\\)\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nRemember: \\(u^{n+a} = u(t_{n+a})\\) and \\(t_{n+a} = (n+a)h\\) and we use \\(h=\\Delta t\\) for simplicity.\n\nAdd equations (-1) and (1) and isolate \\(u''(t_n)\\)\n\\[\n\\begin{equation*}\nu''(t_n) = \\frac{u^{n+1}-2u^n + u^{n-1}}{h^2}  + \\frac{h^2}{12}u'''' +\n\\end{equation*}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-fd-stencil-can-be-applied-to-the-entire-mesh",
    "href": "finitedifferences.html#the-fd-stencil-can-be-applied-to-the-entire-mesh",
    "title": "The finite difference method",
    "section": "The FD stencil can be applied to the entire mesh",
    "text": "The FD stencil can be applied to the entire mesh\nThat is, we can compute \\(\\boldsymbol{u}^{(2)}=(u''(t_n))_{n=0}^{N}\\) from the mesh function \\(\\boldsymbol{u} = (u(t_n))_{n=0}^N \\in \\mathbb{R}^{N+1}\\) as\n\\[\n\\boldsymbol{u}^{(2)} = D^{(2)} \\boldsymbol{u},\n\\]\nwhere \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\) is the second derivative matrix.\n\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{(2)}_0 \\\\\nu^{(2)}_1 \\\\\nu^{(2)}_2 \\\\\n\\vdots \\\\\nu^{(2)}_{N-2} \\\\\nu^{(2)}_{N-1} \\\\\nu^{(2)}_{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{u}^{(2)}} =\n\\underbrace{\n  \\frac{1}{h^2} \\begin{bmatrix}\n? & ? & ? & ?  & ? & ? & ? & ?  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n? & ? & ? & ?  & ? & ? & ? & ? \\\\\n\\end{bmatrix}\n}_{D^{(2)}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-1} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#how-about-the-boundary-nodes",
    "href": "finitedifferences.html#how-about-the-boundary-nodes",
    "title": "The finite difference method",
    "section": "How about the boundary nodes?",
    "text": "How about the boundary nodes?\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{(2)}_0 \\\\\nu^{(2)}_1 \\\\\nu^{(2)}_2 \\\\\n\\vdots \\\\\nu^{(2)}_{N-2} \\\\\nu^{(2)}_{N-1} \\\\\nu^{(2)}_{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{u}^{(2)}} =\n\\underbrace{\n  \\frac{1}{h^2} \\begin{bmatrix}\n? & ? & ? & ?  & ? & ? & ? & ?  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n? & ? & ? & ?  & ? & ? & ? & ? \\\\\n\\end{bmatrix}\n}_{D^{(2)}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-1} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n\\]\nWhat to do with the first and last rows? The central stencils do not work here.\nWe still want to compute \\(u_0^{(2)}=u''(t_0)\\) and \\(u_N^{(2)}=u''(t_N)\\). How?\n\nForward and backward stencils will do"
  },
  {
    "objectID": "finitedifferences.html#we-can-do-a-forward-difference-at-n0",
    "href": "finitedifferences.html#we-can-do-a-forward-difference-at-n0",
    "title": "The finite difference method",
    "section": "We can do a forward difference at \\(n=0\\)",
    "text": "We can do a forward difference at \\(n=0\\)\nRemember: \\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nSubtract 2 times Eq. (1) from Eq. (2) and rearrange\n\\[\\small\n(2)-2(1): \\, u^{n+2} - 2u^{n+1} = -u^n +  \\frac{h^2}{1}u'' + h^3 u''' + \\frac{7 h^4}{12}u'''' +\n\\]\nRearrange to isolate a  first order  accurate stencil for \\(u''(0)\\)\n\\[\\small\nu''(0) = \\frac{u^{2}-2u^{1}+u^0}{h^2} - \\color{red} h \\color{black} u'''(0) - \\frac{7 h^2}{12}u''''(0) +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#we-can-do-a-backward-difference-at-nn",
    "href": "finitedifferences.html#we-can-do-a-backward-difference-at-nn",
    "title": "The finite difference method",
    "section": "We can do a backward difference at \\(n=N\\)",
    "text": "We can do a backward difference at \\(n=N\\)\nUse two backward Taylor expansions:\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2 h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n\\end{align*}\n\\]\nSubtract 2 times Eq. (-1) from Eq. (-2) and rearrange\n\\[\\small\n(-2)-2(-1): \\, u^{n-2} - 2u^{n-1} = -u^n +  \\frac{h^2}{1}u'' - h^3 u''' + \\frac{7 h^4}{12}u'''' +\n\\]\nRearrange to isolate a  first order  accurate backward stencil for \\(u''(T)\\)\n\\[\\small\nu''(T) = \\frac{u^{N-2}-2u^{N-1}+u^N}{h^2} - \\color{red} h \\color{black} u'''(T) - \\frac{7 h^2}{12}u''''(T) +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#we-get-a-second-derivative-matrix",
    "href": "finitedifferences.html#we-get-a-second-derivative-matrix",
    "title": "The finite difference method",
    "section": "We get a second derivative matrix",
    "text": "We get a second derivative matrix\n\\[\nD^{(2)} = \\frac{1}{h^2} \\begin{bmatrix}\n1 & -2 & 1 & 0  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -2 & 1 \\\\\n\\end{bmatrix}\n\\]\nbut with merely first order accuracy at the boundaries. Can we do better?\n\nYes! Of course, just use more points near the boundaries!"
  },
  {
    "objectID": "finitedifferences.html#use-three-forward-points-small-un1-un2-un3-to-get-a-second-order-forward-stencil",
    "href": "finitedifferences.html#use-three-forward-points-small-un1-un2-un3-to-get-a-second-order-forward-stencil",
    "title": "The finite difference method",
    "section": "Use three forward points \\(\\small u^{n+1}, u^{n+2}, u^{n+3}\\) to get a second order forward stencil",
    "text": "Use three forward points \\(\\small u^{n+1}, u^{n+2}, u^{n+3}\\) to get a second order forward stencil\n\\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots \\\\\n(3)\\quad u^{n+3} &= u^n + 3h u' + \\frac{9 h^2}{2}u'' + \\frac{9 h^3}{2}u''' + \\frac{27 h^4}{8}u'''' + \\cdots \\\\\n\\end{align*}\n\\]\nNow to eliminate both \\(u'\\) and \\(u'''\\) terms add the three equations as \\(-(3) + 4\\cdot(2) - 5\\cdot(1)\\) (don’t worry about how I know this yet)\n\\[\n-(3)+4\\cdot(2)-5\\cdot(1): \\, -u^{n+3}+4u^{n+2}-5u^{n+1} = -2 u^n + h^2 u'' - \\frac{11 h^4}{12}u'''' +  \n\\]\nIsolate \\(u''(0)\\)\n\\[\nu''(0) = \\frac{-u^{3} + 4u^{2} - 5u^{1} + 2u^0}{h^2} + \\frac{11 \\color{red} h^2 \\color{black}}{12} u'''' +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#fully-second-order-d2",
    "href": "finitedifferences.html#fully-second-order-d2",
    "title": "The finite difference method",
    "section": "Fully second order \\(D^{(2)}\\)",
    "text": "Fully second order \\(D^{(2)}\\)\n\\[\nD^{(2)} = \\frac{1}{h^2} \\begin{bmatrix}\n2 & -5 & 4 & -1  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & -1 & 4 & -5 & 2 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#assemble-in-python",
    "href": "finitedifferences.html#assemble-in-python",
    "title": "The finite difference method",
    "section": "Assemble in Python",
    "text": "Assemble in Python\n\nN = 8\ndt = 0.5\nT = N*dt\nD2 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD2[0, :4] = 2, -5, 4, -1\nD2[-1, -4:] = -1, 4, -5, 2\nD2 *= (1/dt**2) # don't forget h\nD2.toarray()*dt**2\n\narray([[ 2., -5.,  4., -1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1., -2.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1., -2.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1., -2.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1., -2.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  1., -2.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  1., -2.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -2.,  1.],\n       [ 0.,  0.,  0.,  0.,  0., -1.,  4., -5.,  2.]])\n\n\nApply matrix to a mesh function \\(f(t_n) = t_n^2\\)\n\nt = np.linspace(0, N*dt, N+1)\nf = t**2\nD2 @ f\n\narray([2., 2., 2., 2., 2., 2., 2., 2., 2.])\n\n\nExact for all \\(n\\)!"
  },
  {
    "objectID": "finitedifferences.html#try-with-merely-first-order-boundary",
    "href": "finitedifferences.html#try-with-merely-first-order-boundary",
    "title": "The finite difference method",
    "section": "Try with merely first order boundary",
    "text": "Try with merely first order boundary\n\nD21 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD21[0, :4] = 1, -2, 1, 0\nD21[-1, -4:] = 0, 1, -2, 1\nD21 *= (1/dt**2) \nD21 @ f\n\narray([2., 2., 2., 2., 2., 2., 2., 2., 2.])\n\n\nStill exact! Why?\n\nConsider the forward first order stencil\n\\[\nu''(0) = \\frac{u^{2}-2u^{1}+u^0}{h^2} - \\color{red} h u'''(0) \\color{black} - \\frac{7 h^2}{12}u''''(0) +\n\\]\n\n\nThe leading error is \\(h u'''(0)\\) and \\(u'''(0)\\) is zero for \\(u(t)=t^2\\). The second order stencil captures the second order polynomial \\(t^2\\) exactly!"
  },
  {
    "objectID": "finitedifferences.html#more-challenging-example",
    "href": "finitedifferences.html#more-challenging-example",
    "title": "The finite difference method",
    "section": "More challenging example",
    "text": "More challenging example\nLet \\(f(t) = \\sin(\\pi t / T)\\) such that \\(f''(t) = -(\\pi/T)^2 f(t)\\). Here none of the error terms will disappear and we see the effect of the poor first order boundary.\n\nf = np.sin(np.pi*t / T)\nd2fe = -(np.pi/T)**2*f\nd2f = D2 @ f\nd2f1 = D21 @ f\nplt.plot(t, d2fe, 'k', t, d2f, 'b', t, d2f1, 'r')\nplt.legend(['Exact', '2nd order', '1st order']);"
  },
  {
    "objectID": "finitedifferences.html#first-derivative-matrix",
    "href": "finitedifferences.html#first-derivative-matrix",
    "title": "The finite difference method",
    "section": "First derivative matrix",
    "text": "First derivative matrix\nLets create a similar matrix for a second order accurate single derivative.\n\\[ \\small\n\\begin{align}\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots\n\\end{align}\n\\]\nHere Eq. (1) minus Eq. (-1) leads to\n\\[\nu'(t_n) = \\frac{u^{n+1}-u^{n-1}}{2 h} + \\frac{h^2}{6} u''' +\n\\]\nwhich is second order accurate.\nThe central scheme cannot be used for \\(n=0\\) or \\(n=N\\)."
  },
  {
    "objectID": "finitedifferences.html#we-use-a-forward-scheme-for-n0",
    "href": "finitedifferences.html#we-use-a-forward-scheme-for-n0",
    "title": "The finite difference method",
    "section": "We use a forward scheme for \\(n=0\\)",
    "text": "We use a forward scheme for \\(n=0\\)\n\\[ \\small\n\\begin{align}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align}\n\\]\nFrom Eq. (1):\n\\[\nu'(t_n) = \\frac{u^{n+1}-u^n}{h} - \\frac{\\color{red}h\\color{black}}{2}u'' -\n\\]\nAdding one more equation (2) we get second order: \\((2)-4\\cdot (1)\\) (Note that the terms with \\(u''\\) then cancel)\n\\[\nu'(t_n) = \\frac{-u^{n+2}+4u^{n+1}-3u^n}{2h} + \\frac{\\color{red}h^2\\color{black}}{3}u''' +\n\\]"
  },
  {
    "objectID": "finitedifferences.html#with-forward-and-backward-on-the-edges-we-get-a-second-order-first-derivative-matrix",
    "href": "finitedifferences.html#with-forward-and-backward-on-the-edges-we-get-a-second-order-first-derivative-matrix",
    "title": "The finite difference method",
    "section": "With forward and backward on the edges we get a second order first derivative matrix",
    "text": "With forward and backward on the edges we get a second order first derivative matrix\n\\[ \\small\nD^{(1)} = \\frac{1}{2 h}\\begin{bmatrix}\n-3 & 4 & -1 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 0 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 0 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 0& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& 0& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3 \\\\\n\\end{bmatrix}\n\\]\n\nD1 = sparse.diags([-1, 1], np.array([-1, 1]), (N+1, N+1), 'lil')\nD1[0, :3] = -3, 4, -1\nD1[-1, -3:] = 1, -4, 3 \nD1 *= (1/(2*dt))\nf = t \nD1 @ f \n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
  },
  {
    "objectID": "finitedifferences.html#is-d2-the-same-as-d1d1",
    "href": "finitedifferences.html#is-d2-the-same-as-d1d1",
    "title": "The finite difference method",
    "section": "Is \\(D^{(2)}\\) the same as \\(D^{(1)}D^{(1)}\\)?",
    "text": "Is \\(D^{(2)}\\) the same as \\(D^{(1)}D^{(1)}\\)?\nIs it the same to compute \\(D^{(2)} \\boldsymbol{u}\\) as \\(D^{(1)}(D^{(1)}\\boldsymbol{u})\\)?\n\nAnalytically, we know that \\(u'' = \\frac{d^2u}{dt^2} = \\frac{d}{dt}\\left(\\frac{du}{dt}\\right)\\), but this is not necessarily so numerically.\n\nD22 = D1 @ D1 # @ represents matrix-matrix product \nD22.toarray()*4*dt**2\n\narray([[  5., -11.,   7.,  -1.,   0.,   0.,   0.,   0.,   0.],\n       [  3.,  -5.,   1.,   1.,   0.,   0.,   0.,   0.,   0.],\n       [  1.,   0.,  -2.,   0.,   1.,   0.,   0.,   0.,   0.],\n       [  0.,   1.,   0.,  -2.,   0.,   1.,   0.,   0.,   0.],\n       [  0.,   0.,   1.,   0.,  -2.,   0.,   1.,   0.,   0.],\n       [  0.,   0.,   0.,   1.,   0.,  -2.,   0.,   1.,   0.],\n       [  0.,   0.,   0.,   0.,   1.,   0.,  -2.,   0.,   1.],\n       [  0.,   0.,   0.,   0.,   0.,   1.,   1.,  -5.,   3.],\n       [  0.,   0.,   0.,   0.,   0.,  -1.,   7., -11.,   5.]])\n\n\nThis stencil is wider than \\(D^{(2)}\\), using neighboring points farther away. The central stencil is\n\\[\nu''(t_n) = \\frac{u^{n+2}-2u^{n}+u^{n-2}}{4h^2}\n\\]\nHow accurate is \\(D^{(1)}D^{(1)}\\)?"
  },
  {
    "objectID": "finitedifferences.html#internal-points",
    "href": "finitedifferences.html#internal-points",
    "title": "The finite difference method",
    "section": "Internal points",
    "text": "Internal points\n\\[ \\small\n\\begin{align*}\n(-2)\\quad u^{n-2} &= u^n - 2h u' + \\frac{2 h^2}{1}u'' - \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' - \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nTake Eq. (2) plus Eq. (-2)\n\\[\nu^{n+2} + u^{n-2} = 2u^n + 4h^2 u'' + \\frac{4h^4}{3}u'''' +\n\\]\nand\n\\[\nu''(t_n) = \\frac{u^{n+2}-2u^{n}+u^{n-2}}{4h^2} - \\frac{h^2}{3}u'''' + \\cdots\n\\]\nThe stencil in the center (for \\(1&lt;n&lt;N-1\\)) of \\(D^{(1)}D^{(1)}\\) is second order!"
  },
  {
    "objectID": "finitedifferences.html#how-about-boundaries",
    "href": "finitedifferences.html#how-about-boundaries",
    "title": "The finite difference method",
    "section": "How about boundaries?",
    "text": "How about boundaries?\nWe have obtained \\(u''(t_1) = \\frac{u^{n+2}+u^{n+1}-5u^{n}+3u^{n-1}}{4h^2}\\). Compute the error in this stencil using one backward and two forward points\n\\[ \\small\n\\begin{align*}\n(-1)\\quad u^{n-1} &= u^n - h u' + \\frac{h^2}{2}u'' - \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' - \\cdots \\\\\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots\n\\end{align*}\n\\]\nTake Eq. \\((2) + (1) + 3 \\cdot (-1)\\)\n\\[\nu''(t_1) = \\frac{u^{n+2}+u^{n+1}-5u^{n}+3u^{n-1}}{4h^2} + \\frac{\\color{red}h\\color{black}}{4}u''' + \\cdots\n\\]\nOnly first order."
  },
  {
    "objectID": "finitedifferences.html#how-about-the-border-point-for-n0",
    "href": "finitedifferences.html#how-about-the-border-point-for-n0",
    "title": "The finite difference method",
    "section": "How about the border point for \\(n=0\\)?",
    "text": "How about the border point for \\(n=0\\)?\nWe observe that \\(u''(t_0) = \\frac{-u^{n+3}+7u^{n+2}-11u^{n+1}+5u^{n}}{4h^2}\\)\n\\[ \\small\n\\begin{align*}\n(1)\\quad u^{n+1} &= u^n + h u' + \\frac{h^2}{2}u'' + \\frac{h^3}{6}u''' + \\frac{h^4}{24}u'''' + \\cdots \\\\\n(2)\\quad u^{n+2} &= u^n + 2h u' + \\frac{2 h^2}{1}u'' + \\frac{4 h^3}{3}u''' + \\frac{2 h^4}{3}u'''' + \\cdots \\\\\n(3)\\quad u^{n+3} &= u^n + 3h u' + \\frac{9 h^2}{2}u'' + \\frac{9 h^3}{2}u''' + \\frac{27 h^4}{8}u'''' + \\cdots \\\\\n\\end{align*}\n\\]\nTake Eq. \\(-(3)+7\\cdot (2) -11 \\cdot (1)\\) to obtain\n\\[\nu''(t_0) = \\frac{-u^{n+3}+7u^{n+2}-11u^{n+1}+5u^{n}}{4h^2} -\\frac{3 \\color{red}h\\color{black}}{4}u''' \\cdots\n\\]\nFirst order. So we conclude that \\(D^{(1)}D^{(1)}\\) is still an approximation of the second derivative, but it is only first order accurate near the boundaries."
  },
  {
    "objectID": "finitedifferences.html#test-the-accuracy-of-d2-d2-with-only-first-order-boundary-and-d1d1",
    "href": "finitedifferences.html#test-the-accuracy-of-d2-d2-with-only-first-order-boundary-and-d1d1",
    "title": "The finite difference method",
    "section": "Test the accuracy of \\(D^{(2)}\\), \\(D^{(2)}\\) with only first order boundary and \\(D^{(1)}D^{(1)}\\)",
    "text": "Test the accuracy of \\(D^{(2)}\\), \\(D^{(2)}\\) with only first order boundary and \\(D^{(1)}D^{(1)}\\)\n\nf = np.sin(np.pi*t / T)\nd2fe = -(np.pi/T)**2*f\nd2f = D2 @ f\nd2f1 = D21 @ f\nd2f2 = D1 @ D1 @ f\nplt.plot(t, d2fe, 'k', t, d2f, 'b', t, d2f1, 'r', t, d2f2, 'm')\nplt.legend(['Exact', '2nd order', '1st order', 'D1 @ D1']);"
  },
  {
    "objectID": "finitedifferences.html#the-main-advantage-of-using-difference-matrices-is-that-they-make-it-easy-to-solve-equations",
    "href": "finitedifferences.html#the-main-advantage-of-using-difference-matrices-is-that-they-make-it-easy-to-solve-equations",
    "title": "The finite difference method",
    "section": "The main advantage of using difference matrices is that they make it easy to solve equations",
    "text": "The main advantage of using difference matrices is that they make it easy to solve equations\n\nDiscretize \\(\\rightarrow\\) Simply replace \\(k\\)’th derivative in the ODE with the \\(k\\)’th derivative matrix\nApply boundary conditions to the assembled matrix\n\n\nConsider the exponential decay model\n\\[\nu' + au = 0, \\, t \\in (0, T], u(0)=I.\n\\]\nReplace derivatives with derivative matrices (\\(u'=D^{(1)}\\boldsymbol{u}, u = \\mathbb{I}\\boldsymbol{u}\\)):\n\\[\n(D^{(1)} + a \\mathbb{I})\\boldsymbol{u} = \\boldsymbol{0},\n\\]\nwhere \\(\\mathbb{I} \\in \\mathbb{R}^{N+1\\times N+1}\\) is the identity matrix and \\(\\boldsymbol{0}\\in \\mathbb{R}^{N+1}\\) is a null-vector. We get\n\\[\nA \\boldsymbol{u} = \\boldsymbol{0}, \\quad A = D^{(1)}+a\\mathbb{I}\n\\]\nwhich is trivially solved to \\(\\boldsymbol{u}=\\boldsymbol{0}\\) before adding boundary conditions. Not after."
  },
  {
    "objectID": "finitedifferences.html#apply-boundary-condition-by-modifying-both-the-coefficient-matrix-a-and-the-rhs-vector-boldsymbolb",
    "href": "finitedifferences.html#apply-boundary-condition-by-modifying-both-the-coefficient-matrix-a-and-the-rhs-vector-boldsymbolb",
    "title": "The finite difference method",
    "section": "Apply boundary condition by modifying both the coefficient matrix \\(A\\) and the rhs vector \\(\\boldsymbol{b}\\)",
    "text": "Apply boundary condition by modifying both the coefficient matrix \\(A\\) and the rhs vector \\(\\boldsymbol{b}\\)\nWith just the derivative matrix we have the problem\n\\[\nA \\boldsymbol{u} = \\boldsymbol{b}, \\quad \\text{for} \\quad \\boldsymbol{b}\\in \\mathbb{R}^{N+1}\n\\]\n\\[ \\small\n\\underbrace{\n  \\frac{1}{2h} \\begin{bmatrix}\n-3+2ah & 4 & -1 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 2ah & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 2ah & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 2ah& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& -2ah& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3+2ah\n\\end{bmatrix}\n}_{D^{(1)}+a\\mathbb{I}}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-2} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n=\n\\underbrace{\n\\begin{bmatrix}\nb^0 \\\\\nb^1 \\\\\nb^2 \\\\\n\\vdots \\\\\nb^{N-2} \\\\\nb^{N-1} \\\\\nb^{N} \\\\\n\\end{bmatrix}}_{\\boldsymbol{b}} =\n\\]\nIn order to enforce that \\(u(0)= u^0 = I\\), we can modify the first row of the coefficient matrix \\(A\\) and the right hand side vector \\(\\boldsymbol{b}\\)"
  },
  {
    "objectID": "finitedifferences.html#modify-a-and-boldsymbolb-to-enforce-boundary-conditions",
    "href": "finitedifferences.html#modify-a-and-boldsymbolb-to-enforce-boundary-conditions",
    "title": "The finite difference method",
    "section": "Modify \\(A\\) and \\(\\boldsymbol{b}\\) to enforce boundary conditions",
    "text": "Modify \\(A\\) and \\(\\boldsymbol{b}\\) to enforce boundary conditions\n\\[ \\small\n\\underbrace{\n  \\frac{1}{2h} \\begin{bmatrix}\n2h & 0 & 0 & 0  & 0 & 0 & 0 & 0  \\\\\n-1 & 2ah & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & -1 & 2ah & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  -1& 2ah& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& -1& -2ah& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 1 & -4 & 3+2ah\n\\end{bmatrix}\n}_{A}\n\\underbrace{\\begin{bmatrix}\nu^0 \\\\\nu^1 \\\\\nu^2 \\\\\n\\vdots \\\\\nu^{N-2} \\\\\nu^{N-1} \\\\\nu^{N}\n\\end{bmatrix}}_{\\boldsymbol{u}}\n=\n\\underbrace{\n\\begin{bmatrix}\nI \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{bmatrix}}_{\\boldsymbol{b}}\n\\]\nNow the equation in the first row states that \\(u^0 = I\\), whereas the remaining rows are unchanged and solve the central and implicit problem for row \\(n\\) (row \\(N\\) is slightly different)\n\\[\n\\frac{u^{n+1}-u^{n-1}}{2h} + au^n = 0\n\\]"
  },
  {
    "objectID": "finitedifferences.html#implement-decay-problem",
    "href": "finitedifferences.html#implement-decay-problem",
    "title": "The finite difference method",
    "section": "Implement decay problem",
    "text": "Implement decay problem\n\nN = 10\ndt = 0.5\na = 5\nT = N*dt\nt = np.linspace(0, N*dt, N+1) \nD1 = sparse.diags([-1, 1], np.array([-1, 1]), (N+1, N+1), 'lil')\nD1[0, :3] = -3, 4, -1 \nD1[-1, -3:] = 1, -4, 3\nD1 *= (1/(2*dt))\nId = sparse.eye(N+1)\nA = D1 + a*Id\nb = np.zeros(N+1)\nb[0] = I\nA[0, :3] = 1, 0, 0 # boundary condition\nA.toarray()\n\narray([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [-1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  5.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -4.,  8.]])"
  },
  {
    "objectID": "finitedifferences.html#compare-with-exact-solution",
    "href": "finitedifferences.html#compare-with-exact-solution",
    "title": "The finite difference method",
    "section": "Compare with exact solution",
    "text": "Compare with exact solution\n\n\n\n\n\n\n\n\n\nThe accuracy is similar to the Crank-Nicolson method discussed in lectures 1-2. However, the FD method is not a recursive marching method, since the equation for \\(u^n\\) depends on the solution at \\(u^{n+1}\\)! This implicit FD method is unconditionally stable. Normally, only marching methods are analysed for stability."
  },
  {
    "objectID": "finitedifferences.html#the-vibration-eq.-is-often-considered-as-a-bvp",
    "href": "finitedifferences.html#the-vibration-eq.-is-often-considered-as-a-bvp",
    "title": "The finite difference method",
    "section": "The vibration eq. is often considered as a BVP",
    "text": "The vibration eq. is often considered as a BVP\n\\[\nu''(t) + \\omega^2 u(t) = 0, \\quad u(0) = u(T) = I, \\quad t=(0, T)\n\\]\nDiscretize by replacing the derivative with a second differentiation matrix\n\\[\n(D^{(2)} + \\omega^2 \\mathbb{I}) \\boldsymbol{u} = \\boldsymbol{b}\n\\]\n\nw = 2*np.pi \nT, N, I = 3., 35, 1.\ndt = T/N \nD2 = sparse.diags([1, -2, 1], np.array([-1, 0, 1]), (N+1, N+1), 'lil')\nD2 *= (1/(dt**2)) # never mind the edges n=0 or n=N because these rows will be overwritten\nId = sparse.eye(N+1)\nA = D2 + w**2*Id\nb = np.zeros(N+1)\nA.toarray()*dt**2\n\narray([[-1.71,  1.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 1.  , -1.71,  1.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  1.  , -1.71, ...,  0.  ,  0.  ,  0.  ],\n       ...,\n       [ 0.  ,  0.  ,  0.  , ..., -1.71,  1.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  1.  , -1.71,  1.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  1.  , -1.71]])"
  },
  {
    "objectID": "finitedifferences.html#apply-boundary-conditions-to-the-first-and-last-rows",
    "href": "finitedifferences.html#apply-boundary-conditions-to-the-first-and-last-rows",
    "title": "The finite difference method",
    "section": "Apply boundary conditions to the first and last rows",
    "text": "Apply boundary conditions to the first and last rows\n\\[\n\\begin{align}\nu^0 &= I \\\\\nu^N &= I\n\\end{align}\n\\]\n\nA[0, :3] = 1, 0, 0\nA[-1, -3:] = 0, 0, 1\nb[0] = I\nb[-1] = I\nA.toarray()\n\narray([[   1.   ,    0.   ,    0.   , ...,    0.   ,    0.   ,    0.   ],\n       [ 136.111, -232.744,  136.111, ...,    0.   ,    0.   ,    0.   ],\n       [   0.   ,  136.111, -232.744, ...,    0.   ,    0.   ,    0.   ],\n       ...,\n       [   0.   ,    0.   ,    0.   , ..., -232.744,  136.111,    0.   ],\n       [   0.   ,    0.   ,    0.   , ...,  136.111, -232.744,  136.111],\n       [   0.   ,    0.   ,    0.   , ...,    0.   ,    0.   ,    1.   ]])"
  },
  {
    "objectID": "finitedifferences.html#solve-the-vibration-equation",
    "href": "finitedifferences.html#solve-the-vibration-equation",
    "title": "The finite difference method",
    "section": "Solve the vibration equation",
    "text": "Solve the vibration equation\n\nu2 = sparse.linalg.spsolve(A, b)\nt = np.linspace(0, T, N+1)\nplt.plot(t, u2, 'bo', t, I*np.cos(w*t), 'r--')\nplt.legend(['Numerical', 'Exact'])"
  },
  {
    "objectID": "finitedifferences.html#generic-finite-difference-stencils",
    "href": "finitedifferences.html#generic-finite-difference-stencils",
    "title": "The finite difference method",
    "section": "Generic finite difference stencils",
    "text": "Generic finite difference stencils\nWe have seen that it is quite simple to develop finite difference stencils for any derivative, using either forward or backward points. Can this be generalized?\n\nYes! Of course it can. The generic Taylor expansion around \\(x=x_0\\) reads\n\\[\nu(x) = \\sum_{i=0}^{M} \\frac{(x-x_0)^i}{i!} u^{(i)}(x_0) + \\mathcal{O}((x-x_0)^{M}),\n\\]\nwhere \\(u^{(i)}(x_0) = \\frac{d^{i}u}{dx^{i}}|_{x=x_0}\\) and the are \\(M+1\\) terms in the expansion.\nUse only \\(x=x_0+ph\\), where \\(p\\) is an integer and \\(h\\) is a constant (\\(\\Delta t\\) or \\(\\Delta x\\))\n\\[\nu^{n+p} = \\sum_{i=0}^{M} \\frac{(ph)^i}{i!} u^{(i)}(x_0) + \\mathcal{O}(h^{M+1}),\n\\]\nwhere \\(u^{n+p} = u(x_0+ph)\\)"
  },
  {
    "objectID": "finitedifferences.html#generic-fd",
    "href": "finitedifferences.html#generic-fd",
    "title": "The finite difference method",
    "section": "Generic FD",
    "text": "Generic FD\nThe truncated Taylor expansions for a given \\(p\\) can be written \\[\nu^{n+p} = \\sum_{i=0}^{M} \\frac{(ph)^i}{i!} u^{(i)}(x_0) = \\sum_{i=0}^{M} c_{pi} du_i\n\\]\nusing \\(c_{pi} = \\frac{(ph)^i}{i!}\\) and \\(du_i = u^{(i)}(x_0)\\).\n\nThis can be understood as a matrix-vector product\n\\[\n\\boldsymbol{u} = C \\boldsymbol{du},\n\\]\nwhere \\(\\boldsymbol{u} = (u^{n+p})_{p=p_0}^{M+p_0}\\), \\(C = (c_{p_0+p,i})_{p,i=0}^{M,M}\\) and \\(\\boldsymbol{du}=(du_i)_{i=0}^M\\). Here \\(p_0\\) is an integer representing the lowest value of \\(p\\) in the stencil.\nFor \\(p_0=-2\\) and \\(M=4\\):\n\\[\n\\boldsymbol{u} = (u^{n-2}, u^{n-1}, u^{n}, u^{n+1}, u^{n+2})^T \\quad\n\\boldsymbol{du} =(u^{(0)}, u^{(1)}, u^{(2)}, u^{(3)}, u^{(4)})^T\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-stencil-matrix",
    "href": "finitedifferences.html#the-stencil-matrix",
    "title": "The finite difference method",
    "section": "The stencil matrix",
    "text": "The stencil matrix\nFor \\(p_0=-2\\) and \\(M=4\\) we get 5 Taylor expansions\n\\[\n\\begin{align*}\nu^{n-2} &= \\sum_{i=0}^{M} \\frac{(-2h)^i}{i!} du_i  \\\\\nu^{n-1} &= \\sum_{i=0}^{M} \\frac{(-h)^i}{i!} du_i  \\\\\nu^{n} &= u^{n} \\\\\nu^{n+1} &= \\sum_{i=0}^{M} \\frac{(h)^i}{i!} du_i  \\\\\nu^{n+2} &= \\sum_{i=0}^{M} \\frac{(2h)^i}{i!} du_i  \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#the-stencil-matrix-ctd",
    "href": "finitedifferences.html#the-stencil-matrix-ctd",
    "title": "The finite difference method",
    "section": "The stencil matrix ctd",
    "text": "The stencil matrix ctd\nExpanding the sums these 5 Taylor expansions can be written in matrix form\n\\[ \\small\n\\underbrace{\n\\begin{bmatrix}\nu^{n-2}\\\\\nu^{n-1}\\\\\nu^{n}\\\\\nu^{n+1}\\\\\nu^{n+2}\\\\\n\\end{bmatrix}}_{\\boldsymbol{u}}\n= \\underbrace{\\begin{bmatrix}\n\\frac{(-2h)^0}{0!} & \\frac{(-2h)^1}{1!} & \\frac{(-2h)^2}{2!} & \\frac{(-2h)^3}{3!} & \\frac{(-2h)^4}{4!}  \\\\\n\\frac{(-h)^0}{0!} & \\frac{(-h)^1}{1!} & \\frac{(-h)^2}{2!} & \\frac{(-h)^3}{3!} & \\frac{(-h)^4}{4!} \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n\\frac{(h)^0}{0!} & \\frac{(h)^1}{1!} & \\frac{(h)^2}{2!} & \\frac{(h)^3}{3!} & \\frac{(h)^4}{4!} \\\\\n\\frac{(2h)^0}{0!} & \\frac{(2h)^1}{1!} & \\frac{(2h)^2}{2!} & \\frac{(2h)^3}{3!} & \\frac{(2h)^4}{4!} \\\\\n\\end{bmatrix}}_{C}\n\\underbrace{\n\\begin{bmatrix}\ndu_{0} \\\\\ndu_1 \\\\\ndu_2 \\\\\ndu_3 \\\\\ndu_4\n\\end{bmatrix}}_{\\boldsymbol{du}}\n\\]\n\\[\n\\boldsymbol{u} = C \\boldsymbol{du}\n\\]\n\nInvert to obtain\n\\[\n\\boldsymbol{du} = C^{-1} \\boldsymbol{u}\n\\]\nSince \\(du_i\\) is an approximation to the i’th derivative we can now compute any derivative stencil!"
  },
  {
    "objectID": "finitedifferences.html#second-order-2nd-derivative-stencil",
    "href": "finitedifferences.html#second-order-2nd-derivative-stencil",
    "title": "The finite difference method",
    "section": "Second order 2nd derivative stencil",
    "text": "Second order 2nd derivative stencil\nWe have been using the following stencil\n\\[\ndu_2 = u^{(2)}(x_0) = \\frac{u^{n+1}-2u^n+u^{n-1}}{h^2}.\n\\]\nLet’s derive this with the approach above. The scheme is central and second order so we use \\(p_0=-1\\) and \\(M=2\\) (hence \\(m=(-1, 0, 1)\\)). Insert into the recipe for \\(C\\)\n\\[\nC = \\begin{bmatrix}\n1 & -h & \\frac{h^2}{2} \\\\\n1 & 0 & 0 \\\\\n1 & h & \\frac{h^2}{2}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#in-sympy",
    "href": "finitedifferences.html#in-sympy",
    "title": "The finite difference method",
    "section": "In Sympy",
    "text": "In Sympy\n\nimport sympy as sp\nx, h = sp.symbols('x,h')\nC = sp.Matrix([[1, -h, h**2/2], [1, 0, 0], [1, h, h**2/2]])\n\n\n\nPrint \\(C\\) matrix\n\nC\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & - h & \\frac{h^{2}}{2}\\\\1 & 0 & 0\\\\1 & h & \\frac{h^{2}}{2}\\end{matrix}\\right]\\)\n\n\n\nPrint \\(C^{-1}\\) matrix\n\nC.inv() \n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 1 & 0\\\\- \\frac{1}{2 h} & 0 & \\frac{1}{2 h}\\\\\\frac{1}{h^{2}} & - \\frac{2}{h^{2}} & \\frac{1}{h^{2}}\\end{matrix}\\right]\\)\n\n\n\n\nThe last row in \\(C^{-1}\\) represents \\(u''\\)! The middle row represents a second order central \\(u'\\).\nCreate a vector for \\(\\boldsymbol{u}\\) and print \\(u'\\) and \\(u''\\)\n\nu = sp.Function('u')\ncoef = sp.Matrix([u(x-h), u(x), u(x+h)]) \n\n\n\n\n(C.inv()[1, :] @ coef)[0]\n\n\\(\\displaystyle - \\frac{u{\\left(- h + x \\right)}}{2 h} + \\frac{u{\\left(h + x \\right)}}{2 h}\\)\n\n\n\n\n(C.inv()[2, :] @ coef)[0]\n\n\\(\\displaystyle - \\frac{2 u{\\left(x \\right)}}{h^{2}} + \\frac{u{\\left(- h + x \\right)}}{h^{2}} + \\frac{u{\\left(h + x \\right)}}{h^{2}}\\)"
  },
  {
    "objectID": "finitedifferences.html#we-can-get-any-stencil",
    "href": "finitedifferences.html#we-can-get-any-stencil",
    "title": "The finite difference method",
    "section": "We can get any stencil",
    "text": "We can get any stencil\nCreate a function that computes \\(C\\) for any \\(p_0\\) and \\(M\\)\n\ndef Cmat(p0, M):\n  C = np.zeros((M+1, M+1), dtype=object)\n  for j, p in enumerate(range(p0, p0+M+1)):\n    for i in range(M+1):\n      C[j, i] = (p*h)**i / sp.factorial(i) \n  return sp.Matrix(C)\n\n\\(p_0=-1, M=2\\)\n\nCmat(-1, 2)\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & - h & \\frac{h^{2}}{2}\\\\1 & 0 & 0\\\\1 & h & \\frac{h^{2}}{2}\\end{matrix}\\right]\\)\n\n\n\nA central stencil of order \\(l\\) for the \\(k\\)’th’ derivative requires \\(M+1\\) points, where\n\\[\nM = l + 2 \\left\\lfloor \\frac{k-1}{2} \\right\\rfloor\n\\]"
  },
  {
    "objectID": "finitedifferences.html#forward-and-backward",
    "href": "finitedifferences.html#forward-and-backward",
    "title": "The finite difference method",
    "section": "Forward and backward",
    "text": "Forward and backward\nNon-central schemes requires one more point for the same accuracy as central\n\n\nForward \\(u''\\)\n\\(p_0=0, M=3\\)\n\nC = Cmat(0, 3)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\- \\frac{11}{6 h} & \\frac{3}{h} & - \\frac{3}{2 h} & \\frac{1}{3 h}\\\\\\frac{2}{h^{2}} & - \\frac{5}{h^{2}} & \\frac{4}{h^{2}} & - \\frac{1}{h^{2}}\\\\- \\frac{1}{h^{3}} & \\frac{3}{h^{3}} & - \\frac{3}{h^{3}} & \\frac{1}{h^{3}}\\end{matrix}\\right]\\)\n\n\n\nBackward \\(u''\\)\n\\(p_0 = -3, M=3\\)\n\nC = Cmat(-3, 3)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 1\\\\- \\frac{1}{3 h} & \\frac{3}{2 h} & - \\frac{3}{h} & \\frac{11}{6 h}\\\\- \\frac{1}{h^{2}} & \\frac{4}{h^{2}} & - \\frac{5}{h^{2}} & \\frac{2}{h^{2}}\\\\- \\frac{1}{h^{3}} & \\frac{3}{h^{3}} & - \\frac{3}{h^{3}} & \\frac{1}{h^{3}}\\end{matrix}\\right]\\)\n\n\n\n\nRecognize the third row in the forward scheme:\n\\[\nu'' = \\frac{-u^{n+3} + 4u^{n+2} - 5u^{n+1} +2 u^{n}}{h^2}\n\\]"
  },
  {
    "objectID": "finitedifferences.html#fourth-order-central-small-p_0-2-m4",
    "href": "finitedifferences.html#fourth-order-central-small-p_0-2-m4",
    "title": "The finite difference method",
    "section": "Fourth order central \\(\\small p_0=-2, M=4\\)",
    "text": "Fourth order central \\(\\small p_0=-2, M=4\\)\n\nC = Cmat(-2, 4)\nC.inv()\n\n\\(\\displaystyle \\left[\\begin{matrix}0 & 0 & 1 & 0 & 0\\\\\\frac{1}{12 h} & - \\frac{2}{3 h} & 0 & \\frac{2}{3 h} & - \\frac{1}{12 h}\\\\- \\frac{1}{12 h^{2}} & \\frac{4}{3 h^{2}} & - \\frac{5}{2 h^{2}} & \\frac{4}{3 h^{2}} & - \\frac{1}{12 h^{2}}\\\\- \\frac{1}{2 h^{3}} & \\frac{1}{h^{3}} & 0 & - \\frac{1}{h^{3}} & \\frac{1}{2 h^{3}}\\\\\\frac{1}{h^{4}} & - \\frac{4}{h^{4}} & \\frac{6}{h^{4}} & - \\frac{4}{h^{4}} & \\frac{1}{h^{4}}\\end{matrix}\\right]\\)\n\n\nThird row gives us:\n\\[\nu'' = \\frac{-u^{n+2} + 16u^{n+1} - 30 u^n + 16 u^{n-1} - u^{n-2}}{12 h^2} + \\mathcal{O}(h^4)\n\\]\nwhere the order \\(l = M - 2 \\left\\lfloor \\frac{2-1}{2} \\right \\rfloor = M = 4\\).\n\nWhat is the order of the fourth derivative \\(u''''=\\frac{u^{n+2}-4u^{n+1}+6u^{n}-4u^{n-1}+u^{n-2}}{h^4}\\)?\n\n\n\\(l = M - 2 \\left\\lfloor \\frac{4-1}{2} \\right \\rfloor = M - 2 = 2\\)"
  },
  {
    "objectID": "wave.html#the-wave-equation-is-a-partial-differential-equation-pde",
    "href": "wave.html#the-wave-equation-is-a-partial-differential-equation-pde",
    "title": "Finite difference methods for the wave equation",
    "section": "The wave equation is a partial differential equation (PDE)",
    "text": "The wave equation is a partial differential equation (PDE)\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2}.\n\\]\nWe will consider the time and space domains: \\(t\\in[0, T]\\), \\(x \\in [0, L]\\).\n\n\nThe wave equation is an initial-boundary value problem!\nTwo initial conditions required since two derivatives in time\nTwo boundary conditions required since two derivatives in space\nThe solutions are waves that can be written as \\(u(x+ct)\\) and \\(u(x-ct)\\)"
  },
  {
    "objectID": "wave.html#wave-solution-with-different-boundary-conditions",
    "href": "wave.html#wave-solution-with-different-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Wave solution with different boundary conditions",
    "text": "Wave solution with different boundary conditions"
  },
  {
    "objectID": "wave.html#boundary-conditions",
    "href": "wave.html#boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Boundary conditions",
    "text": "Boundary conditions\n\n\nDirichlet (Fixed end)\n\n\\[\nu(0, t) = u(L, t) = 0\n\\]\n\n\nThe wave will be reflected, but \\(u\\) will change sign. A nonzero Dirichlet condition is also possible, but will not be considered here.\n\n\n\nNeumann (Loose end)\n\n\\[\n\\frac{\\partial u}{\\partial x}(0, t) = \\frac{\\partial u}{\\partial x}(L, t) = 0\n\\]\n\n\nThe wave will be reflected without change in sign. A nonzero Neumann condition is also possible, but will not be considered here."
  },
  {
    "objectID": "wave.html#boundary-conditions-continued",
    "href": "wave.html#boundary-conditions-continued",
    "title": "Finite difference methods for the wave equation",
    "section": "Boundary conditions continued",
    "text": "Boundary conditions continued\n\n\nOpen boundary (No end)\n\n\\[\n\\frac{\\partial u(0, t)}{\\partial t} - c \\frac{\\partial u(0, t)}{\\partial x}= 0\n\\]\n\\[\n\\frac{\\partial u(L, t)}{\\partial t} + c \\frac{\\partial u(L, t)}{\\partial x}=0\n\\]\n\n\nThe wave will simply pass undisturbed and unreflected through an open boundary.\n\n\n\nPeriodic boundary (No end)\n\n\\[\nu(0, t) = u(L, t)\n\\]\n\n\nThe solution repeats itself indefinitely."
  },
  {
    "objectID": "wave.html#discretization",
    "href": "wave.html#discretization",
    "title": "Finite difference methods for the wave equation",
    "section": "Discretization",
    "text": "Discretization\nThe simplest possible discretization is uniform in time and space\n\\[\nt_n = n \\Delta t, \\quad n = 0, 1, \\ldots, N_t\n\\]\n\\[\nx_j = j \\Delta x, \\quad j = 0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "wave.html#a-mesh-function-in-space-and-time-is-defined-as",
    "href": "wave.html#a-mesh-function-in-space-and-time-is-defined-as",
    "title": "Finite difference methods for the wave equation",
    "section": "A mesh function in space and time is defined as",
    "text": "A mesh function in space and time is defined as\n\\[\nu^n_j = u(x_j, t_n)\n\\]\nThe mesh function has one value at each node in the mesh. For simplicity in later algorithms we will use the vectors\n\\[\nu^n = (u^n_0, u^n_1, \\ldots, u^n_N)^T,\n\\]\nwhich is the solution vector at time \\(t_n\\).\nA second order accurate discretization of the wave equation is\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points",
    "href": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil makes use of 5 neighboring points",
    "text": "The finite difference stencil makes use of 5 neighboring points\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\nCan only be used for internal points"
  },
  {
    "objectID": "wave.html#we-use-a-marching-method-in-time",
    "href": "wave.html#we-use-a-marching-method-in-time",
    "title": "Finite difference methods for the wave equation",
    "section": "We use a marching method in time",
    "text": "We use a marching method in time\n\nInitialize \\(u^0\\) and \\(u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\nfor j in range(\\(1, N-1\\)):\n\n\\(u^{n+1}_j = 2u^n_j - u^{n-1}_j + \\left(\\frac{c \\Delta t}{ \\Delta x}\\right)^2 (u^{n}_{j+1}-2u^n_{j} + u^n_{j-1})\\)\n\n\n\nand apply the chosen boundary conditions.\n\nAll the indices makes it a bit messy. Lets make use of a differentiation matrix for the spatial dimension! And the Courant (or CFL) number\n\\[\n\\overline{c} = \\frac{c \\Delta t}{\\Delta x}\n\\]"
  },
  {
    "objectID": "wave.html#use-differentiation-matrix-to-simplify-the-notation",
    "href": "wave.html#use-differentiation-matrix-to-simplify-the-notation",
    "title": "Finite difference methods for the wave equation",
    "section": "Use differentiation matrix to simplify the notation",
    "text": "Use differentiation matrix to simplify the notation\nWe define the second differentiation matrix without the scaling \\(1/(\\Delta x)^2\\) such that\n\\[\\small\nD^{(2)} = \\begin{bmatrix}\n-2 & 1 & 0 & 0  & 0 & 0 & 0 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 0 & 1 & -2 \\\\\n\\end{bmatrix}\n\\]\nand thus row \\(0 &lt; j &lt; N\\) of \\(D^{(2)}u^n\\) becomes\n\\[\n(D^{(2)}u^n)_j = u^n_{j+1}-2u^n_j+u^n_{j-1}\n\\]"
  },
  {
    "objectID": "wave.html#the-vectorized-marching-method-becomes",
    "href": "wave.html#the-vectorized-marching-method-becomes",
    "title": "Finite difference methods for the wave equation",
    "section": "The vectorized marching method becomes",
    "text": "The vectorized marching method becomes\n\nInitialize \\(u^0\\) and \\(u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\\)\nApply boundary conditions to \\(u^{n+1}_0\\) and \\(u^{n+1}_N\\)\n\n\n\n\nThe boundary step can often, but not always, be incorporated into the matrix \\(D^{(2)}\\)\nVery easy to vectorize using the matrix vector product!"
  },
  {
    "objectID": "wave.html#pde-solvers-of-time-dependent-problems-should-use-memory-carefully",
    "href": "wave.html#pde-solvers-of-time-dependent-problems-should-use-memory-carefully",
    "title": "Finite difference methods for the wave equation",
    "section": "PDE solvers (of time-dependent problems) should use memory carefully",
    "text": "PDE solvers (of time-dependent problems) should use memory carefully\n\n\n\n\n\n\nNote\n\n\n\nAt any time we only need to store three vectors: \\(u^{n+1}, u^{n}\\) and \\(u^{n-1}\\).\n\nMemory requirement = \\(3(N+1)\\) floating point numbers\n\nStoring all time steps requires \\((N_t+1) \\times (N+1)\\) floating point numbers\nNot a huge problem for our case, but for 2 or 3 spatial dimensions it is very important!"
  },
  {
    "objectID": "wave.html#implementation---a-low-memory-marching-method-needs-to-update-solution-vectors",
    "href": "wave.html#implementation---a-low-memory-marching-method-needs-to-update-solution-vectors",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation - A low-memory marching method needs to update solution vectors",
    "text": "Implementation - A low-memory marching method needs to update solution vectors\n\nAllocate three vectors \\(u^{nm1}, u^n, u^{np1}\\), representing \\(u^{n-1}, u^n, u^{n+1}\\).\nInitialize \\(u^0\\) and \\(u^1\\) by setting \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 D^{(2)} u^n\\)\nApply boundary conditions to \\(u^{np1}_0\\) and \\(u^{np1}_N\\)\nUpdate to next iteration:\n\n\\(u^{nm1} \\gets u^n\\)\n\\(u^n \\gets u^{np1}\\)"
  },
  {
    "objectID": "wave.html#in-python",
    "href": "wave.html#in-python",
    "title": "Finite difference methods for the wave equation",
    "section": "In Python",
    "text": "In Python\nSet up solver\n\nimport numpy as np \nfrom scipy import sparse\nimport sympy as sp \nx, t = sp.symbols('x,t')\nN = 100\nNt = 500 \nL = 2\nc = 1 # wavespeed\ndx = L / N\nCFL = 1.0\ndt = CFL*dx/c\nxj = np.linspace(0, L, N+1)\nunm1, un, unp1 = np.zeros((3, N+1))\nD2 = sparse.diags([1, -2, 1], [-1, 0, 1], (N+1, N+1))\nu0 = sp.exp(-200*(x-L/2+t)**2)\n\nSolve by marching method\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1"
  },
  {
    "objectID": "wave.html#store-results-at-intermediate-intervals-for-plotting",
    "href": "wave.html#store-results-at-intermediate-intervals-for-plotting",
    "title": "Finite difference methods for the wave equation",
    "section": "Store results at intermediate intervals for plotting",
    "text": "Store results at intermediate intervals for plotting\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nplotdata = {0: unm1.copy()}\nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1\n  if n % 10 == 0:\n    plotdata[n] = unp1.copy()\n\nFor example every tenth time step. Normally you do not need every time step to get a good animation."
  },
  {
    "objectID": "wave.html#create-animation-after-the-simulation-is-finished",
    "href": "wave.html#create-animation-after-the-simulation-is-finished",
    "title": "Finite difference methods for the wave equation",
    "section": "Create animation after the simulation is finished",
    "text": "Create animation after the simulation is finished\n\n\ndef animation(data):\n  from matplotlib import animation \n  fig, ax = plt.subplots()\n  v = np.array(list(data.values()))\n  t = np.array(list(data.keys()))\n  save_step = t[1]-t[0]\n  line, = ax.plot(xj, data[0])\n  ax.set_ylim(v.min(), v.max())\n  def update(frame):\n    line.set_ydata(data[frame*save_step])\n    return (line,)\n  ani = animation.FuncAnimation(fig=fig, func=update, frames=len(data), blit=True)\n  ani.save('wavemovie.apng', writer='pillow', fps=5) # This animated png opens in a browser"
  },
  {
    "objectID": "wave.html#how-to-implement-the-initial-conditions",
    "href": "wave.html#how-to-implement-the-initial-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "How to implement the initial conditions?",
    "text": "How to implement the initial conditions?\nTo initialize a mesh function \\(u^0\\), we write \\[\nu^0 = I(x)\n\\]\nwhich represents\n\\[\nu^0_j = I(x_j), \\quad \\forall \\, j=0, 1, \\ldots, N\n\\]\n\n\nu0 = sp.exp(-200*(x-L/2+t)**2) \nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj) \n\n\n\nHow about the second condition \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\)?\n\n\nJust like for the vibration equation there are several options. If you have an analytical solution \\(I(x, t)\\) that is time-dependent you can specify one wave as:\n\\[\nu^1 = I(x, \\Delta t)\n\\]\n\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj)"
  },
  {
    "objectID": "wave.html#if-you-do-not-have-ix-t-then-what",
    "href": "wave.html#if-you-do-not-have-ix-t-then-what",
    "title": "Finite difference methods for the wave equation",
    "section": "If you do not have \\(I(x, t)\\), then what?",
    "text": "If you do not have \\(I(x, t)\\), then what?\n\n\nHow to fix \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 1\nUse a forward difference\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) \\approx \\frac{u^1-u^0}{\\Delta t}=0, \\quad \\text{such that} \\quad u^1 = u^0\n\\]\nOnly first order accurate, but still a possibility.\n\n\nUse a second order forward difference\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) \\approx \\frac{-u^2+4u^1-3u^0}{2 \\Delta t}=0, \\quad \\text{such that} \\quad u^1 = \\frac{3u^0+u^2}{4}\n\\]\nSecond order accurate, but implicit. And you get two waves!"
  },
  {
    "objectID": "wave.html#how-to-implement-fracpartial-upartial-tx-0-0-option-2",
    "href": "wave.html#how-to-implement-fracpartial-upartial-tx-0-0-option-2",
    "title": "Finite difference methods for the wave equation",
    "section": "How to implement \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 2",
    "text": "How to implement \\(\\frac{\\partial u}{\\partial t}(x, 0) = 0\\), option 2\nUse a second order central differece\n\\[\n\\frac{\\partial u}{\\partial t}(x, 0) = \\frac{u^1-u^{-1}}{2 \\Delta t}=0, \\quad \\text{such that} \\quad u^1 = u^{-1}\n\\]\nand the PDE at \\(n=0\\)\n\\[\nu^1 = 2u^0 - u^{-1} + \\underline{c}^2 D^{(2)}u^{0}\n\\]\n\nInsert for \\(u^{-1}=u^1\\) to obtain\n\\[\nu^1 = u^0 + \\frac{\\underline{c}^2}{2} D^{(2)}u^{0}\n\\]\nSecond order accurate and explicit"
  },
  {
    "objectID": "wave.html#how-to-fix-boundary-conditions",
    "href": "wave.html#how-to-fix-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "How to fix boundary conditions?",
    "text": "How to fix boundary conditions?\nWe will consider 4 different types of boundary conditions\n\n\n\n\n\n\n\nDirichlet\n\\(u(0, t)\\) and \\(u(L, t)\\)\n\n\nNeumann\n\\(\\frac{\\partial u}{\\partial x}(0, t)\\) and \\(\\frac{\\partial u}{\\partial x}(L, t)\\)\n\n\nOpen\n\\(\\frac{\\partial u}{\\partial t}(0, t)-c\\frac{\\partial u}{\\partial x}(0, t) = 0\\) and \\(\\frac{\\partial u}{\\partial t}(L, t)+c\\frac{\\partial u}{\\partial x}(L, t) = 0\\)\n\n\nPeriodic\n\\(u(L, t) = u(0, t)\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAccounting for boundary conditions very often takes more than 50 % of the lines of code in a PDE solver!"
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions",
    "href": "wave.html#dirichlet-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions",
    "text": "Dirichlet boundary conditions\nWe need to fix \\(u(0, t) = I(0)\\) and \\(u(L, t) = I(L)\\) and start by fixing this at \\(t=0\\)\n\\[\nu^0_0=I(0) \\quad \\text{and}\\quad u^0_N=I(L)\n\\]\n\nNext, we compute\n\\[\nu^1 = u^0 + \\frac{\\underline{c}^2}{2} D^{(2)}u^{0}\n\\]\nHere, if the first and last rows of \\(D^{(2)}\\) are set to zero, then \\(u^1_0 = u^0_0\\) and \\(u^1_N=u^0_N\\).\n\n\nNext, for \\(n=1, 2, \\ldots, N_t-1\\)\n\\[\nu^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\n\\]\nAgain, if the first and last rows of \\(D^{(2)}\\) are zero, then \\(u^{n+1}_0 = u^0_0\\) and \\(u^{n+1}_N=u^0_N\\) for all \\(n\\). The boundary values remain as initially set at \\(t=0\\)."
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions-summary",
    "href": "wave.html#dirichlet-boundary-conditions-summary",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions summary",
    "text": "Dirichlet boundary conditions summary\nSet \\(u^0=I(x)\\) and define a modified differentiation matrix\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0 \\\\\n\\end{bmatrix}\n\\]\nNow boundary conditions will be ok at all time steps simply by:\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\nUpdate to next iteration: \\(u^{nm1} = u^n; u^n = u^{np1}\\)"
  },
  {
    "objectID": "wave.html#dirichlet-boundary-conditions-summary-1",
    "href": "wave.html#dirichlet-boundary-conditions-summary-1",
    "title": "Finite difference methods for the wave equation",
    "section": "Dirichlet boundary conditions summary",
    "text": "Dirichlet boundary conditions summary\n\n\n\n\n\n\nNote\n\n\nIt is also possible to do nothing with \\(D^{(2)}\\) and simply fix the boundary conditions after updating all the internal points\n\n\n\n\n\nInitialize \\(u^{nm1}=u^0\\) and compute \\(u^n = u^1 = u^0+\\frac{\\underline{c}^2}{2}{D}^{(2)} u^0\\).\nSet \\(u^{nm1}_0=u^n_0=0\\) and \\(u^{nm1}_N=u^n_N=0\\).\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 {D}^{(2)} u^n\\)\nSet \\(u^{np1}_0=0\\) and \\(u^{np1}_N=0\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)\n\n\n\n\n\n\n\n\nNote\n\n\nRegular, unmodified \\(D^{(2)}\\), where the first and last rows are completely irrelevant."
  },
  {
    "objectID": "wave.html#neumann-boundary-conditions",
    "href": "wave.html#neumann-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann boundary conditions",
    "text": "Neumann boundary conditions\nWe need to fix \\(\\frac{\\partial u}{\\partial x}(0, t) = 0\\) and \\(\\frac{\\partial u}{\\partial x}(L, t) = 0\\). We already have \\(u^0=I(x)\\).\n\nA second order central scheme at \\(x=0\\) is using ghost cell at \\(j=-1\\)\n\\[\n\\frac{\\partial u}{\\partial x}(0, t_n) = \\frac{u^{n}_1 - u^n_{-1}}{2 \\Delta x} = 0 \\rightarrow u^n_{-1} = u^{n}_{1}\n\\]"
  },
  {
    "objectID": "wave.html#neumann-at-xl-is-the-same",
    "href": "wave.html#neumann-at-xl-is-the-same",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann at \\(x=L\\) is the same",
    "text": "Neumann at \\(x=L\\) is the same\n\\[\n\\frac{\\partial u}{\\partial x}(L, t_n) = \\frac{u^{n}_{N+1} - u^n_{N-1}}{2 \\Delta x} = 0 \\rightarrow u^n_{N+1} = u^{n}_{N-1}\n\\]\nThe PDE at the right hand side \\(j=N\\) using ghost cell:\n\\[\nu^{n+1}_N = 2u^{n}_N - u^{n-1}_N + \\underline{c}^2(u^n_{N+1}-2u^n_N+u^n_{N-1})\n\\]\nInsert for \\(u^{n}_{N+1}=u^n_{N-1}\\) and obtain\n\\[\nu^{n+1}_N = 2u^{n}_N - u^{n-1}_N + \\underline{c}^2(2u^n_{N-1}-2u^n_N)\n\\]\n\nAnd for \\(n=1\\) we similarly get\n\\[\nu^{1}_0 = u^{0}_0 +\\frac{\\underline{c}^2}{2}(2u^n_{1}-2u^n_0) \\quad \\text{and} \\quad u^{1}_N = u^{0}_N + \\frac{\\underline{c}^2}{2}(2u^n_{N-1}-2u^n_N)\n\\]"
  },
  {
    "objectID": "wave.html#neumann-summary",
    "href": "wave.html#neumann-summary",
    "title": "Finite difference methods for the wave equation",
    "section": "Neumann summary",
    "text": "Neumann summary\nSet \\(u^0=I(x)\\) and define a modified differentiation matrix\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}-2 & \\color{red}2 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n\\color{red}0 & \\color{red}0 & \\color{red}0 & \\color{red}0  & \\color{red}0 & \\color{red}0 & \\color{red}2 & \\color{red}-2 \\\\\n\\end{bmatrix}\n\\]\nNow boundary conditions will be ok at all time steps simply by:\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)"
  },
  {
    "objectID": "wave.html#open-boundary",
    "href": "wave.html#open-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "Open boundary",
    "text": "Open boundary\nThe wave simply disappears through the boundary\n\\[\n\\frac{\\partial u}{\\partial t}(0, t) - c \\frac{\\partial u}{\\partial x}(0, t) = 0 \\quad \\text{and} \\quad \\frac{\\partial u}{\\partial t}(L, t) + c \\frac{\\partial u}{\\partial x}(L, t) = 0\n\\]\nAs for Neumann there are several ways to implement these boundary conditions. The simplest option is to solve the first order accurate\n\\[\n\\frac{u^{n+1}_0-u^{n}_0}{\\Delta t} - c \\frac{u^n_1-u^{n}_{0}}{\\Delta x} = 0\n\\]\nsuch that\n\\[\nu^{n+1}_0 = u^n_0 + \\frac{c \\Delta t}{\\Delta x}(u^n_1-u^n_0)\n\\]"
  },
  {
    "objectID": "wave.html#second-order-option",
    "href": "wave.html#second-order-option",
    "title": "Finite difference methods for the wave equation",
    "section": "Second order option",
    "text": "Second order option\n\\[\n\\frac{u^{n+1}_0-u^{n-1}_0}{2\\Delta t} - c \\frac{-u^n_2+4u^n_1-3u^{n}_{0}}{2 \\Delta x} = 0\n\\]\nSolve for the boundary node \\(u^{n+1}_0\\)\n\\[\nu^{n+1}_0 = u^{n-1}_0 + \\frac{c \\Delta t}{\\Delta x}(-u^n_2+4u^n_1-3u^n_0)\n\\]\nNice option, but difficult to incorporate in the \\(D^{(2)}\\) matrix, since there is no way to modify the first and last rows of \\(D^{(2)}\\) such that\n\\[\nu^{n+1}_0 = 2u^n_0-u^{n-1}_0 + \\underline{c}^2 (D^{(2)} u^n)_0\n\\]"
  },
  {
    "objectID": "wave.html#second-second-order-option",
    "href": "wave.html#second-second-order-option",
    "title": "Finite difference methods for the wave equation",
    "section": "Second second order option",
    "text": "Second second order option\nUse central, second order scheme\n\\[\n\\frac{u^{n+1}_0-u^{n-1}_0}{2\\Delta t} - c \\frac{u^n_1-u^{n}_{-1}}{2 \\Delta x} = 0\n\\]\nand isolate the ghost node \\(u^{n}_{-1}\\):\n\\[\n\\color{red}u^{n}_{-1} \\color{black} = u^n_1 - \\frac{1}{\\underline{c}}(u^{n+1}_0 - u^{n-1}_0)\n\\]\n\nUse regular PDE at the boundary that includes the ghost node:\n\\[\nu^{n+1}_0 = 2u^n_0 - u^{n-1}_0 + \\underline{c}^2(u^{n}_1-2u^n_0+\\color{red}u^n_{-1}\\color{black})\n\\]\nThis gives an equation for \\(u^{n+1}_0\\) that fixes the open boundary condition:\n\\[\nu^{n+1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_1\n\\]"
  },
  {
    "objectID": "wave.html#open-boundary-conditions",
    "href": "wave.html#open-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Open boundary conditions",
    "text": "Open boundary conditions\nLeft boundary:\n\\[\nu^{n+1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_1\n\\]\nRight boundary:\n\\[\nu^{n+1}_N = 2(1-\\underline{c})u^n_N - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{n-1}_N + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{n_1}_{N-1}\n\\]\nBoth explicit and second order. But not possible to implement into the matrix such that\n\\[\nu^{n+1} = 2u^n-u^{n-1} + \\underline{c}^2 D^{(2)} u^n\n\\]"
  },
  {
    "objectID": "wave.html#implementation-open-boundaries",
    "href": "wave.html#implementation-open-boundaries",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation open boundaries",
    "text": "Implementation open boundaries\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(\\(1, N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 {D}^{(2)} u^n\\)\n\\(u^{np1}_0 = 2(1-\\underline{c})u^n_0 - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{nm1}_0 + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{nm1}_1\\)\n\\(u^{np1}_N = 2(1-\\underline{c})u^n_N - \\frac{1-\\underline{c}}{1+\\underline{c}}u^{nm1}_N + \\frac{2 \\underline{c}^2}{1+\\underline{c}}u^{nm1}_{N-1}\\)\nUpdate to next iteration: \\(u^{nm1} \\gets u^n; u^n \\gets u^{np1}\\)\n\n\n\n\n\n\n\n\nNote\n\n\nThere is no need to use a modified \\(D^{(2)}\\). The two updates of \\(u^{np1}_0\\) and \\(u^{np1}_N\\) will overwrite anything computed in the first step."
  },
  {
    "objectID": "wave.html#periodic-boundary-conditions",
    "href": "wave.html#periodic-boundary-conditions",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic boundary conditions",
    "text": "Periodic boundary conditions\nA periodic solution is a solution that is repeating itself indefinitely. For example \\(u(x) = \\sin(2\\pi x)\\):\n\n\n\n\n\n\n\n\n\nWe solve the problem for example for \\(x \\in [0, 1]\\), but the actual solution will be like above, with no boundaries.\n\n\n\n\n\n\nNote\n\n\nA periodic domain is also referred to as a domain with no boundaries."
  },
  {
    "objectID": "wave.html#a-periodic-mesh-in-time",
    "href": "wave.html#a-periodic-mesh-in-time",
    "title": "Finite difference methods for the wave equation",
    "section": "A periodic mesh in time",
    "text": "A periodic mesh in time\n\n\\[u(t_N) = u(0) \\quad \\text{or} \\quad u^N = u^0\\]\n\n\n\n\n\n\nNote\n\n\nThere are only \\(N\\) unknowns \\(u^0, u^1, \\ldots, u^{N-1}\\) for a mesh with \\(N+1\\) nodes."
  },
  {
    "objectID": "wave.html#consider-the-discretization-of-u",
    "href": "wave.html#consider-the-discretization-of-u",
    "title": "Finite difference methods for the wave equation",
    "section": "Consider the discretization of \\(u''\\)",
    "text": "Consider the discretization of \\(u''\\)\n\nAt the left hand side of the domain, the point to the left of \\(u^0\\) is \\(u^{N-1}\\)\n\\[\nu''(0) \\approx \\frac{u^1 - 2u^0 + \\color{red}u^{-1}}{h^2} = \\frac{u^1 - 2u^0 + \\color{red}u^{N-1}}{h^2}\n\\]\nAt the right hand side of the domain the point to the right of \\(u^{N-1}\\) is \\(u^N=u^0\\)\n\\[\nu''(t_{N-1}) \\approx \\frac{\\color{red}u^N \\color{black} - 2u^{N-1} + u^{N-2}}{h^2} = \\frac{\\color{red}u^0 \\color{black} - 2u^{N-1} + u^{N-2}}{h^2}\n\\]"
  },
  {
    "objectID": "wave.html#periodic-boundary-conditions-can-be-implemented-in-the-matrix-d2-in-mathbbrn1-times-n1",
    "href": "wave.html#periodic-boundary-conditions-can-be-implemented-in-the-matrix-d2-in-mathbbrn1-times-n1",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic boundary conditions can be implemented in the matrix \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\)",
    "text": "Periodic boundary conditions can be implemented in the matrix \\(D^{(2)} \\in \\mathbb{R}^{N+1 \\times N+1}\\)\n\\[\\small\n\\tilde{D}^{(2)} = \\begin{bmatrix}\n\\color{red}-2 & \\color{red}1 & 0 & 0  & 0 & 0 & \\color{red}1 & 0  \\\\\n1 & -2 & 1 & 0 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & -2 & 1 & 0 & 0 & 0 & \\cdots \\\\\n\\vdots & & & \\ddots &  & & &\\cdots \\\\\n\\vdots & 0 & 0 & 0 &  1& -2& 1& 0 \\\\\n\\vdots & 0 & 0& 0& 0& 1& -2& 1 \\\\\n0 & 0 & 0 & 0  & 0 & 0 & 1 & -2 \\\\\n\\end{bmatrix}\n\\]\nNote that the matrix expects \\(\\boldsymbol{u} = (u^0, u^1, \\ldots, u^{N-1}, u^N)\\), even though \\(u^0=u^N\\). The last row in \\(\\tilde{D}^{(2)}\\) is thus irrelevant, because we wil set \\(u^0=u^N\\) manually."
  },
  {
    "objectID": "wave.html#implementation-periodic-boundaries",
    "href": "wave.html#implementation-periodic-boundaries",
    "title": "Finite difference methods for the wave equation",
    "section": "Implementation periodic boundaries",
    "text": "Implementation periodic boundaries\n\nInitialize \\(u^0\\) and compute \\(u^1 = u^0+\\frac{\\underline{c}^2}{2}\\tilde{D}^{(2)} u^0\\). Set \\(u^{nm1}=u^0, u^n=u^1\\)\nfor n in range(1, \\(N_t-1\\)):\n\n\\(u^{np1} = 2u^n-u^{nm1} + \\underline{c}^2 \\tilde{D}^{(2)} u^n\\)\n\\(u^{np1}_N = u^{np1}_0\\)\nUpdate to next iteration: \\(u^{nm1} = u^n; u^n = u^{np1}\\)"
  },
  {
    "objectID": "wave.html#periodic-wave",
    "href": "wave.html#periodic-wave",
    "title": "Finite difference methods for the wave equation",
    "section": "Periodic wave",
    "text": "Periodic wave"
  },
  {
    "objectID": "wave.html#properties-of-the-wave-equation",
    "href": "wave.html#properties-of-the-wave-equation",
    "title": "Finite difference methods for the wave equation",
    "section": "Properties of the wave equation",
    "text": "Properties of the wave equation\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2}\n\\]\nIf the initial condition is \\(u(x, 0)=I(x)\\) and \\(\\frac{\\partial u}{\\partial t}(x, 0)=0\\), then the solution at \\(t&gt;0\\) is\n\\[\nu(x, t) = \\frac{1}{2}\\left( I(x-ct) + I(x+ct)\\right)\n\\]\nThese are two waves - one traveling to the left and the other traveling to the right\n\nIf the initial condition \\(I(x)=e^{i k x}\\), then\n\\[\nu(x, t) = \\frac{1}{2} \\left(e^{ik(x-ct)} + e^{ik(x+ct)} \\right)\n\\]\nis a solution"
  },
  {
    "objectID": "wave.html#representation-of-waves-as-complex-exponentials",
    "href": "wave.html#representation-of-waves-as-complex-exponentials",
    "title": "Finite difference methods for the wave equation",
    "section": "Representation of waves as complex exponentials",
    "text": "Representation of waves as complex exponentials\nIf the initial condition is a sum of waves (superposition, each wave is a solution of the wave equation)\n\\[\nI(x) = \\sum_{k=0}^K a_k e^{i k x} = \\sum_{k=0}^K a_k \\left(\\cos kx + i \\sin kx\\right)\n\\]\nfor some \\(K\\), then the solution is\n\\[\nu(x, t) = \\frac{1}{2}\\sum_{k=0}^K a_k \\left(e^{ik(x-ct)} + e^{ik(x+ct)}\\right)\n\\]\nWe will analyze one component \\(e^{ik(x+ct)} = e^{ikx+ \\omega t}\\), where \\(\\omega = kc\\) is the frequency in time. This is very similar to the investigation we did for the numerical frequency for the vibration equation."
  },
  {
    "objectID": "wave.html#assume-that-the-numerical-solution-is-a-complex-wave",
    "href": "wave.html#assume-that-the-numerical-solution-is-a-complex-wave",
    "title": "Finite difference methods for the wave equation",
    "section": "Assume that the numerical solution is a complex wave",
    "text": "Assume that the numerical solution is a complex wave\n\\[\nu(x_j, t_n) = u^{n}_j = e^{ik(x_j+\\tilde{\\omega} t_n)}\n\\]\n\nHow accurate is \\(\\tilde{\\omega}\\) compared to the exact \\(\\omega=kc\\)?\nWhat can be concluded about stability?\n\n\nNote that the solution is a recurrence relation\n\\[\nu^n_j = e^{ikx_j} e^{i\\tilde{\\omega} n \\Delta t} = (e^{i\\tilde{\\omega} \\Delta t})^n e^{ikx_j}   \n\\]\nwith an amplification factor \\(A = e^{i\\tilde{\\omega} \\Delta t}\\) such that\n\\[\nu^n_j = A^n e^{ikx_j}\n\\]"
  },
  {
    "objectID": "wave.html#numerical-dispersion-relation",
    "href": "wave.html#numerical-dispersion-relation",
    "title": "Finite difference methods for the wave equation",
    "section": "Numerical dispersion relation",
    "text": "Numerical dispersion relation\nWe can find \\(\\tilde{\\omega}\\) by inserting for \\(e^{ik(x_j+\\tilde{\\omega}t_n)}\\) in the discretized wave equation\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\n\nThis is a lot of work, just like it was for the vibration equation. In the end we should get\n\\[\n\\tilde{\\omega} = \\frac{2}{\\Delta t} \\sin^{-1}\\left(C \\sin \\left(\\frac{k \\Delta x}{2}\\right)\\right)\n\\]\nwhere the CFL number is \\(C = \\frac{c \\Delta t}{\\Delta x}\\)\n\n\\(\\tilde{\\omega}(k, c, \\Delta x, \\Delta t)\\) is the numerical dispersion relation\n\\(\\omega = kc\\) is the exact dispersion relation\nWe can compare the two to investigate numerical accuracy and stability"
  },
  {
    "objectID": "wave.html#stability",
    "href": "wave.html#stability",
    "title": "Finite difference methods for the wave equation",
    "section": "Stability",
    "text": "Stability\nA simpler approach is to insert for \\(u^n_j = A^n e^{ikx_j}\\) directly in\n\\[\n\\frac{u^{n+1}_j - 2u^n_j + u^{n-1}_j}{\\Delta t^2} = c^2 \\frac{u^n_{j+1}-2 u^{n}_j + u^n_{j-1}}{\\Delta x^2}\n\\]\nand solve for \\(A\\). We get\n\\[\n\\frac{\\left(A^{n+1} - 2A^n + A^{n-1} \\right) e^{ikx_j}}{\\Delta t^2} = c^2 A^n \\frac{e^{ik(x_j+\\Delta x)} - 2e^{ikx_j} + e^{ik(x_j-\\Delta x)}}{\\Delta x^2}\n\\]\n\nDivide by \\(A^ne^{ikx_j}\\), multiply by \\(\\Delta t^2\\) and use \\(C=c\\Delta t/\\Delta x\\) to get\n\\[\nA -2 + A^{-1} = C^2 (e^{ik \\Delta x} - 2 + e^{-ik\\Delta x})\n\\]\ncontinue on next slide"
  },
  {
    "objectID": "wave.html#stability-1",
    "href": "wave.html#stability-1",
    "title": "Finite difference methods for the wave equation",
    "section": "Stability",
    "text": "Stability\n\\[\nA + A^{-1} = 2 + C^2 (e^{ik \\Delta x} - 2 + e^{-ik\\Delta x})\n\\]\nUse \\(e^{ix}+e^{-ix}=2\\cos x\\) to obtain\n\\[\nA + A^{-1} = 2 + 2 C^2(\\cos k\\Delta x - 1)\n\\]\n\nThis is a quadratic equation to solve for A. Using \\(\\beta=2(1+C^2 (\\cos(k\\Delta x)-1))\\) we get that\n\\[\nA = \\frac{\\beta \\pm \\sqrt{\\beta^2-4}}{2}\n\\]\nWe see that \\(|A| = 1\\) for any real numbers \\(-2 \\le \\beta \\le 2\\).\n\n\n\n\n\n\nFor all real numbers \\(-2 \\le \\beta \\le 2\\)\n\n\n\\[\n|\\beta \\pm \\sqrt{\\beta^2-4}| = 2\n\\] since \\(|\\beta \\pm \\sqrt{\\beta^2-4}| = |\\beta + i \\sqrt{4-\\beta^2}| = \\sqrt{\\beta^2 + 4 - \\beta^2} = 2\\)"
  },
  {
    "objectID": "wave.html#for-a-le-1-and-stability-we-need--2-le-beta-le-2-and-thus",
    "href": "wave.html#for-a-le-1-and-stability-we-need--2-le-beta-le-2-and-thus",
    "title": "Finite difference methods for the wave equation",
    "section": "For \\(|A| \\le 1\\) and stability we need \\(-2 \\le \\beta \\le 2\\) and thus",
    "text": "For \\(|A| \\le 1\\) and stability we need \\(-2 \\le \\beta \\le 2\\) and thus\n\\[\n-2 \\le 2(1+C^2(\\cos(k\\Delta x)-1)) \\le 2\n\\]\nRearrange to get that\n\\[\n-2 \\le C^2 (\\cos (k\\Delta x)-1) \\le 0\n\\]\nSince \\(\\cos(k\\Delta x)\\) can at worst be \\(-1\\) we get that the positive real CFL number must be smaller than 1\n\\[\nC \\le 1\n\\]\n\nHence (since \\(C=c\\Delta t/\\Delta x\\)) for stability we require that\n\\[\n\\Delta t \\le  \\frac{\\Delta x}{c}\n\\]"
  },
  {
    "objectID": "wave.html#test-dirichlet-solver-using-cfl1.01-vs-cfl1.0",
    "href": "wave.html#test-dirichlet-solver-using-cfl1.01-vs-cfl1.0",
    "title": "Finite difference methods for the wave equation",
    "section": "Test Dirichlet solver using CFL=1.01 vs CFL=1.0",
    "text": "Test Dirichlet solver using CFL=1.01 vs CFL=1.0\n\n\n\n\n\n\n\n\nunm1[:] = sp.lambdify(x, u0.subs(t, 0))(xj)\nun[:] = sp.lambdify(x, u0.subs(t, dt))(xj) \nplotdata = {0: unm1.copy()}\nCFL = 1.01\nfor n in range(Nt):\n  unp1[:] = 2*un - unm1 + CFL**2 * D2 @ un \n  unp1[0] = 0\n  unp1[-1] = 0\n  unm1[:] = un \n  un[:] = unp1\n  if n % 10 == 0:\n    plotdata[n] = unp1.copy()"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "",
    "text": "2011-2015 Editor-In-Chief SIAM J of Scientific Computing\nAuthor of 13 published books on scientific computing\nProfessor of Mechanics, University of Oslo 1998\nDeveloped INF5620 (which became IN5270 and now MAT-MEK4270)\nMemorial page"
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example-programming-topics",
    "href": "intro.html#what-to-learn-in-the-start-up-example-programming-topics",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example; programming topics",
    "text": "What to learn in the start-up example; programming topics\n\nHow to verify an implementation and automate verification through tests in Python\nHow to structure code in terms of functions, classes, and modules\nHow to work with Python concepts such as arrays, lists, dictionaries, lambda functions, functions in functions (closures), unit tests, command-line interfaces\nHow to perform array computing and understand the difference from scalar computing. Vectorization."
  },
  {
    "objectID": "intro.html#what-to-learn-in-the-start-up-example-mathematical-analysis",
    "href": "intro.html#what-to-learn-in-the-start-up-example-mathematical-analysis",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "What to learn in the start-up example; mathematical analysis",
    "text": "What to learn in the start-up example; mathematical analysis\n\nHow to uncover numerical artifacts in the computed solution\nHow to analyze the numerical schemes mathematically to understand why artifacts occur\nHow to derive mathematical expressions for various measures of the error in numerical methods, frequently by using the sympy software for symbolic computation\nIntroduce concepts such as finite difference operators, mesh (grid), mesh functions, stability, truncation error, consistency, and convergence"
  },
  {
    "objectID": "intro.html#topics-in-the-first-intro-to-the-finite-difference-method",
    "href": "intro.html#topics-in-the-first-intro-to-the-finite-difference-method",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Topics in the first intro to the finite difference method",
    "text": "Topics in the first intro to the finite difference method\n\nHow to think about finite difference discretization\nKey concepts:\n\nmesh\nmesh function\nfinite difference approximations\n\nThe Forward Euler, Backward Euler, and Crank-Nicolson methods\nFinite difference operator notation\nHow to derive an algorithm and implement it in Python\nHow to test the implementation"
  },
  {
    "objectID": "intro.html#python---pros-and-cons",
    "href": "intro.html#python---pros-and-cons",
    "title": "Algorithms and implementations for exponential decay models",
    "section": "Python - pros and cons",
    "text": "Python - pros and cons"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Presentations for MATMEK-4270",
    "section": "",
    "text": "Lecture 1 - Algorithms and implementations for exponential decay models\nLecture 2 - Analysis of exponential decay models\nLecture 3 - A simple vibration problem\nLecture 4 - The finite difference method\nLecture 5 - Finite difference methods for the wave equation\nLecture 7 - Postprocessing and interpolation\nLecture 8 - Function approximation with global functions\nLecture 9 - Function approximation with Chebyshev polynomials and in 2 dimensions\nLecture 10 - Function approximation by the finite element method\nLecture 11 - Solving PDEs with the method of weighted residuals"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points-1",
    "href": "wave.html#the-finite-difference-stencil-makes-use-of-5-neighboring-points-1",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil makes use of 5 neighboring points",
    "text": "The finite difference stencil makes use of 5 neighboring points\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-is-not-used-at-the-boundary",
    "href": "wave.html#the-finite-difference-stencil-is-not-used-at-the-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil is not used at the boundary",
    "text": "The finite difference stencil is not used at the boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#the-finite-difference-stencil-is-not-used-at-the-spatial-boundary",
    "href": "wave.html#the-finite-difference-stencil-is-not-used-at-the-spatial-boundary",
    "title": "Finite difference methods for the wave equation",
    "section": "The finite difference stencil is not used at the spatial boundary",
    "text": "The finite difference stencil is not used at the spatial boundary\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\frac{u^{n+1}_N - 2u^n_N + u^{n-1}_N}{\\Delta t^2} = c^2 \\frac{\\color{red}u^n_{N+1} \\color{black} -2 u^{n}_N + u^n_{N-1}}{\\Delta x^2}\n\\]\n\n\nUsed at the boundary the regular stencil will contain a ghost node\nBut at the boundary we use boundary conditions and do not solve the PDE!"
  },
  {
    "objectID": "wave.html#ny",
    "href": "wave.html#ny",
    "title": "Finite difference methods for the wave equation",
    "section": "Ny",
    "text": "Ny\n\nThe PDE at the left hand side \\(j=0\\) using ghost cell (same for \\(j=N\\)):\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(u^n_{1}-2u^n_0+u^n_{-1})\n\\]\nInsert for \\(u^{n}_{-1}=u^n_{1}\\) and obtain\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(2u^n_{1}-2u^n_0)\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit. Can be implemented by modifying \\(D^{(2)}\\)!"
  },
  {
    "objectID": "wave.html#use-the-ghost-cell-and-the-pde-to-fix-the-neumann-condition",
    "href": "wave.html#use-the-ghost-cell-and-the-pde-to-fix-the-neumann-condition",
    "title": "Finite difference methods for the wave equation",
    "section": "Use the ghost cell and the PDE to fix the Neumann condition",
    "text": "Use the ghost cell and the PDE to fix the Neumann condition\nThe PDE at the left hand side \\(j=0\\) using ghost cell:\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(u^n_{1}-2u^n_0+u^n_{-1})\n\\]\nInsert for \\(u^{n}_{-1}=u^n_{1}\\) and obtain\n\\[\nu^{n+1}_0 = 2u^{n}_0 - u^{n-1}_0 + \\underline{c}^2(2u^n_{1}-2u^n_0)\n\\]\n\n\n\n\n\n\nNote\n\n\nSecond order accurate and explicit. Can be implemented by modifying \\(D^{(2)}\\)!"
  },
  {
    "objectID": "postprocessing.html#postprocessing",
    "href": "postprocessing.html#postprocessing",
    "title": "Postprocessing and Interpolation",
    "section": "Postprocessing",
    "text": "Postprocessing\nWe have computed the solution. Now what?\n\n\nWe analyze the solution!\nWe process the solution into presentable tables and figures\nWe extract important numbers that the solution allows us to compute. For example lift or drag on an airplane wing, or the flux through a boundary.\n\n\n\nIn order to achieve this we need\n\n\nInterpolation - to get the solution everywhere and not just in nodes\nIntegrals - Averages arise from integrals over domains and boundaries\nDerivatives - For example, drag is the integral of the gradient over a boundary \\(\\int_{\\Gamma}\\nu \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\\).\nLagrange interpolation polynomials will help us get there!"
  },
  {
    "objectID": "postprocessing.html#one-dimensional-interpolation",
    "href": "postprocessing.html#one-dimensional-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "One-dimensional interpolation",
    "text": "One-dimensional interpolation\n\\[\nu(x) = x(1-x)\n\\]\n\n\n\n\n\n\n\n\n\n\nWe have the mesh function \\((u(x_i))_{i=0}^{5}\\). What is \\(u(0.75)\\)?\nLinear interpolation requires interpolating between \\(u(x_3)\\) and \\(u(x_4)\\)\n\\[\n\\overline{u}(x) = u_3 + (u_4-u_3)\\frac{x-x_3}{x_4-x_3}.\n\\]"
  },
  {
    "objectID": "postprocessing.html#section",
    "href": "postprocessing.html#section",
    "title": "Postprocessing and Interpolation",
    "section": "",
    "text": "N = 20\nxij, yij = mesh2D(N, N, 1, 1, False)\nu2 = np.cos(xij)*(1-xij)*np.sin(yij)*(1-yij)\nue = sp.cos(x)*(1-x)*sp.sin(y)*(1-y)\n\ndef I(u, dx, dy):\n  um = (u[:-1, :-1]+u[1:, :-1]+u[:-1, 1:]+u[1:, 1:])/4\n  return np.sqrt(np.sum(um**2*dx*dy))\n\nI(u2, 1/N, 1/N)\n\n0.09552823074589575\n\n\nCompared to the exact\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue**2, (y, 0, 1)), (x, 0, 1))))\n\n0.09586332023451888"
  },
  {
    "objectID": "postprocessing.html#linear-interpolation-using-sympy",
    "href": "postprocessing.html#linear-interpolation-using-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Linear interpolation using Sympy",
    "text": "Linear interpolation using Sympy\n\nx = sp.Symbol('x')\nuo = u[3] + (u[4]-u[3])/(xj[4]-xj[3])*(x-xj[3])\n\n\n\n\n\n\n\n\n\n\nLinear interpolation is not very accurate. How do we increase accuracy?"
  },
  {
    "objectID": "postprocessing.html#lagrange-interpolation",
    "href": "postprocessing.html#lagrange-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange interpolation",
    "text": "Lagrange interpolation\nLagrange interpolation is a generic approach that uses any number of points from the mesh function and defines a Lagrange interpolation polynomial\n\\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\nwhere \\(\\{u^j\\}_{j=0}^k\\) are the mesh function values at the chosen \\(k+1\\) mesh points. For the linear example we have just seen\n\\[\n(u^0, u^1) = (u_3, u_4)\n\\]\n\n\n\n\n\n\nNote\n\n\nLagrange mesh function values are given a superscript that starts counting from 0."
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions",
    "href": "postprocessing.html#lagrange-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions",
    "text": "Lagrange basis functions\nThe Lagrange basis functions are defined as\n\\[\n    \\ell_j(x) = \\frac{x-x^0}{x^j-x^0} \\cdots \\frac{x-x^{j-1}}{x^j-x^{j-1}}\\frac{x-x^{j+1}}{x^j-x^{j+1}} \\cdots \\frac{x-x^{k}}{x^j-x^{k}}\n\\]"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions-and-nodes",
    "href": "postprocessing.html#lagrange-basis-functions-and-nodes",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions and nodes",
    "text": "Lagrange basis functions and nodes\nThe Lagrange interpolation polynomial \\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\ncontains \\(k+1\\) basis functions \\(\\{\\ell_j(x)\\}_{j=0}^k\\).\n\nThe basis functions are defined using any number of chosen interpolation nodes \\((x^0, \\ldots, x^k)\\). For our linear example the nodes are simply\n\\[\n(x^0, x^1) = (x_3, x_4)\n\\]\n\n\n\n\n\n\nNote\n\n\nLagrange nodes are given a superscript that starts counting from 0. Choosing different nodes leads to different results!"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-functions-are-defined-as",
    "href": "postprocessing.html#lagrange-basis-functions-are-defined-as",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis functions are defined as",
    "text": "Lagrange basis functions are defined as\n\\[\n    \\ell_j(x) = \\frac{x-x^0}{x^j-x^0} \\cdots \\frac{x-x^{j-1}}{x^j-x^{j-1}}\\frac{x-x^{j+1}}{x^j-x^{j+1}} \\cdots \\frac{x-x^{k}}{x^j-x^{k}}\n\\]\nLet that one sink in.\n\n\nNote index \\(j\\) in \\(\\ell_j(x)\\), the \\(j\\)’th basis function.\nNumerator contains the product of all differences \\((x-x^m)\\) for all \\(m\\) except \\(m=j\\)\nDenominator contains the product of all differences \\((x^j-x^m)\\) for all \\(m\\) except \\(m=j\\)\n\n\n\nWe can write\n\\[\n\\ell_j(x) = \\prod_{\\substack{0 \\le m \\le k \\\\ m \\ne j}} \\frac{x-x^m}{x^j-x^m}.\n\\]"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-function-in-sympy",
    "href": "postprocessing.html#lagrange-basis-function-in-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis function in Sympy",
    "text": "Lagrange basis function in Sympy\n\ndef Lagrangebasis(xj, x=x):\n    \"\"\"Construct Lagrange basis function for points in xj\n\n    Parameters\n    ----------\n    xj : array\n        Interpolation points\n    x : Sympy Symbol\n\n    Returns\n    -------\n    Lagrange basis functions as Sympy function \n    \"\"\"\n    from sympy import Mul\n    n = len(xj)\n    ell = []\n    numert = Mul(*[x - xj[i] for i in range(n)])\n\n    for i in range(n):\n        numer = numert/(x - xj[i])\n        denom = Mul(*[(xj[i] - xj[j]) for j in range(n) if i != j])\n        ell.append(numer/denom)\n    return ell"
  },
  {
    "objectID": "postprocessing.html#lagrange-basis-in-sympy",
    "href": "postprocessing.html#lagrange-basis-in-sympy",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange basis in Sympy",
    "text": "Lagrange basis in Sympy\nA basis is a collection of basis functions\n\\[\n\\{\\ell_j(x)\\}_{j=0}^k\n\\]\n\ndef Lagrangebasis(xj, x=x):\n  \"\"\"Construct Lagrange basis for points in xj\n\n  Parameters\n  ----------\n  xj : array\n    Interpolation points (nodes)\n  x : Sympy Symbol\n\n  Returns\n  -------\n  Lagrange basis as a list of Sympy functions \n  \"\"\"\n  from sympy import Mul\n  n = len(xj)\n  ell = []\n  numert = Mul(*[x - xj[i] for i in range(n)])\n  for i in range(n):\n    numer = numert/(x - xj[i])\n    denom = Mul(*[(xj[i] - xj[j]) for j in range(n) if i != j])\n    ell.append(numer/denom)\n  return ell"
  },
  {
    "objectID": "postprocessing.html#the-basis-for-linear-interpolation-of-our-first-example-is",
    "href": "postprocessing.html#the-basis-for-linear-interpolation-of-our-first-example-is",
    "title": "Postprocessing and Interpolation",
    "section": "The basis for linear interpolation of our first example is",
    "text": "The basis for linear interpolation of our first example is\n\nell = Lagrangebasis(xj[3:5], x=x)\nell\n\n[4.0 - 5.0*x, 5.0*x - 3.0]\n\n\nWe can plot these two linear functions between \\(x_3\\) and \\(x_4\\)\n\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nxf = np.linspace(xj[3], xj[4], 10)\nplt.plot(xf, sp.lambdify(x, ell[0])(xf), 'g--')\nplt.plot(xf, sp.lambdify(x, ell[1])(xf), 'r--')"
  },
  {
    "objectID": "postprocessing.html#lagrange-interpolation-polynomial",
    "href": "postprocessing.html#lagrange-interpolation-polynomial",
    "title": "Postprocessing and Interpolation",
    "section": "Lagrange interpolation polynomial",
    "text": "Lagrange interpolation polynomial\nWith the basis we can now compute the Lagrange interpolation polynomial\n\\[\nL(x) = \\sum_{j=0}^k u^j \\ell_j(x)\n\\]\n\n\n\ndef Lagrangefunction(u, basis):\n  \"\"\"Return Lagrange polynomial\n\n  Parameters\n  ----------\n  u : array\n    Mesh function values\n  basis : tuple of Lagrange basis functions\n    Output from Lagrangebasis\n  \"\"\"\n  f = 0\n  for j, uj in enumerate(u):\n    f += basis[j]*uj\n  return f\n\n\n\nL = Lagrangefunction(u[3:5], ell)\nxf = np.linspace(xj[3]-0.1, xj[4]+0.1, 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#the-great-thing-about-lagrange-polynomials-is-that-we-can-choose-the-number-of-basis-functions",
    "href": "postprocessing.html#the-great-thing-about-lagrange-polynomials-is-that-we-can-choose-the-number-of-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "The great thing about Lagrange polynomials is that we can choose the number of basis functions",
    "text": "The great thing about Lagrange polynomials is that we can choose the number of basis functions\n\n\nChoose \\((x^0, x^1, x^2) = (x_2, x_3, x_4)\\)\n\nell3 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell3)\nxf = np.linspace(xj[2], xj[4], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')\n\n\n\n\n\n\n\n\n\nChoose \\((x^0, x^1, x^2) = (x_3, x_4, x_5)\\)\n\nell3 = Lagrangebasis(xj[3:6], x=x) \nL = Lagrangefunction(u[3:6], ell3)\nxf = np.linspace(xj[3], xj[5], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#we-can-choose-any-and-any-number-of-basis-functions",
    "href": "postprocessing.html#we-can-choose-any-and-any-number-of-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "We can choose any, and any number of basis functions",
    "text": "We can choose any, and any number of basis functions\n\n\nChoose \\((x^0, x^1, x^2) = (x_2, x_3, x_4)\\)\n\nell2 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell2)\nxf = np.linspace(xj[2], xj[4], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')\n\n\n\n\n\n\n\n\n\nChoose \\((x^0, x^1, x^2) = (x_3, x_4, x_5)\\)\n\nell3 = Lagrangebasis(xj[3:6], x=x) \nL = Lagrangefunction(u[3:6], ell3)\nxf = np.linspace(xj[3], xj[5], 10)\nplt.figure(figsize=(6, 3))\nplt.plot(xj, u, 'bo')\nplt.plot(xf, sp.lambdify(x, L)(xf), 'g') \nplt.plot(0.75, L.subs(x, 0.75), 'ko')"
  },
  {
    "objectID": "postprocessing.html#with-three-nodes-the-lagrange-basis-functions-are-second-order-polynomials",
    "href": "postprocessing.html#with-three-nodes-the-lagrange-basis-functions-are-second-order-polynomials",
    "title": "Postprocessing and Interpolation",
    "section": "With three nodes the Lagrange basis functions are second order polynomials",
    "text": "With three nodes the Lagrange basis functions are second order polynomials\n\n\n\nplt.figure(figsize=(6, 4)) \nell2 = Lagrangebasis(xj[2:5], x=x) \nplt.plot(xj, u, 'bo-')\nxl = np.linspace(xj[2], xj[4], 100)\nplt.plot(xl, sp.lambdify(x, ell2[0])(xl), 'r:')\nplt.plot(xl, sp.lambdify(x, ell2[1])(xl), 'g:')\nplt.plot(xl, sp.lambdify(x, ell2[2])(xl), 'b:')\nplt.legend([r'$u(x_i)$', r'$\\ell_0(x)$', r'$\\ell_1(x)$', r'$\\ell_2(x)$'], loc='upper left'); \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll Lagrange basis functions are such that on the chosen mesh points \\(\\{x^{i}\\}_{i=0}^{k}\\) we have\n\\[\n\\ell_j(x^i) = \\delta_{ij} = \\begin{cases} 1 \\text{ for } i=j \\\\\n0 \\text{ for }  i\\ne j\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe Lagrange basis functions do not depend on the mesh function values, only on the mesh points. The mesh points do not need to be uniform, any mesh will do as long as all points are different."
  },
  {
    "objectID": "postprocessing.html#interpolation-of-ux",
    "href": "postprocessing.html#interpolation-of-ux",
    "title": "Postprocessing and Interpolation",
    "section": "Interpolation of \\(u'(x)\\)",
    "text": "Interpolation of \\(u'(x)\\)\nThe solution has been obtained in mesh points \\(\\{u(x_j)\\}_{j=0}^N\\). How can we compute \\(u'(x)\\) for any given \\(x\\)?\n\nCan we use the derivative matrix \\(D^{(1)}\\) and thus\n\\[\nu' \\approx f = D^{(1)} u\n\\]\nsuch that the mesh function \\(f=\\{u'(x_j)\\}_{j=0}^N\\)?\n\n\nYes, but you need to interpolate \\(f\\) just like the mesh function \\(\\{u(x_j)\\}_{j=0}^N\\)\nIs there a better way?\n\n\nYes! Just take the derivative of the Lagrange function\n\\[\nu' \\approx L'(x) = \\sum_{j=0}^k u^j \\ell'_j(x)\n\\]"
  },
  {
    "objectID": "postprocessing.html#interpolation-of-derivatives",
    "href": "postprocessing.html#interpolation-of-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "Interpolation of derivatives",
    "text": "Interpolation of derivatives\nThe solution has been obtained in mesh points \\(\\boldsymbol{u}=(u(x_j))_{j=0}^N\\). How can we compute \\(u'(x)\\) for any given \\(x\\)?\n\nCan we use the derivative matrix \\(D^{(1)}\\) and thus\n\\[\n\\boldsymbol{f} = D^{(1)} \\boldsymbol{u}\n\\]\nsuch that the mesh function \\(\\boldsymbol{f}=(u'(x_j))_{j=0}^N\\)?\n\n\nYes, but you then need to interpolate \\(\\boldsymbol{f}\\)!\nIs there a better way?\n\n\nYes! Just take the derivative of the Lagrange function\n\n\n\\[\nu' \\approx L'(x) = \\sum_{j=0}^k u^j \\ell'_j(x)\n\\]\n\n\nell2 = Lagrangebasis(xj[2:5], x=x) \nL = Lagrangefunction(u[2:5], ell2) \nf = L.diff(x, 1)\ndisplay(L)\ndisplay(f)\n\n\\(\\displaystyle 3.0 \\left(x - 0.8\\right) \\left(x - 0.6\\right) - 6.0 \\left(x - 0.8\\right) \\left(x - 0.4\\right) + 2.0 \\left(x - 0.6\\right) \\left(x - 0.4\\right)\\)\n\n\n\\(\\displaystyle 1.0 - 2.0 x\\)"
  },
  {
    "objectID": "postprocessing.html#two-dimensional-interpolation",
    "href": "postprocessing.html#two-dimensional-interpolation",
    "title": "Postprocessing and Interpolation",
    "section": "Two-dimensional interpolation",
    "text": "Two-dimensional interpolation\nA solution in 2D is the mesh function\n\\[\nu_{ij} = u(x_i, y_j), \\quad i=0, 1, \\ldots, N \\, \\text{and} \\, j=0, 1, \\ldots, N\n\\]\nHow do we use \\(\\boldsymbol{u} = (u_{ij})_{i,j=0}^N\\) to compute \\(u(x)\\) for any \\(x\\)?\n\n\n\n\n\n\n\n2D interpolation\n\n\nWe need to do interpolation in 2D!"
  },
  {
    "objectID": "postprocessing.html#if-the-problem-is-two-dimensional-then-we-have-the-solution-as-a-mesh-function",
    "href": "postprocessing.html#if-the-problem-is-two-dimensional-then-we-have-the-solution-as-a-mesh-function",
    "title": "Postprocessing and Interpolation",
    "section": "If the problem is two-dimensional, then we have the solution as a mesh function",
    "text": "If the problem is two-dimensional, then we have the solution as a mesh function\n\\[\nu_{ij} = u(x_i, y_j), \\quad i=0, 1, \\ldots, N \\, \\text{and} \\, j=0, 1, \\ldots, N\n\\]\nHow do we use \\(U = (u_{ij})_{i,j=0}^N\\) to compute \\(u(x, y)\\) for any point \\((x, y)\\) in the domain?\n\n\n\n\n\n\n\n2D interpolation\n\n\nWe need to do interpolation in 2D!"
  },
  {
    "objectID": "postprocessing.html#d-interpolation-1",
    "href": "postprocessing.html#d-interpolation-1",
    "title": "Postprocessing and Interpolation",
    "section": "2D-interpolation",
    "text": "2D-interpolation\nWe consider first a simple function\n\\[\nu(x, y) = x(1-x)y(1-y), \\quad x, y \\in \\Omega = [0, 1]^2\n\\]\nand we want to find \\(u(0.55, 0.65)\\) from the mesh function \\((u(x_i, y_j))_{i,j=0}^{10}\\).\n\ndef mesh2D(Nx, Ny, Lx, Ly, sparse=False):\n  x = np.linspace(0, Lx, Nx+1)\n  y = np.linspace(0, Ly, Ny+1)\n  return np.meshgrid(x, y, indexing='ij', sparse=sparse)\n\nN = 10\nxij, yij = mesh2D(N, N, 1, 1, False)\nU = xij*(1-xij)*yij*(1-yij)\nplt.figure(figsize=(3, 3))\nplt.contourf(xij, yij, U)\nplt.plot(0.55, 0.65, 'ro')"
  },
  {
    "objectID": "postprocessing.html#d",
    "href": "postprocessing.html#d",
    "title": "Postprocessing and Interpolation",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d",
    "href": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need to choose interpolation points in 2D",
    "text": "In 2D we need to choose interpolation points in 2D"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d-surrounding-the-point-of-interest",
    "href": "postprocessing.html#in-2d-we-need-to-choose-interpolation-points-in-2d-surrounding-the-point-of-interest",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need to choose interpolation points in 2D surrounding the point of interest",
    "text": "In 2D we need to choose interpolation points in 2D surrounding the point of interest"
  },
  {
    "objectID": "postprocessing.html#d-lagrange-interpolation-polynomials-makes-use-of-tensor-product-basis-functions",
    "href": "postprocessing.html#d-lagrange-interpolation-polynomials-makes-use-of-tensor-product-basis-functions",
    "title": "Postprocessing and Interpolation",
    "section": "2D Lagrange interpolation polynomials makes use of tensor product basis functions",
    "text": "2D Lagrange interpolation polynomials makes use of tensor product basis functions\n\\[\nL(x, y) = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\ell_{m}(x) \\ell_{n}(y),\n\\]\nHere one (tensor product) basis function is\n\\[\n\\psi_{mn}(x, y) = \\ell_{m}(x) \\ell_{n}(y)\n\\]\nand\n\\[\nu^{m,n}  \\quad m=0, \\ldots, k \\quad \\text{and} \\quad n=0, \\ldots, l\n\\]\nare the \\((k+1)(l+1)\\) mesh points used for the interpolation.\n\nTwo sets of mesh points: \\((x^0, \\ldots, x^k)\\) and \\((y^0, \\ldots, y^l)\\)\n\\(\\{\\ell_m(x)\\}_{m=0}^k\\) and \\(\\{\\ell_n(y)\\}_{n=0}^l\\) are computed exactly as in 1D."
  },
  {
    "objectID": "postprocessing.html#d-lagrange",
    "href": "postprocessing.html#d-lagrange",
    "title": "Postprocessing and Interpolation",
    "section": "2D Lagrange",
    "text": "2D Lagrange\n\nxij, yij = mesh2D(N, N, 1, 1, False) \ny = sp.Symbol('y')\nlx = Lagrangebasis(xij[5:7, 0], x=x)\nly = Lagrangebasis(yij[0, 6:8], x=y)\n\ndef Lagrangefunction2D(u, basisx, basisy):\n  N, M = u.shape\n  f = 0\n  for i in range(N):\n    for j in range(M):\n      f += basisx[i]*basisy[j]*u[i, j]\n  return f\n\nf = Lagrangefunction2D(U[5:7, 6:8], lx, ly)\nprint('The 2D Lagrange polynomial is:')\nsp.nsimplify(sp.simplify(f), tolerance=1e-8)\n\nThe 2D Lagrange polynomial is:\n\n\n\\(\\displaystyle \\frac{3 x y}{100} - \\frac{21 x}{500} - \\frac{9 y}{100} + \\frac{63}{500}\\)\n\n\nCompare with exact. Not perfect since only linear interpolation.\n\nue = x*(1-x)*y*(1-y)\nprint('Numerical          Exact')\nprint(f.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0551250000000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#d-interpolation-with-more-points",
    "href": "postprocessing.html#d-interpolation-with-more-points",
    "title": "Postprocessing and Interpolation",
    "section": "2D interpolation with more points",
    "text": "2D interpolation with more points\n\nlx = Lagrangebasis(xij[5:8, 0], x=x)\nly = Lagrangebasis(yij[0, 5:8], x=y)\nL2 = Lagrangefunction2D(u2[5:8, 5:8], lx, ly)\nprint('Numerical          Exact')\nprint(L2.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0563062500000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#improve-accuracy-using-more-interpolation-points",
    "href": "postprocessing.html#improve-accuracy-using-more-interpolation-points",
    "title": "Postprocessing and Interpolation",
    "section": "Improve accuracy using more interpolation points",
    "text": "Improve accuracy using more interpolation points\n\nlx = Lagrangebasis(xij[5:8, 0], x=x)\nly = Lagrangebasis(yij[0, 5:8], x=y)\nL2 = Lagrangefunction2D(U[5:8, 5:8], lx, ly)\nprint('Numerical          Exact')\nprint(L2.subs({x: 0.55, y: 0.65}), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n0.0563062500000000 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#in-2d-we-need-partial-derivatives",
    "href": "postprocessing.html#in-2d-we-need-partial-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "In 2D we need partial derivatives",
    "text": "In 2D we need partial derivatives\nUsing finite difference matrices is possible, but requires interpolation of outcome:\n\\[ \\small\n\\left(\\frac{\\partial u}{\\partial x}(x_i, y_j)\\right)_{i,j=0}^{N} = D^{(1)} U \\quad \\text{and} \\quad \\left(\\frac{\\partial u}{\\partial y}(x_i, y_j)\\right)_{i,j=0}^{N} =  U (D^{(1)})^T.\n\\]\n\nAs in 1D take the derivatives of the Lagrange polynomials:\n\\[ \\small\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial L(x, y)}{\\partial x} = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\frac{\\partial \\ell_{m}(x)}{\\partial x} \\ell_{n}(y)\n\\] \\[ \\small\n\\frac{\\partial u}{\\partial y} = \\frac{\\partial L(x, y)}{\\partial y} = \\sum_{m=0}^k\\sum_{n=0}^l u^{m,n} \\ell_{m}(x) \\frac{\\partial \\ell_{n}(y)}{\\partial y}\n\\]\nVerification:\n\ndLx = sp.diff(L2, x)\ndLy = sp.diff(L2, y)\nprint('Numerical          Exact')\nprint(dLx.subs({x: 0.55, y: 0.65}), sp.diff(ue, x).subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n-0.0227500000000000 -0.0227500000000000"
  },
  {
    "objectID": "postprocessing.html#other-tools-for-interpolation-are-available",
    "href": "postprocessing.html#other-tools-for-interpolation-are-available",
    "title": "Postprocessing and Interpolation",
    "section": "Other tools for interpolation are available",
    "text": "Other tools for interpolation are available\nScipy.interpolate\n\nfrom scipy.interpolate import interpn\nprint('Numerical          Exact')\nprint(interpn((xij[5:8, 0], yij[0, 5:8]), U[5:8, 5:8], np.array([0.55, 0.65])), ue.subs({x: 0.55, y: 0.65}))\n\nNumerical          Exact\n[0.055125] 0.0563062500000000\n\n\nDefaults to linear interpolation, so not very accurate.\n\nEasy to use more points and cubic interpolation:\n\nprint('Numerical          Exact') \nprint(interpn((xij[5:9, 0], yij[0, 5:9]), U[5:9, 5:9], np.array([0.55, 0.65]), method='cubic'), ue.subs({x: 0.55, y: 0.65})) \n\nNumerical          Exact\n[0.05630625] 0.0563062500000000"
  },
  {
    "objectID": "postprocessing.html#how-to-compute-errors-in-2d",
    "href": "postprocessing.html#how-to-compute-errors-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "How to compute errors in 2D",
    "text": "How to compute errors in 2D\nFor the numerical solution \\(u\\) and the exact solution \\(u^{e}\\) the \\(L^2\\) error norm can in general, for any domain \\(\\Omega\\) and number of dimensions, be defined as\n\\[\n\\|u-u^{e}\\|_{L^2(\\Omega)} = \\sqrt{\\int_{\\Omega} (u-u^{e})^2 d\\Omega}.\n\\]\nIn 2D \\(\\Omega = [0, L_x] \\times [0, L_y]\\) and the integral becomes\n\\[\n\\|u-u^{e}\\|_{L^2(\\Omega)} = \\sqrt{\\int_{0}^{L_x}\\int_0^{L_y}(u-u^{e})^2 dy dx}.\n\\]\n\nNumerical integration can be performed using, e.g., the midpoint rule\n\\[\nI(u)  = \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} u((i+0.5)\\Delta x, (j+0.5) \\Delta y) \\Delta x \\Delta y \\approx \\int_{0}^{L_x}\\int_0^{L_y}u(x,y) dy dx.\n\\]"
  },
  {
    "objectID": "postprocessing.html#midpoint-integration",
    "href": "postprocessing.html#midpoint-integration",
    "title": "Postprocessing and Interpolation",
    "section": "Midpoint integration",
    "text": "Midpoint integration\n\n\n\n\n\n\n\n\n\nAll values \\(u(x, y)\\) are required in the center of computational cells and not in nodes.\n\\[\n\\| u-u^{e} \\|_{L^2} = \\sqrt{I((u - u^{e})^2)}\n\\]"
  },
  {
    "objectID": "postprocessing.html#numerical-midpoint-integration-tested",
    "href": "postprocessing.html#numerical-midpoint-integration-tested",
    "title": "Postprocessing and Interpolation",
    "section": "Numerical midpoint integration tested",
    "text": "Numerical midpoint integration tested\n\nN = 20\nxij, yij = mesh2D(N, N, 1, 1, False)\nU = np.cos(xij)*(1-xij)*np.sin(yij)*(1-yij)\nue = sp.cos(x)*(1-x)*sp.sin(y)*(1-y)\n\ndef I(u, dx, dy):\n  um = (u[:-1, :-1]+u[1:, :-1]+u[:-1, 1:]+u[1:, 1:])/4\n  return np.sqrt(np.sum(um*dx*dy))\n\nI(U, 1/N, 1/N)\n\n0.2696557024695919\n\n\nCompared to the exact integral\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue, (y, 0, 1)), (x, 0, 1))).n())\n\n0.26995448271292816\n\n\n\nThe error norm \\(\\|u-u^{e}\\|_{L^2} = \\sqrt{I((u-u^{e})^2)}\\) is\n\nnp.sqrt(I((U-sp.lambdify((x, y), ue)(xij, yij))**2, 1/N, 1/N))\n\n3.1759481967064892e-09\n\n\nwhich is very small because the Lagrange interpolator happens to be close to exact exact for all the midpoints for this function. It is not zero in general."
  },
  {
    "objectID": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-using-the-small-ell2-norm-instead",
    "href": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-using-the-small-ell2-norm-instead",
    "title": "Postprocessing and Interpolation",
    "section": "Just like for the 1D case there is a simplification using the small \\(\\ell^2\\) norm instead",
    "text": "Just like for the 1D case there is a simplification using the small \\(\\ell^2\\) norm instead\n\\[\n\\|(u-u_e)\\|_{\\ell^2} = \\sqrt{ \\Delta x \\Delta y \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} (u_{ij}-u^{e}_{ij})^2 }.\n\\]\nThis is an approximation of the numerical integration using the trapezoidal method\nImplementation:\n\ndef l2_error(u, dx, dy):\n    return np.sqrt(dx*dy*np.sum(u**2))\nl2_error(u2-sp.lambdify((x, y), ue)(xij, yij), 1/N, 1/N)\n\n1.00866469481632e-17"
  },
  {
    "objectID": "postprocessing.html#numerical-integration-can-also-for-example-use-the-trapezoidal-or-simpson-methods",
    "href": "postprocessing.html#numerical-integration-can-also-for-example-use-the-trapezoidal-or-simpson-methods",
    "title": "Postprocessing and Interpolation",
    "section": "Numerical integration can also, for example, use the trapezoidal or Simpson methods",
    "text": "Numerical integration can also, for example, use the trapezoidal or Simpson methods\nTrapezoidal:\n\ndef I_trapz(u, dx, dy):\n  return np.sqrt(np.trapz(np.trapz(u**2, dx=dy, axis=1), dx=dx))\nI_trapz(U, 1/N, 1/N)\n\n0.09592899344305951\n\n\nSimpson’s rule:\n\ndef I_simps(u, dx, dy):\n  from scipy.integrate import simpson as simp\n  return np.sqrt(simp(simp(u**2, dx=dy, axis=1), dx=dx))\nI_simps(U, 1/N, 1/N)\n\n0.09586418448463656\n\n\nExact:\n\nfloat(sp.sqrt(sp.integrate(sp.integrate(ue**2, (y, 0, 1)), (x, 0, 1))).n())\n\n0.09586332023451888"
  },
  {
    "objectID": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-for-l2-using-the-small-ell2-norm-instead",
    "href": "postprocessing.html#just-like-for-the-1d-case-there-is-a-simplification-for-l2-using-the-small-ell2-norm-instead",
    "title": "Postprocessing and Interpolation",
    "section": "Just like for the 1D case there is a simplification for \\(L^2\\) using the small \\(\\ell^2\\) norm instead",
    "text": "Just like for the 1D case there is a simplification for \\(L^2\\) using the small \\(\\ell^2\\) norm instead\n\\[\n\\|(u-u_e)\\|_{\\ell^2} = \\sqrt{ \\Delta x \\Delta y \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} (u_{ij}-u^{e}_{ij})^2 }.\n\\]\nThis is an approximation of the numerical integration using the trapezoidal method\nImplementation:\n\ndef l2_error(u, dx, dy):\n  return np.sqrt(dx*dy*np.sum(u**2))\nl2_error(U-sp.lambdify((x, y), ue)(xij, yij), 1/N, 1/N)\n\n1.00866469481632e-17"
  },
  {
    "objectID": "postprocessing.html#integration-over-boundary",
    "href": "postprocessing.html#integration-over-boundary",
    "title": "Postprocessing and Interpolation",
    "section": "Integration over boundary",
    "text": "Integration over boundary\nIn many real problems we are not interested in the solution at a point, but rather in average values, or values integrated over a boundary."
  },
  {
    "objectID": "postprocessing.html#our-computational-domain-is-simpler-and-boundary-integrals-are-simpler-as-well",
    "href": "postprocessing.html#our-computational-domain-is-simpler-and-boundary-integrals-are-simpler-as-well",
    "title": "Postprocessing and Interpolation",
    "section": "Our computational domain is simpler and boundary integrals are simpler as well",
    "text": "Our computational domain is simpler and boundary integrals are simpler as well\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrate over boundary at \\(x=L_x\\)\n\\[\n\\int_{0}^{L_y} u(x=L_x, y) dy\n\\]\n\nnp.trapz(U[-1], dx=1/N, axis=0) \n\n0.0\n\n\nIntegrate over boundary at \\(y=L_y\\)\n\\[\n\\int_{0}^{L_x} u(x, y=L_y) dx\n\\]\n\nnp.trapz(U[:, -1], dx=1/N, axis=0) \n\n0.0"
  },
  {
    "objectID": "postprocessing.html#friction-requires-derivatives",
    "href": "postprocessing.html#friction-requires-derivatives",
    "title": "Postprocessing and Interpolation",
    "section": "Friction requires derivatives",
    "text": "Friction requires derivatives\n\n\n\n\n\n\n\n\n\n\n\nGradient in normal direction\n\\[\n\\int_{\\Gamma} \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\n\\]\n\nIntegrate gradient at \\(y=0\\)\n\\[\n\\int_{0}^{L_x} \\frac{\\partial u}{\\partial y}(x, y=0) dx\n\\]\n\ndudy = (N/2)*(-3*U[:, 0] + 4*U[:, 1] - U[:, 2])\nnp.trapz(dudy, dx=1/N)\n\n0.4601188642377852\n\n\n\ndudye = sp.diff(ue, y)\nfloat(sp.integrate(dudye.subs(y, 0), (x, 0, 1)).n())\n\n0.4596976941318603\n\n\nIntegrate over boundary at \\(x=L_x\\)\n\\[\n\\int_{0}^{L_y} \\frac{\\partial u}{\\partial x}(x=L_x, y) dy\n\\]\n\ndudx = (N/2)*(3*U[-1] - 4*U[-2] + U[-3])\nnp.trapz(dudx, dx=1/N) \n\n-0.08567622273572727"
  },
  {
    "objectID": "postprocessing.html#friction-requires-derivatives-use-forward-or-backward-stencils",
    "href": "postprocessing.html#friction-requires-derivatives-use-forward-or-backward-stencils",
    "title": "Postprocessing and Interpolation",
    "section": "Friction requires derivatives, use forward or backward stencils",
    "text": "Friction requires derivatives, use forward or backward stencils\n\n\n\n\n\n\n\n\n\n\n\nGradient in normal direction\n\\[\n\\int_{\\Gamma} \\nabla u \\cdot \\boldsymbol{n} d\\Gamma\n\\]\n\nIntegrate gradient at \\(x=1\\)\n\\[\n\\int_{0}^{L_y} \\frac{\\partial u}{\\partial x}(x=L_x, y) dy\n\\]\n\nduy = (N/2)*(-3*u2[:, 0] + 4*u2[:, 1] - u2[:, 2])\nnp.trapz(duy, dx=1/N)\n\n0.4601188642377852\n\n\n\ndudye = sp.diff(ue, y)\nfloat(sp.integrate(dudye.subs(y, 0), (x, 0, 1)).n())\n\n0.4596976941318603\n\n\nIntegrate over boundary at \\(y=1\\)\n\\[\n\\int_{0}^{L_x} \\frac{\\partial u}{\\partial y}(x, y=L_y) dx\n\\]\n\nduy = (N/2)*(-3*u2[0] + 4*u2[1] - u2[2])\nnp.trapz(duy, dx=1/N) \n\n-0.15854507179346725"
  },
  {
    "objectID": "postprocessing.html#two-dimensional-wave-equation-1",
    "href": "postprocessing.html#two-dimensional-wave-equation-1",
    "title": "Postprocessing and Interpolation",
    "section": "Two-dimensional wave equation",
    "text": "Two-dimensional wave equation\n\\[\n\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\nabla^2 u, \\quad \\Omega = (0, L_x) \\times (0, L_y), t \\in (0, T]\n\\]\nWe use an initial condition \\(u(x, y, 0) = I(x, y)\\) and \\(\\frac{\\partial u}{\\partial t}(x, y, 0)=0\\) and homogeneous Dirichlet boundary conditions \\(u(x, y, t)=0\\) for the entire boundary.\n\nFor 2D waves we have frequencies in two directions\n\\[\n\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\n\\]\n\n\nAny solution to the wave equation can be written as combinations of waves\n\\[\nu(x, y, t) = g(k_x x + k_y y - |\\boldsymbol{k}|ct)\n\\]\n\n\n\n\n\n\nNote\n\n\nTry to validate by inserting for \\(u(x, y, t)= g(k_x x + k_y y - |\\boldsymbol{k}|ct)\\) into the wave equation. Use for example a variable \\(\\alpha = k_x x + k_y y - |\\boldsymbol{k}|ct\\) and the chain rule, such that \\(\\frac{\\partial g}{\\partial t} = \\frac{\\partial g}{\\partial \\alpha} \\frac{\\partial \\alpha}{\\partial t}\\) etc."
  },
  {
    "objectID": "postprocessing.html#fourier-exponentials-are-solution-in-a-periodic-domain",
    "href": "postprocessing.html#fourier-exponentials-are-solution-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Fourier exponentials are solution in a periodic domain",
    "text": "Fourier exponentials are solution in a periodic domain\n\\[\nu(x, y, t) = e^{\\imath (\\boldsymbol{k} \\cdot \\boldsymbol{x} - \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\)."
  },
  {
    "objectID": "postprocessing.html#fourier-exponentials-are-solutions-in-a-periodic-domain",
    "href": "postprocessing.html#fourier-exponentials-are-solutions-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Fourier exponentials are solutions in a periodic domain",
    "text": "Fourier exponentials are solutions in a periodic domain\n\\[\nu(x, y, t) = e^{\\hat{\\imath} (\\boldsymbol{k} \\cdot \\boldsymbol{x} + \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\) and \\(\\hat{\\imath}=\\sqrt{-1}\\).\n\n\n\n\n\n\nNote\n\n\n\\(\\boldsymbol{x} = x \\boldsymbol{i} + y \\boldsymbol{j}\\) and \\(\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\\), such that \\(\\boldsymbol{k} \\cdot \\boldsymbol{x} = k_x x + k_y y\\).\n\n\n\n\nA mesh solution is then\n\\[\n\\begin{align}\nu^n_{ij} &= u(x_i, y_j, t_n) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y + \\omega n \\Delta t)} \\\\\n&=(e^{\\hat{\\imath} \\omega \\Delta t})^n e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)} \\\\\n&= A^n E(i, j)\n\\end{align}\n\\]\nwith amplification factor \\(A=e^{\\hat{\\imath} \\omega \\Delta t}\\) and \\(E(i, j) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)}\\)."
  },
  {
    "objectID": "postprocessing.html#discretization-of-the-wave-equation-in-2d",
    "href": "postprocessing.html#discretization-of-the-wave-equation-in-2d",
    "title": "Postprocessing and Interpolation",
    "section": "Discretization of the wave equation in 2D",
    "text": "Discretization of the wave equation in 2D\nFor all internal points we have the second order accurate \\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nor on vectorized form using \\(U^n = \\Big( u(x_i, y_j, n \\Delta t)\\Big)_{i,j=0}^{N, N}\\) and \\(D_x^{(2)}\\) and \\(D_y^{(2)}\\) as second derivative matrices for \\(x\\) and \\(y\\) directions, respectively\n\\[\n\\frac{U^{n+1}-2U^n+U^{n-1}}{\\Delta t^2} = c^2 \\left( D^{(2)}_x U^n + U^n (D^{(2)}_y)^T \\right).\n\\]"
  },
  {
    "objectID": "postprocessing.html#stability",
    "href": "postprocessing.html#stability",
    "title": "Postprocessing and Interpolation",
    "section": "Stability",
    "text": "Stability\nWe have\n\\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nTo calculate the stability of this scheme we need to insert for \\(u^n_{ij} = A^n E(i, j)\\) and compute the amplification factor \\(A\\). For stability we require\n\\[\n|A| \\le 1\n\\]\nThis is the same stability criterion as used in the 1D case."
  },
  {
    "objectID": "postprocessing.html#stability-1",
    "href": "postprocessing.html#stability-1",
    "title": "Postprocessing and Interpolation",
    "section": "Stability",
    "text": "Stability\n\\[\nA - 2 + A^{-1} =\nc^2 \\Delta t^2 \\left( \\frac{ e^{\\hat{\\imath} k_x \\Delta x} - 2 + e^{- \\hat{\\imath} k_x \\Delta x} }{\\Delta x^2} + \\frac{ e^{\\hat{\\imath} k_y \\Delta y} - 2 + e^{- \\hat{\\imath} k_y \\Delta y} }{\\Delta y^2} \\right)\n\\]\ncan be written as\n\\[\nA + A^{-1} = \\beta\n\\]\nwith\n\\[\n\\beta =  2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right)\n\\]\nwhere we have used \\(e^{\\hat{\\imath} x} + e^{-\\hat{\\imath} x} = 2\\cos x\\).\n\n\n\n\n\n\nNote\n\n\nThe derivation is more or less exactly as for the wave equation in 1D."
  },
  {
    "objectID": "postprocessing.html#from-lecture-5-we-know-that",
    "href": "postprocessing.html#from-lecture-5-we-know-that",
    "title": "Postprocessing and Interpolation",
    "section": "From lecture 5 we know that",
    "text": "From lecture 5 we know that\n\\[\nA + A^{-1} = \\beta\n\\]\nimplies that \\(|A|=1\\) when\n\\[\n-2 \\le \\beta \\le 2\n\\]"
  },
  {
    "objectID": "postprocessing.html#from-lecture-5-we-remember-that",
    "href": "postprocessing.html#from-lecture-5-we-remember-that",
    "title": "Postprocessing and Interpolation",
    "section": "From lecture 5 we remember that",
    "text": "From lecture 5 we remember that\n\\[\nA + A^{-1} = \\beta\n\\]\nimplies that \\(|A|=1\\) when\n\\[\n-2 \\le \\beta \\le 2\n\\]\nHence, for stability we need\n\\[\n-2 \\le 2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 2\n\\]\nor\n\\[\n-2 \\le c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 0\n\\]"
  },
  {
    "objectID": "postprocessing.html#stability-limit-on-time-step",
    "href": "postprocessing.html#stability-limit-on-time-step",
    "title": "Postprocessing and Interpolation",
    "section": "Stability limit on time step",
    "text": "Stability limit on time step\nSince \\(\\cos (k_x \\Delta x)\\) and \\(\\cos (k_y \\Delta y)\\) are at worst \\(-1\\) each, we get from \\[ \\small\n-2 \\le c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right) \\le 0\n\\]\nthat\n\\[ \\small\n-2 \\le c^2 \\Delta t^2 \\left( \\frac{-2}{\\Delta x^2} + \\frac{-2}{\\Delta y^2} \\right)\n\\]\nwhich is simplified further into\n\\[ \\small\n\\left(\\frac{c \\Delta t}{\\Delta x} \\right)^2 + \\left(\\frac{c \\Delta t}{\\Delta y} \\right)^2 \\le 1\n\\]\nThis is the stability limit on \\(\\Delta t\\) for the 2D wave equation. If \\(\\Delta x = \\Delta y = h\\), then\n\\[ \\small\n\\frac{c \\Delta t}{h} \\le \\frac{1}{\\sqrt{2}}\n\\]"
  },
  {
    "objectID": "postprocessing.html#insert-for-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "href": "postprocessing.html#insert-for-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Insert for \\(u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation",
    "text": "Insert for \\(u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation\n\\[ \\small\n\\begin{equation}\n\\begin{split}\n\\frac{(A^{n+1} - 2A^n + A^{n-1})E(i, j)}{\\Delta t^2} &=\nc^2 A^n \\Big\\{ \\\\\n    & \\frac{ E(i+1, j) - 2E(i, j) +  E(i-1, j)}{\\Delta x^2} \\\\\n+ & \\frac{E(i, j+1) - 2E(i, j) + e(i, j-1)}{\\Delta y^2} \\Big\\}\n\\end{split}\n\\end{equation}\n\\]\nDivide by \\(A^n E(i, j)\\) and multiply by \\(\\Delta t^2\\) in order to find \\(A\\). We also use that, e.g.,\n\\[ \\small\n\\frac{E(i+1, j)}{E(i, j)} = \\frac{e^{\\hat{\\imath}((i+1)k_x \\Delta x + j k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{\\hat{\\imath} k_x \\Delta x}\n\\]\n\\[ \\small\n\\frac{E(i, j-1)}{E(i, j)} = \\frac{e^{\\hat{\\imath}(ik_x \\Delta x + (j-1) k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{- \\hat{\\imath} k_y \\Delta y}\n\\]"
  },
  {
    "objectID": "postprocessing.html#after-some-manipulations-we-get",
    "href": "postprocessing.html#after-some-manipulations-we-get",
    "title": "Postprocessing and Interpolation",
    "section": "After some manipulations we get",
    "text": "After some manipulations we get\n\\[\nA - 2 + A^{-1} =\nc^2 \\Delta t^2 \\left( \\frac{ e^{\\hat{\\imath} k_x \\Delta x} - 2 + e^{- \\hat{\\imath} k_x \\Delta x} }{\\Delta x^2} + \\frac{ e^{\\hat{\\imath} k_y \\Delta y} - 2 + e^{- \\hat{\\imath} k_y \\Delta y} }{\\Delta y^2} \\right)\n\\]\nwhich can be written as\n\\[\nA + A^{-1} = \\beta\n\\]\nwith\n\\[\n\\beta =  2 + 2 c^2 \\Delta t^2 \\left(\\frac{\\cos (k_x \\Delta x) - 1}{\\Delta x^2} + \\frac{\\cos (k_y \\Delta y) - 1}{\\Delta y^2} \\right)\n\\]\nwhere we have used \\(e^{\\hat{\\imath} x} + e^{-\\hat{\\imath} x} = 2\\cos x\\).\n\n\n\n\n\n\nNote\n\n\nThe derivation is more or less exactly as for the wave equation in 1D."
  },
  {
    "objectID": "postprocessing.html#implementation",
    "href": "postprocessing.html#implementation",
    "title": "Postprocessing and Interpolation",
    "section": "Implementation",
    "text": "Implementation\ndef D2(N):\n  D = sparse.diags([1, -2, 1], [-1, 0, 1], (N+1, N+1), 'lil')\n  D[0, :4] = 2, -5, 4, -1\n  D[-1, -4:] = -1, 4, -5, 2\n  return D\n\ndef solver(N, L, Nt, cfl=0.5, c=1, store_data=10, u0=lambda x, y: np.exp(-40*((x-0.6)**2+(y-0.5)**2))):\n  xij, yij = mesh2D(N, N, L, L)\n  Unp1, Un, Unm1 = np.zeros((3, N+1, N+1))\n  Unm1[:] = u0(xij, yij)\n  dx = L / N\n  D = D2(N)/dx**2\n  dt = cfl*dx/c\n  Un[:] = Unm1[:] + 0.5*(c*dt)**2*(D @ Un + Un @ D.T)\n  plotdata = {0: Unm1.copy()}\n  for n in range(1, Nt):\n    Unp1[:] = 2*Un - Unm1 + (c*dt)**2*(D @ Un + Un @ D.T)\n    # Set boundary conditions\n    # Swap solutions\n    # Store plotdata\n  return xij, yij, plotdata"
  },
  {
    "objectID": "postprocessing.html#test-solver",
    "href": "postprocessing.html#test-solver",
    "title": "Postprocessing and Interpolation",
    "section": "Test solver",
    "text": "Test solver\nCFL = 0.5\nxij, yij, data = solver(40, 1, 501, cfl=CFL, store_data=5)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nframes = []\nfor n, val in data.items():\n  frame = ax.plot_wireframe(xij, yij, val, rstride=2, cstride=2);\n  frames.append([frame])\n\nani = animation.ArtistAnimation(fig, frames, interval=400, blit=True,  repeat_delay=1000)"
  },
  {
    "objectID": "postprocessing.html#test-solver-with-slightly-higher-than-allowed-cfl",
    "href": "postprocessing.html#test-solver-with-slightly-higher-than-allowed-cfl",
    "title": "Postprocessing and Interpolation",
    "section": "Test solver with slightly higher than allowed CFL",
    "text": "Test solver with slightly higher than allowed CFL\nCFL = 0.71\nxij, yij, data = solver(40, 1, 171, cfl=CFL, store_data=5)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nframes = []\nfor n, val in data.items():\n  frame = ax.plot_wireframe(xij, yij, val, rstride=2, cstride=2);\n  frames.append([frame])\n\nani = animation.ArtistAnimation(fig, frames, interval=400, blit=True,  repeat_delay=1000)"
  },
  {
    "objectID": "postprocessing.html#stability-considerations-make-use-of-fourier-exponentials-as-solutions-in-a-periodic-domain",
    "href": "postprocessing.html#stability-considerations-make-use-of-fourier-exponentials-as-solutions-in-a-periodic-domain",
    "title": "Postprocessing and Interpolation",
    "section": "Stability considerations make use of Fourier exponentials as solutions in a periodic domain",
    "text": "Stability considerations make use of Fourier exponentials as solutions in a periodic domain\n\\[\nu(x, y, t) = e^{\\hat{\\imath} (\\boldsymbol{k} \\cdot \\boldsymbol{x} + \\omega t)}\n\\]\nwhere \\(\\omega = |\\boldsymbol{k}| c\\) and \\(\\hat{\\imath}=\\sqrt{-1}\\).\n\n\n\n\n\n\nNote\n\n\n\\(\\boldsymbol{x} = x \\boldsymbol{i} + y \\boldsymbol{j}\\) and \\(\\boldsymbol{k} = k_x \\boldsymbol{i} + k_y \\boldsymbol{j}\\), such that \\(\\boldsymbol{k} \\cdot \\boldsymbol{x} = k_x x + k_y y\\).\n\n\n\n\nA mesh solution is then\n\\[\n\\begin{align}\nu^n_{ij} &= u(x_i, y_j, t_n) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y + \\omega n \\Delta t)} \\\\\n&=(e^{\\hat{\\imath} \\omega \\Delta t})^n e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)} \\\\\n&= A^n E(i, j)\n\\end{align}\n\\]\nwith amplification factor \\(A=e^{\\hat{\\imath} \\omega \\Delta t}\\) and \\(E(i, j) = e^{\\hat{\\imath} (i k_x \\Delta x + j k_y \\Delta y)}\\)."
  },
  {
    "objectID": "postprocessing.html#stability-of-discretization",
    "href": "postprocessing.html#stability-of-discretization",
    "title": "Postprocessing and Interpolation",
    "section": "Stability of discretization",
    "text": "Stability of discretization\nWe have\n\\[\n\\frac{u^{n+1}_{i,j} - 2u^n_{i,j} + u^{n-1}_{i, j}}{\\Delta t^2} =\nc^2 \\left(\\frac{u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1, j}}{\\Delta x^2} + \\frac{u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i, j-1}}{\\Delta y^2}\\right)\n\\]\n\nTo calculate the stability of this scheme we need to insert for \\(u^n_{ij} = A^n E(i, j)\\) and compute the amplification factor \\(A\\). For stability we require\n\\[\n|A| \\le 1\n\\]\nThis is the same stability criterion as used in the 1D case."
  },
  {
    "objectID": "postprocessing.html#how-to-specify-initial-conditions",
    "href": "postprocessing.html#how-to-specify-initial-conditions",
    "title": "Postprocessing and Interpolation",
    "section": "How to specify initial conditions",
    "text": "How to specify initial conditions\nThe initial condition \\(\\frac{\\partial u}{\\partial t}(x, y, 0) = 0\\) can be implemented like we did in lecture 5 for the wave equation with one spatial dimension. We use a ghost node:\n\\[\n\\frac{\\partial u}{\\partial t}(x, y, t=0) = 0 = \\frac{U^1-U^{-1}}{2 \\Delta t} \\rightarrow U^1 = U^{-1},\n\\]\nAnd we use the PDE for \\(n=0\\)\n\\[\n\\frac{U^{1}-2U^0+U^{-1}}{\\Delta t^2} = c^2 \\left( D^{(2)}_x U^0 + U^0 (D^{(2)}_y)^T \\right),\n\\]\nsuch that\n\\[\nU^1 = U^0 + \\frac{c^2 \\Delta t^2}{2} \\left( D^{(2)}_x U^0 + U^0 (D^{(2)}_y)^T \\right).\n\\]"
  },
  {
    "objectID": "postprocessing.html#ths-solution-algorithm-for-the-2d-wave-equation",
    "href": "postprocessing.html#ths-solution-algorithm-for-the-2d-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Ths solution algorithm for the 2D wave equation",
    "text": "Ths solution algorithm for the 2D wave equation\n\nSpecify \\(U^0\\) and \\(U^1\\) from initial conditions\nfor n in (1, 2, …, \\(N_t-1\\)) compute\n\n\\(U^{n+1} = 2U^n - U^{n-1} + (c\\Delta t)^2 \\left( D^{(2)}_x U^n + U^n (D^{(2)}_y)^T \\right)\\)\nApply boundary conditions to \\(U^{n+1}\\)\nSwap \\(U^{n-1} \\leftarrow U^n\\) and \\(U^n \\leftarrow U^{n+1}\\) if using only three solution vectors"
  },
  {
    "objectID": "postprocessing.html#insert-for-small-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "href": "postprocessing.html#insert-for-small-un_ij-an-ei-j-in-the-discretized-wave-equation",
    "title": "Postprocessing and Interpolation",
    "section": "Insert for \\(\\small u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation",
    "text": "Insert for \\(\\small u^n_{ij} = A^n E(i, j)\\) in the discretized wave equation\n\\[ \\small\n\\begin{equation}\n\\begin{split}\n\\frac{(A^{n+1} - 2A^n + A^{n-1})E(i, j)}{\\Delta t^2} &=\nc^2 A^n \\Big\\{ \\\\\n    & \\frac{ E(i+1, j) - 2E(i, j) +  E(i-1, j)}{\\Delta x^2} \\\\\n+ & \\frac{E(i, j+1) - 2E(i, j) + E(i, j-1)}{\\Delta y^2} \\Big\\}\n\\end{split}\n\\end{equation}\n\\]\nDivide by \\(A^n E(i, j)\\) and multiply by \\(\\Delta t^2\\) in order to find \\(A\\). We also use that, e.g.,\n\\[ \\small\n\\frac{E(i+1, j)}{E(i, j)} = \\frac{e^{\\hat{\\imath}((i+1)k_x \\Delta x + j k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{\\hat{\\imath} k_x \\Delta x}\n\\]\n\\[ \\small\n\\frac{E(i, j-1)}{E(i, j)} = \\frac{e^{\\hat{\\imath}(ik_x \\Delta x + (j-1) k_y \\Delta y)}}{e^{\\hat{\\imath}(i k_x \\Delta x + j k_y \\Delta y)}} = e^{- \\hat{\\imath} k_y \\Delta y}\n\\]"
  },
  {
    "objectID": "functions.html#function-approximation-with-global-functions",
    "href": "functions.html#function-approximation-with-global-functions",
    "title": "Function approximation with global functions",
    "section": "Function approximation with global functions",
    "text": "Function approximation with global functions\nConsider a generic function\n\\[\nu(x), \\quad x \\in [a, b],\n\\]\nwhere \\(u(x)\\) can by anything, like \\(u(x)=\\exp(\\sin(2\\pi x))\\), \\(u(x) = x^x\\), etc., etc.\nWe want to find an approximation to \\(u(x)\\) using\n\\[\nu(x) \\approx u_N(x) = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x)\n\\]\nwhere \\(\\psi_k(x)\\) is a global basis function defined on the entire domain \\([a, b]\\)."
  },
  {
    "objectID": "functions.html#function-spaces",
    "href": "functions.html#function-spaces",
    "title": "Function approximation with global functions",
    "section": "Function spaces",
    "text": "Function spaces\n\\[\nu_N(x) = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x)\n\\]\n\n\\(\\psi_k(x)\\) is a basis function\n\\(\\{\\psi_k\\}_{k=0}^N\\) is a basis\n\\(\\{\\hat{u}_k\\}_{k=0}^N\\) are expansion coefficients (the unknown)\n\\(V_N = \\text{span}\\{\\psi_k\\}_{k=0}^N\\) is a function space\n\nFor example, \\(V_N = \\text{span}\\{x^k\\}_{k=0}^N\\) is the space of all polynomials of order less than or equal to \\(N\\). But it is more common to call this space \\(\\mathbb{P}_N\\)."
  },
  {
    "objectID": "functions.html#in-the-approximation",
    "href": "functions.html#in-the-approximation",
    "title": "Function approximation with global functions",
    "section": "In the approximation",
    "text": "In the approximation\n\\[\nu_N(x) = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x) \\tag{1}\n\\]\n\n\\(\\psi_k(x)\\) is a basis function\n\\(\\{\\psi_k\\}_{k=0}^N\\) is a basis\n\\(\\{\\hat{u}_k\\}_{k=0}^N\\) are expansion coefficients (the unknowns)\n\\(V_N = \\text{span}\\{\\psi_k\\}_{k=0}^N\\) is a function space (which is a vector space)\n\n\nFor example, \\(V_N = \\text{span}\\{x^k\\}_{k=0}^N\\) is the space of all polynomials of order less than or equal to \\(N\\). (More commonly referred to as \\(\\mathbb{P}_N\\))\n\n\nIf we say that \\(u_N \\in V_N\\), then we mean that \\(u_N\\) can be written as \\((1)\\) and that \\(u_N\\) is a vector in the vector space (or function space) \\(V_N\\)."
  },
  {
    "objectID": "functions.html#lets-consider-a-simple-example",
    "href": "functions.html#lets-consider-a-simple-example",
    "title": "Function approximation with global functions",
    "section": "Lets consider a simple example",
    "text": "Lets consider a simple example\nLet \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1,2]\\)."
  },
  {
    "objectID": "functions.html#let-ux-10x-12-1-for-xin-12",
    "href": "functions.html#let-ux-10x-12-1-for-xin-12",
    "title": "Function approximation with global functions",
    "section": "Let \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1,2]\\)",
    "text": "Let \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1,2]\\)\n\n\n\n\n\n\n\n\n\nLet \\(V_N = \\{1, x\\}\\) be the space of all straight lines.\nWhat is the best approximation \\(u_N \\in V_N\\) to the function \\(u(x)\\)?\n\nHow do we decide what is best? All we know is that\n\\[\nu_N(x) = \\hat{u}_0 + \\hat{u}_1 x.\n\\]\nHow to find the best \\(\\{\\hat{u}_0, \\hat{u}_1\\}\\)?"
  },
  {
    "objectID": "functions.html#how-to-find-the-best-approximations",
    "href": "functions.html#how-to-find-the-best-approximations",
    "title": "Function approximation with global functions",
    "section": "How to find the best approximations?",
    "text": "How to find the best approximations?\n\n\nThe least squares method\nThe Galerkin method\nThe collocation method"
  },
  {
    "objectID": "functions.html#how-to-find-the-best-approximation",
    "href": "functions.html#how-to-find-the-best-approximation",
    "title": "Function approximation with global functions",
    "section": "How to find the best approximation?",
    "text": "How to find the best approximation?\nWhat options are there? Who decides what is best?\n\n\n\nThe three methods covered in this class are\n\nThe least squares method\nThe Galerkin method\nThe collocation method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first two are variational methods, whereas the third is an interpolation method.\nEach method will give us \\(N+1\\) equations for the \\(N+1\\) unknowns \\(\\{\\hat{u}_k\\}_{k=0}^N\\)!\n\n\n\nA variational method derives equations by using integration over the domain\nA collocation method derives equations at different points in the mesh"
  },
  {
    "objectID": "functions.html#variational-methods",
    "href": "functions.html#variational-methods",
    "title": "Function approximation with global functions",
    "section": "Variational methods",
    "text": "Variational methods\nSince a variational method derives equations by integrating over the domain, we need to define a special notation. The \\(L^2\\) inner product is defined as\n\\[\n\\left(f, g \\right) = \\int_{\\Omega} f(x)g(x) d\\Omega,\n\\]\nfor two real functions \\(f(x)\\) and \\(g(x)\\) (complex functions have a slightly different inner product). The symbol \\(\\Omega\\) here represents the domain of interest. For our first example \\(\\Omega = [1, 2]\\).\n\n\n\n\n\n\nNote\n\n\nSometimes the inner product is written as \\(\\left(f, g \\right)_{L^2(\\Omega)}\\), in order to clarify that it is the \\(L^2\\) inner product on a certain domain. Normally, the \\(L^2(\\Omega)\\) subscript will be dropped."
  },
  {
    "objectID": "functions.html#the-small-l2omega-norm",
    "href": "functions.html#the-small-l2omega-norm",
    "title": "Function approximation with global functions",
    "section": "The \\(\\small L^2(\\Omega)\\) norm",
    "text": "The \\(\\small L^2(\\Omega)\\) norm\nThe inner product is used to define an \\(L^2(\\Omega)\\) norm\n\\[\n\\|u\\| = \\sqrt{(u, u)}\n\\]\nThe norm gives us a measure for the length or size of a vector.\nThe \\(\\small L^2(\\Omega)\\) error norm\nIf we define the pointwise error as\n\\[\ne(x) = u(x) - u_N(x)\n\\]\nthen the \\(L^2(\\Omega)\\) error norm\n\\[\n\\| e \\| = \\sqrt{(e, e)} = \\sqrt{\\int_{\\Omega} e^2 dx}\n\\]\ngives us a measure of the error over the entire domain"
  },
  {
    "objectID": "functions.html#back-to-our-example---how-do-we-find-the-best-possible-u_n-approx-u",
    "href": "functions.html#back-to-our-example---how-do-we-find-the-best-possible-u_n-approx-u",
    "title": "Function approximation with global functions",
    "section": "Back to our example - how do we find the best possible \\(u_N \\approx u\\)?",
    "text": "Back to our example - how do we find the best possible \\(u_N \\approx u\\)?\nIf we use the error\n\\[\ne(x) = u(x) - u_N(x)\n\\]\nthen the \\(L^2\\) error norm \\(\\|e\\|\\) represents the error integrated over the entire domain.\n\n\n\n\n\n\n\nIdea!\n\n\nWhy don’t we find \\(u_N\\) such that it minimizes the \\(L^2\\) error norm!\n\n\n\n\n\n\n\\(\\longrightarrow\\) The Least squares method!"
  },
  {
    "objectID": "functions.html#the-least-squares-method-for-function-approximation",
    "href": "functions.html#the-least-squares-method-for-function-approximation",
    "title": "Function approximation with global functions",
    "section": "The least squares method for function approximation",
    "text": "The least squares method for function approximation\nWe want to approximate\n\\[\nu_N(x) \\approx u(x)\n\\]\nusing a function space \\(V_N\\) for \\(u_N\\). The pointwise error in \\(u_N\\) is defined as\n\\[\ne(x) = u(x) - u_N(x)\n\\]\nDefine the square of the \\(L^2\\) error norm\n\\[\nE = \\| e\\|^2 = (e, e)\n\\]\nThe least squares method is to find \\(u_N \\in V_N\\) such that\n\\[\n\\frac{\\partial E}{\\partial \\hat{u}_j} = 0, \\quad j=0, 1, \\ldots, N.\n\\]"
  },
  {
    "objectID": "functions.html#the-least-squares-method",
    "href": "functions.html#the-least-squares-method",
    "title": "Function approximation with global functions",
    "section": "The least squares method",
    "text": "The least squares method\nFind \\(u_N \\in V_N\\) such that\n\\[\n\\frac{\\partial E}{\\partial \\hat{u}_j} = 0, \\quad j=0, 1, \\ldots, N\n\\]\nThis gives us \\(N+1\\) equations for \\(N+1\\) unknowns \\(\\{\\hat{u}_j\\}_{j=0}^N\\).\n\nBut how can we compute \\(\\frac{\\partial E}{\\partial \\hat{u}_j}\\)?\n\n\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_j} = \\frac{\\partial }{\\partial \\hat{u}_j} \\int_{\\Omega} e^2 dx = \\int_{\\Omega} 2e \\frac{\\partial e}{\\partial \\hat{u}_j}dx\n\\]\n\n\nInsert for \\(e(x)=u(x)-u_N(x)=u(x)-\\sum_{k=0}^N \\hat{u}_k \\psi_k\\)\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_j} = \\int 2\\left( u-\\sum_{k=0}^N \\hat{u}_k \\psi_k \\right) \\overbrace{\\frac{\\partial}{\\partial \\hat{u}_j} \\left(u - \\sum_{k=0}^N \\hat{u}_k \\psi_k \\right)}^{-\\psi_j} dx\n\\]"
  },
  {
    "objectID": "functions.html#lsm-continued",
    "href": "functions.html#lsm-continued",
    "title": "Function approximation with global functions",
    "section": "LSM continued",
    "text": "LSM continued\nHow can we compute \\(\\frac{\\partial E}{\\partial \\hat{u}_k}\\)?\n\nStraightforward\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_k} = \\frac{\\partial }{\\partial \\hat{u}_k} \\int e^2 dx = \\int 2e \\frac{\\partial e}{\\partial \\hat{u}_k}dx\n\\]\n\n\nInsert for \\(u_N = \\sum_{j=0}^N \\hat{u}_j \\psi_j\\)\n\\[\n\\begin{align}\n\\frac{\\partial E}{ \\partial \\hat{u}_k} &= \\int 2\\left( u-\\sum_{j=0}^N \\hat{u}_j \\psi_j \\right) \\underbrace{\\frac{\\partial}{\\partial \\hat{u}_k} \\left(u - \\sum_{j=0}^N \\hat{u}_j \\psi_j \\right)}_{=-\\psi_k} dx \\\\\n&=-2 \\int\\left( u-\\sum_{j=0}^N \\hat{u}_j \\psi_j \\right)  \\psi_k  dx\n\\end{align}\n\\]"
  },
  {
    "objectID": "functions.html#section",
    "href": "functions.html#section",
    "title": "Function approximation with global functions",
    "section": "",
    "text": "Legendre polynomials also have very good approximation properties, meaning that a Legendre series converges very quickly\n\\[\nu_N(x) = \\sum_{k=0}^N \\hat{u}_k P_k(x)\n\\]"
  },
  {
    "objectID": "functions.html#lsm-continued-1",
    "href": "functions.html#lsm-continued-1",
    "title": "Function approximation with global functions",
    "section": "LSM continued",
    "text": "LSM continued\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_k} = 0\n\\]\ngives us that\n\\[\n\\int u \\psi_j dx = \\sum_{k=0}^N\\int \\psi_k \\psi_j dx \\hat{u}_k\n\\]"
  },
  {
    "objectID": "functions.html#if-we-now-set-smallfracpartial-e-partial-hatu_k0-then",
    "href": "functions.html#if-we-now-set-smallfracpartial-e-partial-hatu_k0-then",
    "title": "Function approximation with global functions",
    "section": "If we now set \\(\\small\\frac{\\partial E}{ \\partial \\hat{u}_k}=0\\), then",
    "text": "If we now set \\(\\small\\frac{\\partial E}{ \\partial \\hat{u}_k}=0\\), then\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_k} = -\\int 2\\left( u-\\sum_{j=0}^N \\hat{u}_j \\psi_j \\right) \\psi_k dx = 0\n\\]\n\\[\n\\longrightarrow \\int u \\psi_j dx = \\sum_{k=0}^N\\int \\psi_k \\psi_j dx \\hat{u}_k, \\quad j=0,1, \\ldots, N.\n\\]\ngives us \\(N+1\\) equations for the \\(N+1\\) unknown \\(\\{\\hat{u}_k\\}_{k=0}^N\\)\n\n\nBack to the example: \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1, 2]\\)\nFind \\(u_N \\in \\mathbb{P}_1\\) (meaning find \\(\\hat{u}_0\\) and \\(\\hat{u}_1\\)) such that\n\\[\n\\frac{\\partial E}{\\partial \\hat{u}_0}=0 \\text{ and } \\frac{\\partial E}{\\partial \\hat{u}_1}=0\n\\]\nTwo equations for two unknowns."
  },
  {
    "objectID": "functions.html#implement-in-sympy",
    "href": "functions.html#implement-in-sympy",
    "title": "Function approximation with global functions",
    "section": "Implement in Sympy",
    "text": "Implement in Sympy\n\nfrom IPython.display import display\n\ndef inner(u, v, domain=(-1, 1), x=x):\n    return sp.integrate(u*v, (x, domain[0], domain[1]))\n\nu0, u1 = sp.symbols('u0,u1')\nerr = u-(u0+u1*x)\nE = inner(err, err, (1, 2))\neq1 = sp.Eq(sp.diff(E, u0, 1), 0)\neq2 = sp.Eq(sp.diff(E, u1, 1), 0)\ndisplay(eq1)\ndisplay(eq2)\n\n\\(\\displaystyle 2 u_{0} + 3 u_{1} - \\frac{14}{3} = 0\\)\n\n\n\\(\\displaystyle 3 u_{0} + \\frac{14 u_{1}}{3} - \\frac{26}{3} = 0\\)\n\n\nSolve the equations\n\nuhat = sp.solve((eq1, eq2), (u0, u1))\nprint(uhat) \nprint(f\"uN = {uhat[u0]} + {uhat[u1]}x\")\n\n{u0: -38/3, u1: 10}\nuN = -38/3 + 10x"
  },
  {
    "objectID": "functions.html#implement-the-2-equations-in-sympy",
    "href": "functions.html#implement-the-2-equations-in-sympy",
    "title": "Function approximation with global functions",
    "section": "Implement the 2 equations in Sympy",
    "text": "Implement the 2 equations in Sympy\n\ndef inner(u, v, domain=(-1, 1), x=x):\n    return sp.integrate(u*v, (x, domain[0], domain[1]))\n\nu0, u1 = sp.symbols('u0,u1')\nerr = u-(u0+u1*x)\nE = inner(err, err, (1, 2))\neq1 = sp.Eq(sp.diff(E, u0, 1), 0)\neq2 = sp.Eq(sp.diff(E, u1, 1), 0)\ndisplay(eq1)\ndisplay(eq2)\n\n\\(\\displaystyle 2 u_{0} + 3 u_{1} - \\frac{14}{3} = 0\\)\n\n\n\\(\\displaystyle 3 u_{0} + \\frac{14 u_{1}}{3} - \\frac{26}{3} = 0\\)\n\n\n\n\n\nuhat = sp.solve((eq1, eq2), (u0, u1))\nprint(uhat) \n\n{u0: -38/3, u1: 10}\n\n\nThe LSM best fit equation is\n\n\nuN = -38/3 + 10x"
  },
  {
    "objectID": "functions.html#linear-algebra-approach",
    "href": "functions.html#linear-algebra-approach",
    "title": "Function approximation with global functions",
    "section": "Linear algebra approach",
    "text": "Linear algebra approach\n\\[\n\\underbrace{\\int u \\psi_j dx}_{b_j} = \\sum_{k=0}^N \\underbrace{\\int \\psi_k \\psi_j dx}_{a_{jk}} \\underbrace{\\hat{u}_k}_{x_k}, \\quad j=0,1, \\ldots, N.\n\\] \\[\n\\boldsymbol{b} = A \\boldsymbol{x} \\longrightarrow \\boldsymbol{x} = A^{-1} \\boldsymbol{b}\n\\]\nAssemble mass matrix \\(a_{jk}=\\int \\psi_k \\psi_j dx\\) and vector \\(b_j = \\int u \\psi_j dx\\)\n\nA = sp.zeros(2, 2)\nb = sp.zeros(2, 1)\npsi = (sp.S(1), x)\nu = 10*(x-1)**2-1\nfor i in range(2):\n    for j in range(2):\n        A[i, j] = sp.integrate(psi[i]*psi[j], (x, 1, 2))\n    b[i] = sp.integrate(psi[i]*u, (x, 1, 2))\nprint(A)\nprint(A**(-1) @ b)\n\nMatrix([[1, 3/2], [3/2, 7/3]])\nMatrix([[-38/3], [10]])\n\n\nSame result as before!"
  },
  {
    "objectID": "functions.html#the-galerkin-method",
    "href": "functions.html#the-galerkin-method",
    "title": "Function approximation with global functions",
    "section": "The Galerkin method",
    "text": "The Galerkin method\nThe error is required to be orthogonal to the basis functions (for the \\(L^2\\) inner product)\n\\[\n(e, \\psi_j) = 0, \\quad  j = 0, 1, \\ldots, N.\n\\]\nAgain \\(N+1\\) equations for \\(N+1\\) unknowns.\n\nNot as much work as the least squares method, and even easier to implement in Sympy:\n\neq1 = sp.Eq(inner(err, 1, domain=(1, 2)), 0)\neq2 = sp.Eq(inner(err, x, domain=(1, 2)), 0)\ndisplay(eq1)\ndisplay(eq2) \n\n\\(\\displaystyle - u_{0} - \\frac{3 u_{1}}{2} + \\frac{7}{3} = 0\\)\n\n\n\\(\\displaystyle - \\frac{3 u_{0}}{2} - \\frac{7 u_{1}}{3} + \\frac{13}{3} = 0\\)\n\n\n\nsp.solve((eq1, eq2), (u0, u1))\n\n{u0: -38/3, u1: 10}"
  },
  {
    "objectID": "functions.html#the-galerkin-formulation-of-the-problem-is",
    "href": "functions.html#the-galerkin-formulation-of-the-problem-is",
    "title": "Function approximation with global functions",
    "section": "The Galerkin formulation of the problem is",
    "text": "The Galerkin formulation of the problem is\nFind \\(u_N \\in V_N\\) such that\n\\[\n(e, v) = 0, \\quad \\forall \\, v \\in V_N\n\\]\nHere \\(v\\) is often referred to as a test function, whereas \\(u_N\\) is referred to as a trial function.\nAlternative formulation:\nFind \\(u_N \\in V_N\\) such that\n\\[\n(e, \\psi_j) = 0, \\quad  j = 0, 1, \\ldots, N.\n\\]\n\n\n\n\n\n\n\nNote\n\n\nIn order to satisfy \\((e, v)=0\\) for all \\(v \\in V_N\\), we can insert for \\(v=\\sum_{j=0}^N\\hat{v}_j \\psi_j\\) such that\n\\[\n\\sum_{j=0}^N (e, \\psi_j) \\hat{v}_j =0.\n\\] In order for this to always be satisfied, we require that \\((e, \\psi_j)=0\\) for all \\(j=0,1,\\ldots, N\\)."
  },
  {
    "objectID": "functions.html#the-linear-algebra-problem-for-the-galerkin-method",
    "href": "functions.html#the-linear-algebra-problem-for-the-galerkin-method",
    "title": "Function approximation with global functions",
    "section": "The linear algebra problem for the Galerkin method",
    "text": "The linear algebra problem for the Galerkin method\nWe have the \\(N+1\\) equations\n\\[\n(e, \\psi_i) = 0, \\quad  i = 0, 1, \\ldots, N.\n\\]\nInsert for \\(e=u-u_N\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) to get\n\\[\n(u - \\sum_{j=0}^N \\hat{u}_j \\psi_j, \\psi_i) = 0, \\quad i =0, 1, \\ldots, N\n\\]\nand thus the linear algebra problem \\(A \\boldsymbol{x} = \\boldsymbol{b}\\):\n\\[\n\\sum_{j=0}^N \\underbrace{(\\psi_j, \\psi_i)}_{a_{ij}} \\, \\underbrace{\\hat{u}_j}_{x_j} = \\underbrace{(u, \\psi_i)}_{b_i}, \\quad i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#orthogonality-is-a-big-deal",
    "href": "functions.html#orthogonality-is-a-big-deal",
    "title": "Function approximation with global functions",
    "section": "Orthogonality is a big deal!",
    "text": "Orthogonality is a big deal!\nIf we choose basis \\(\\{\\psi_j\\}_{j=0}^N\\) such that all basis functions are orthonormal, then\n\\[\na_{ij} = (\\psi_j, \\psi_i) = \\delta_{ij} = \\begin{cases}\n1 \\text{ if } i = j, \\\\\n0 \\text{ if } i \\ne j.\n\\end{cases}\n\\]\nand thus\n\\[\nA\\boldsymbol{x} = \\boldsymbol{b} \\rightarrow \\boldsymbol{x} = \\boldsymbol{b} \\quad \\text{or} \\quad \\hat{u}_j = (u, \\psi_j), \\quad j=0, 1, \\ldots, N\n\\]\n\nIf the basis functions are merely orthogonal, then\n\\[\n(\\psi_j, \\psi_i) = \\|\\psi_i\\|^2 \\delta_{ij},\n\\]\nwhere \\(\\|\\psi_i\\|^2\\) is the squared \\(L^2\\) norm of \\(\\psi_i\\). We can still easily solve the linear algebra system (because \\(A\\) is a diagonal matrix)\n\\[\n\\hat{u}_i = \\frac{(u, \\psi_i)}{\\|\\psi_i\\|^2} \\quad \\text{for } i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#polynomial-basis-functions",
    "href": "functions.html#polynomial-basis-functions",
    "title": "Function approximation with global functions",
    "section": "Polynomial basis functions",
    "text": "Polynomial basis functions\nThe monomial basis\n\\[\n\\{x^n \\}_{n=0}^N,\n\\]\nis a basis for \\(\\mathbb{P}_N\\). However, it is not a good basis. This is because the basis functions are not orthogonal and the mass matrix \\(A\\) is ill conditioned. In short, this means it is difficult to solve \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) with good accuracy using finite precision computers.\nThe basis functions are shown below for \\(x\\in [-1, 1]\\)"
  },
  {
    "objectID": "functions.html#legendre-polynomials",
    "href": "functions.html#legendre-polynomials",
    "title": "Function approximation with global functions",
    "section": "Legendre polynomials",
    "text": "Legendre polynomials\nLegendre polynomials are defined on the domain \\(\\Omega = [-1, 1]\\) as\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\nThe first 5 Legendre polynomials are plotted below"
  },
  {
    "objectID": "functions.html#the-legendre-polynomials-are-l2-orthogonal-on-omega-1-1",
    "href": "functions.html#the-legendre-polynomials-are-l2-orthogonal-on-omega-1-1",
    "title": "Function approximation with global functions",
    "section": "The Legendre polynomials are \\(L^2\\) orthogonal on \\(\\Omega=[-1, 1]\\)",
    "text": "The Legendre polynomials are \\(L^2\\) orthogonal on \\(\\Omega=[-1, 1]\\)\n\\[\n(P_j, P_i)_{L^2(-1, 1)} =  \\frac{2}{2j+1}\\delta_{ij}.\n\\]\nOrthogonality in the \\(L^2\\) inner product space makes the Legendre polynomials very popular in the world of numerical methods!\nUsing \\(|P_i|^2 = \\frac{2}{2i+1}\\) we get that\n\\[\n\\hat{u}_j = \\frac{(u, P_j)}{|P_j|^2} = \\frac{2j+1}{2}(u, P_j)\n\\]\n\n\n\n\n\n\nNote\n\n\nThis requires that the domain of \\(u(x)\\) is \\([-1, 1]\\)"
  },
  {
    "objectID": "functions.html#error-in-legendre-approximations",
    "href": "functions.html#error-in-legendre-approximations",
    "title": "Function approximation with global functions",
    "section": "Error in Legendre approximations",
    "text": "Error in Legendre approximations\nThe smooth function \\(u(x)\\) can be exactly represented as\n\\[\nu(x) = \\sum_{k=0}^{\\infty} \\hat{u}_k P_k(x)\n\\]\nFor all \\(k \\le N\\) the series for \\(u(x)\\) and the series for \\(u_N(x)\\) have exactly the same coefficients \\(\\hat{u}_k\\). Hence the error in the approximation \\(u_N\\) is easily computed as\n\\[\ne(x) = u(x) - u_N(x) = \\sum_{k=N+1}^{\\infty} \\hat{u}_k P_k(x)\n\\]\nSpectral convergence - it can be shown that\n\\[\n\\| u - u_N\\| \\sim | \\hat{u}_{N+1}| \\sim e^{-\\mu N}, \\quad \\mu \\in \\mathbb{R}^+\n\\]\nExponential convergence!"
  },
  {
    "objectID": "functions.html#if-the-physical-domain-is-different-from--1-1-then-we-need-to-map",
    "href": "functions.html#if-the-physical-domain-is-different-from--1-1-then-we-need-to-map",
    "title": "Function approximation with global functions",
    "section": "If the physical domain is different from \\([-1, 1]\\), then we need to map",
    "text": "If the physical domain is different from \\([-1, 1]\\), then we need to map\nLet \\(X\\) be the coordinate in the computational (reference) domain \\([A, B]\\), where \\(A\\) and \\(B\\) are real numbers. A linear (affine) mapping, from the computational coordinate \\(X\\) to the physical coordinate \\(x\\) is then\n\\[\n\\begin{equation}\nx = a + \\frac{b-a}{B-A}(X-A).\n\\end{equation}\n\\]\nThe reverse mapping is\n\\[\n\\begin{equation}\nX = A + \\frac{B-A}{b-a}(x-a).\n\\end{equation}\n\\]\nThese mappings are valid for any basis function."
  },
  {
    "objectID": "functions.html#the-legendre-polynomials-are-l2-orthogonal-on-the-reference-domain-omega-1-1",
    "href": "functions.html#the-legendre-polynomials-are-l2-orthogonal-on-the-reference-domain-omega-1-1",
    "title": "Function approximation with global functions",
    "section": "The Legendre polynomials are \\(L^2\\) orthogonal on the reference domain \\(\\Omega=[-1, 1]\\)",
    "text": "The Legendre polynomials are \\(L^2\\) orthogonal on the reference domain \\(\\Omega=[-1, 1]\\)\n\\[\n(P_j, P_i)_{L^2(-1, 1)} =  \\frac{2}{2j+1}\\delta_{ij}.\n\\]\nOrthogonality in the \\(L^2\\) inner product space makes the Legendre polynomials very popular!\nUsing \\(\\|P_j\\|^2 = \\frac{2}{2j+1}\\) we get that\n\\[\n\\hat{u}_j = \\frac{(u, P_j)}{\\|P_j\\|^2} = \\frac{2j+1}{2}(u, P_j)\n\\]\n\n\n\n\n\n\nNote\n\n\nThis requires that the domain of \\(u(x)\\) is \\([-1, 1]\\)"
  },
  {
    "objectID": "functions.html#if-the-physical-domain-a-b-is-different-from--1-1-then-we-need-to-map",
    "href": "functions.html#if-the-physical-domain-a-b-is-different-from--1-1-then-we-need-to-map",
    "title": "Function approximation with global functions",
    "section": "If the physical domain \\([a, b]\\) is different from \\([-1, 1]\\), then we need to map",
    "text": "If the physical domain \\([a, b]\\) is different from \\([-1, 1]\\), then we need to map\nMany well-known basis functions work only on a given reference domain\n\nSines and cosines \\([-\\pi, \\pi]\\)\nLegendre polynomials \\([-1, 1]\\)\nBernstein polynomials \\([0, 1]\\)\nBessel functions \\([0, \\infty]\\)\n\nLet \\(X\\) be the coordinate in the computational (reference) domain \\([A, B]\\), where \\(A\\) and \\(B\\) are real numbers. A linear (affine) mapping, from the computational coordinate \\(X\\) to the physical coordinate \\(x\\) is then\n\\[\n\\begin{equation}\nx = a + \\frac{b-a}{B-A}(X-A).\n\\end{equation}\n\\]\nThe reverse mapping is\n\\[\n\\begin{equation}\nX = A + \\frac{B-A}{b-a}(x-a).\n\\end{equation}\n\\]\nThese mappings are valid for any basis function."
  },
  {
    "objectID": "functions.html#if-the-physical-domain-a-b-is-different-from-the-reference-domain-then-we-need-to-map",
    "href": "functions.html#if-the-physical-domain-a-b-is-different-from-the-reference-domain-then-we-need-to-map",
    "title": "Function approximation with global functions",
    "section": "If the physical domain \\([a, b]\\) is different from the reference domain, then we need to map",
    "text": "If the physical domain \\([a, b]\\) is different from the reference domain, then we need to map\nMany well-known basis functions work only on a given reference domain\n\n\n\nSines and cosines \\([0, \\pi]\\)\nLegendre polynomials \\([-1, 1]\\)\n\n\n\nBernstein polynomials \\([0, 1]\\)\nChebyshev polynomials \\([-1, 1]\\)\n\n\n\n\nLet \\(X\\) be the coordinate in the computational (reference) domain \\([A, B]\\) and \\(x\\) be the coordinate in the true physical domain \\([a, b]\\). A linear (affine) mapping, from \\(X\\) to \\(x\\) (and back) is then\n\\[\nX \\in [A, B] \\quad \\text{and} \\quad x \\in [a, b]\n\\]\n\\[\nx = a + \\frac{b-a}{B-A}(X-A) \\quad \\text{and} \\quad  X = A + \\frac{B-A}{b-a}(x-a)\n\\]"
  },
  {
    "objectID": "functions.html#the-mapping-makes-it-possible-to-use-legendre-polynomials-in-any-domain-a-b",
    "href": "functions.html#the-mapping-makes-it-possible-to-use-legendre-polynomials-in-any-domain-a-b",
    "title": "Function approximation with global functions",
    "section": "The mapping makes it possible to use Legendre polynomials in any domain \\([a, b]\\)",
    "text": "The mapping makes it possible to use Legendre polynomials in any domain \\([a, b]\\)\nThe basis functions \\(\\psi_j(x)\\) are simply\n\\[\n\\psi_j(x) = P_j(X(x)) \\quad X \\in [-1, 1], \\quad x \\in [a, b]\n\\]\n\\[\nX(x) = -1 + \\frac{2}{b-a}(x-a)\n\\]\n\n\nxj = np.linspace(1, 2, 100)\nA, B = -1, 1\na, b = 1, 2\nXj = -1 + (B-A)/(b-a)*(xj-a)\nplt.figure(figsize=(6, 3.5))\nlegend = []\np = np.zeros(100)\nfor n in range(5):\n    l = sp.legendre(n, x)\n    p[:] = sp.lambdify(x, l)(Xj)\n    plt.plot(xj, p)\n    legend.append(f'$P_{n}(X(x))$')\nplt.title(r'$\\psi_j(x)=P_j(X(x))$')\nplt.xlabel('x')\nplt.legend(legend);"
  },
  {
    "objectID": "functions.html#the-mapping-complicates-the-inner-product-slightly",
    "href": "functions.html#the-mapping-complicates-the-inner-product-slightly",
    "title": "Function approximation with global functions",
    "section": "The mapping complicates the inner product slightly",
    "text": "The mapping complicates the inner product slightly\nThe Galerkin method is defined on the true domain using \\(L^2([a, b])\\)\n\\[\n(u(x) - u_N(x), \\psi_j(x))_{L^2([a,b])}= \\int_{a}^b (u(x)-u_N(x)) \\psi_j(x) dx = 0, \\quad j=0,1,\\ldots, N.\n\\]\nInsert for \\(u_N = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x)\\) and rearrange\n\\[\n\\sum_{k=0}^N \\int_{a}^b \\psi_k(x)  \\psi_j(x) \\, dx \\, \\hat{u}_k =  \\int_{a}^b u(x) \\psi_j(x) dx, \\quad  j = 0, 1, \\ldots, N.\n\\]\nSo far just regular Galerkin. Now introduce \\(\\psi_j(x) = P_j(X(x))\\) and integrate with a change of variables \\(\\frac{dx}{dX} = \\frac{b-a}{2}\\). The limits are then \\(X(a)=-1\\) and \\(X(b)=1\\)\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X)\\, \\frac{dx}{dX} \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, \\frac{dx}{dX} \\, dX, \\quad j=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#section-1",
    "href": "functions.html#section-1",
    "title": "Function approximation with global functions",
    "section": "",
    "text": "Legendre polynomials also have very good approximation properties, meaning that a Legendre series converges very quickly\n\\[\nu_N(x) = \\sum_{k=0}^N \\hat{u}_k P_k(x)\n\\]"
  },
  {
    "objectID": "functions.html#legendre-polynomials-1",
    "href": "functions.html#legendre-polynomials-1",
    "title": "Function approximation with global functions",
    "section": "Legendre polynomials",
    "text": "Legendre polynomials\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X) \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, dX, \\quad j=0, 1, \\ldots, N\n\\]\nwhich can be written as\n\\[\n\\sum_{k=0}^N  (P_k(X), P_j(X))_{L^2(-1, 1)} \\, \\hat{u}_k = (u(x(X)), P_j(X))_{L^2(-1, 1)}\n\\]"
  },
  {
    "objectID": "functions.html#the-mapping-complicates-the-inner-product",
    "href": "functions.html#the-mapping-complicates-the-inner-product",
    "title": "Function approximation with global functions",
    "section": "The mapping complicates the inner product",
    "text": "The mapping complicates the inner product\nThe Galerkin method is defined on the true domain using \\(L^2([a, b])\\)\n\\[\n(u(x) - u_N(x), \\psi_j(x))_{L^2([a,b])}= \\int_{a}^b (u(x)-u_N(x)) \\psi_j(x) dx = 0, \\quad j=0,1,\\ldots, N.\n\\]\nInsert for \\(u_N = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x)\\) and rearrange\n\\[\n\\sum_{k=0}^N \\int_{a}^b \\psi_k(x)  \\psi_j(x) \\, dx \\, \\hat{u}_k =  \\int_{a}^b u(x) \\psi_j(x) dx, \\quad  j = 0, 1, \\ldots, N.\n\\]\nSo far just regular Galerkin.\n\nNow introduce \\(\\psi_j(x) = P_j(X(x))\\) and integrate with a change of variables \\(x\\rightarrow X\\). The new integration limits are then \\(X(a)=-1\\) and \\(X(b)=1\\)\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X)\\, \\cancel{\\frac{dx}{dX}} \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, \\cancel{\\frac{dx}{dX}} \\, dX, \\quad j=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#the-mapping-complicates-ctd",
    "href": "functions.html#the-mapping-complicates-ctd",
    "title": "Function approximation with global functions",
    "section": "The mapping complicates ctd",
    "text": "The mapping complicates ctd\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X) \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, dX, \\quad j=0, 1, \\ldots, N\n\\]\nwhich can be written as\n\\[\n\\sum_{k=0}^N  (P_k(X), P_j(X))_{L^2(-1, 1)} \\, \\hat{u}_k = (u(x(X)), P_j(X))_{L^2(-1, 1)}\n\\]"
  },
  {
    "objectID": "functions.html#the-mapping-complicates",
    "href": "functions.html#the-mapping-complicates",
    "title": "Function approximation with global functions",
    "section": "The mapping complicates …",
    "text": "The mapping complicates …\nWe get the linear system with inner products over the reference domain\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X) \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, dX, \\quad j=0, 1, \\ldots, N\n\\]\nwhich can be written as\n\\[\n\\sum_{k=0}^N  \\underbrace{(P_k(X), P_j(X))_{L^2(-1, 1)}}_{\\frac{2}{2j+1} \\delta_{kj}} \\, \\hat{u}_k = (u(x(X)), P_j(X))_{L^2(-1, 1)}\n\\]\nsuch that\n\\[\n\\hat{u}_j = \\frac{2j+1}{2} \\left( u(x(X)), P_j(X) \\right)_{L^2(-1, 1)} \\quad j=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#implementation-of-the-mapped-inner-product",
    "href": "functions.html#implementation-of-the-mapped-inner-product",
    "title": "Function approximation with global functions",
    "section": "Implementation of the mapped inner product",
    "text": "Implementation of the mapped inner product\nThe following inner product is valid for any mapping\n\ndef inner(u, v, domain, ref_domain=(-1, 1)):\n  A, B = ref_domain\n  a, b = domain\n  X = a + (b-a)*(x-A)/(B-A)\n  us = u.subs(x, X) #  u(x(X))\n  return sp.integrate(us*v, (x, A, B))\n\n\nFor example, for our \\(u(x)=10(x-1)^2-1\\) in the domain \\(\\Omega = [a, b] = [1, 2]\\) we get\n\nuhat = lambda u, j: (2*j+1) * inner(u, sp.legendre(j, x), (1, 2))/2\nu = 10*(x-1)**2-1 \nu0, u1 = uhat(u, 0), uhat(u, 1)\nprint(u0, u1) \nprint(f\"uN(x) = {u0}+{u1}X(x)\")\n\n7/3 5\nuN(x) = 7/3+5X(x)\n\n\nThe Legendre polynomials use the reference coordinate \\(X\\), whereas the true function \\(u_N(x)\\) is a function of \\(x\\) from the true space.\n\n\n\n\n\n\nNote\n\n\nThe inner product implemented here is using Sympy’s integrate function, which may be too slow or not find the solution at all. It is normally better to use scipy.integrate.quad"
  },
  {
    "objectID": "functions.html#plot-the-legendre-approximation",
    "href": "functions.html#plot-the-legendre-approximation",
    "title": "Function approximation with global functions",
    "section": "Plot the Legendre approximation",
    "text": "Plot the Legendre approximation\n\nplt.figure(figsize=(4, 3))\nxj = np.linspace(1, 2, 100)\nXj = -1 + 2/(b-a)*(xj-a)\nplt.plot(xj, sp.lambdify(x, u)(xj))\nplt.plot(xj, uhat(u, 0) + uhat(u, 1)*Xj, 'r:')\nplt.legend(['$10(x-1)^2-1$', f'{uhat(u, 0)} + {uhat(u, 1)}X(x)']);\n\n\n\n\n\n\n\n\nThe result is exactly the same as was found with the monomial basis \\(\\{1, x\\}\\).\n\n\n\n\n\n\nTip\n\n\nNumpy has a complete module dedicated to Legendre polynomials"
  },
  {
    "objectID": "functions.html#the-collocation-method",
    "href": "functions.html#the-collocation-method",
    "title": "Function approximation with global functions",
    "section": "The collocation method",
    "text": "The collocation method\nTakes a very different approach than the variational methods, but the objective is still to find \\(u_N \\in V_N\\) such that\n\\[\nu_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x)\n\\]\nThe collocation method requires that for some \\(N+1\\) chosen mesh points \\(\\{x_j\\}_{j=0}^N\\) the following \\(N+1\\) equations are satisfied\n\\[\nu(x_i) - u_N(x_i) = 0, \\quad i = 0, 1, \\ldots, N.\n\\]\nInserting for \\(u_N(x_i)\\) we get the \\(N+1\\) linear algebra equations\n\\[\n\\sum_{j=0}^N \\hat{u}_j \\underbrace{\\psi_{j}(x_i)}_{a_{ij}} = u(x_i) \\longrightarrow\n\\sum_{j=0}^N a_{ij} \\hat{u}_j = u(x_i), \\quad i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#linear-algebra",
    "href": "functions.html#linear-algebra",
    "title": "Function approximation with global functions",
    "section": "Linear algebra",
    "text": "Linear algebra\nwhich can be written as the linear algebra system\n\\[\n\\sum_{j} a_{ij} \\hat{u}_j = u_i,\n\\]\nwhere the matrix components \\(a_{ij} = \\psi_j(x_i)\\) and \\(u_i = u(x_i)\\)."
  },
  {
    "objectID": "functions.html#the-lagrange-interpolation-method-described-in-lecture-7-is-a-collocation-method",
    "href": "functions.html#the-lagrange-interpolation-method-described-in-lecture-7-is-a-collocation-method",
    "title": "Function approximation with global functions",
    "section": "The Lagrange interpolation method described in lecture 7 is a collocation method",
    "text": "The Lagrange interpolation method described in lecture 7 is a collocation method\nThe Lagrange basis functions \\(\\ell_j(x)\\) are defined for points \\(\\{x_j\\}_{j=0}^N\\) such that\n\\[ \\small\n\\ell_j(x) = \\prod_{\\substack{0 \\le m \\le N \\\\ m \\ne j}} \\frac{x-x_m}{x_j-x_m} \\quad \\text{and} \\quad \\ell_j(x_i) = \\delta_{ij} = \\begin{cases} 1 \\quad &\\text{for } i=j \\\\\n0 \\quad &\\text{for }  i\\ne j\n\\end{cases}\n\\]\nHence the matrix \\(a_{ij}= \\psi_j(x_i)=\\delta_{ij}\\) and we can simply use the coefficients\n\\[ \\small\n\\hat{u}_j = u(x_j) \\quad \\text{such that}\\quad u_N(x)=\\sum_{j=0}^Nu(x_j) \\ell_j(x)\n\\]\nThere is no integration and the method is often favored for its simplicity. There is a problem though. How do you choose the collocation points?!\n\n\n\n\n\n\nNote\n\n\nThe Lagrange polynomial here is using all \\(N+1\\) mesh points. This is different from lecture 7, where we only used a few mesh points close to the interpolation point."
  },
  {
    "objectID": "functions.html#lagrange-collocation-method-for-ux10x-12-1-xin12",
    "href": "functions.html#lagrange-collocation-method-for-ux10x-12-1-xin12",
    "title": "Function approximation with global functions",
    "section": "Lagrange collocation method for \\(u(x)=10(x-1)^2-1, x\\in[1,2]\\)",
    "text": "Lagrange collocation method for \\(u(x)=10(x-1)^2-1, x\\in[1,2]\\)\nThe approximation using two collocation points (linear function, \\(u_N \\in V_N=\\text{span}\\{1, x\\}\\)) is\n\\[\nu_N(x) = \\hat{u}_0 \\ell_0(x) + \\hat{u}_1 \\ell_1(x),\n\\]\nor more simply\n\\[\nu_N(x) = u(x_0) \\ell_0(x) + u(x_1) \\ell_1(x).\n\\]\nWe can choose the end points \\(x_0=1\\) and \\(x_1=2\\) and reuse the two functions Lagrangebasis and Lagrangefunction from lecture 7. The result is then as shown on the next slide"
  },
  {
    "objectID": "functions.html#collocation",
    "href": "functions.html#collocation",
    "title": "Function approximation with global functions",
    "section": "Collocation",
    "text": "Collocation\n\n\nfrom lagrange import Lagrangebasis, Lagrangefunction\nxj = np.linspace(1, 2, 100)\nu = 10*(x-1)**2-1\nxp = np.array([1, 2])\nell = Lagrangebasis(xp)\nL = Lagrangefunction([u.subs(x, xi) for xi in xp], ell)\nplt.figure(figsize=(5, 3))\nplt.plot(xj, sp.lambdify(x, u)(xj), 'b')\nplt.plot(xj, sp.lambdify(x, L)(xj), 'r:')\nplt.plot(xp, [u.subs(x, xi) for xi in xp], 'bo')\nplt.legend(['$u(x)$', '$u_N(x), (x_0=1, x_1=2)$']);\n\n\n\n\n\n\n\n\n\n\n\nUse three points and the approximation is perfect because \\(u(x)\\) is a 2nd order polynomial\n\n\nxp = np.array([1, 1.5, 2]) \nell = Lagrangebasis(xp)\nL = Lagrangefunction([u.subs(x, xi) for xi in xp], ell)\nplt.figure(figsize=(5, 3))\nplt.plot(xj, sp.lambdify(x, u)(xj), 'b')\nplt.plot(xj, sp.lambdify(x, L)(xj), 'r:')\nplt.plot(xp, [u.subs(x, xi) for xi in xp], 'bo')\nplt.legend(['$u(x)$', '$(x_0, x_1, x_2) = (1, 1.5, 2)$']);"
  },
  {
    "objectID": "functions.html#high-order-interpolation-on-uniform-grids-is-a-bad-idea",
    "href": "functions.html#high-order-interpolation-on-uniform-grids-is-a-bad-idea",
    "title": "Function approximation with global functions",
    "section": "High order interpolation on uniform grids is a bad idea",
    "text": "High order interpolation on uniform grids is a bad idea\nLets consider a more difficult function\n\\[\nu(x) = \\frac{1}{1+25x^2}, \\quad x \\in [-1, 1],\n\\]\nand attempt to approximate it with Lagrange polynomials on a uniform grid.\nUse\n\\[\\begin{align}\nN &= 15 \\\\\nx_i &= -1 + \\frac{2i}{N}, \\quad i=0, 1, \\ldots, N \\\\\nu(x_i) &= u_N(x_i) = \\sum_{j=0}^N u(x_j) \\ell_j(x_i), \\quad i=0, 1, \\ldots, N\n\\end{align}\\]"
  },
  {
    "objectID": "functions.html#implementation",
    "href": "functions.html#implementation",
    "title": "Function approximation with global functions",
    "section": "Implementation",
    "text": "Implementation\n\n\nu = 1/(1+25*x**2)\nN = 15\nxj = np.linspace(-1, 1, N+1)\nuj = sp.lambdify(x, u)(xj)\nell = Lagrangebasis(xj)\nL = Lagrangefunction(uj, ell)\nyj = np.linspace(-1, 1, 1000)\nplt.figure(figsize=(6, 4))\nplt.plot(xj, uj, 'bo')\nplt.plot(yj, sp.lambdify(x, u)(yj))\nplt.plot(yj, sp.lambdify(x, L)(yj), 'r:')\nax = plt.gca()\nax.set_ylim(-0.5, 2.2);\n\n\n\n\n\n\n\n\n\n\nLarge over and undershoots in the approximation Lagrange polynomial!\nRunge’s phenomenon! Interpolation on uniform grids may lead to large over and undershoots.\nWhy?"
  },
  {
    "objectID": "functions.html#runges-phenomenon",
    "href": "functions.html#runges-phenomenon",
    "title": "Function approximation with global functions",
    "section": "Runge’s phenomenon",
    "text": "Runge’s phenomenon\nDefine the monic polynomial with roots in \\(\\{x_j\\}_{j=0}^N\\) as\n\\[\np_N(x) = \\prod_{j=0}^N (x-x_j)\n\\]\nIt can be shown that the error in the Lagrange polynomial \\(u_N(x)\\) using \\(\\{x_j\\}_{j=0}^N\\) is\n\\[\nu(x)-u_N(x)=\\frac{u^{(N)}(\\xi)}{(N+1)!}p_N(x)\n\\]\nfor some \\(\\xi \\in [a, b]\\).\nHence, large errors occur when \\(p_N(x)\\) is large or \\(u^{(N)}(\\xi) = \\frac{d^N u}{dx^N}(\\xi)\\) is large.\nLets look at \\(p_N(x)\\), which is independent of the function \\(u(x)\\)!"
  },
  {
    "objectID": "functions.html#plot-the-monic-polynomial-at-uniform-and-chebyshev-mesh",
    "href": "functions.html#plot-the-monic-polynomial-at-uniform-and-chebyshev-mesh",
    "title": "Function approximation with global functions",
    "section": "Plot the monic polynomial at uniform and Chebyshev mesh",
    "text": "Plot the monic polynomial at uniform and Chebyshev mesh\n\npp = np.poly(xp)\npc = np.poly(xc)\nxj = np.linspace(-1, 1, 1000)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\nax1.plot(xj, np.polyval(pp, xj)); ax2.plot(xj, np.polyval(pc, xj))\nax1.set_title('Uniform grid'); ax2.set_title('Chebyshev grid');\n\n\n\n\n\n\n\n\nUniform roots lead to large oscillations near the edges, whereas Chebyshev points leads to a polynomial with near uniform magnitude."
  },
  {
    "objectID": "functions.html#plot-the-monic-polynomial-small-pxprod_j0n-x-x_j-at-uniform-and-chebyshev-mesh",
    "href": "functions.html#plot-the-monic-polynomial-small-pxprod_j0n-x-x_j-at-uniform-and-chebyshev-mesh",
    "title": "Function approximation with global functions",
    "section": "Plot the monic polynomial \\(\\small p(x)=\\prod_{j=0}^N (x-x_j)\\) at uniform and Chebyshev mesh",
    "text": "Plot the monic polynomial \\(\\small p(x)=\\prod_{j=0}^N (x-x_j)\\) at uniform and Chebyshev mesh\n\npp = np.poly(xp); pc = np.poly(xc)\nxj = np.linspace(-1, 1, 1000)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\nax1.plot(xj, np.polyval(pp, xj)); ax2.plot(xj, np.polyval(pc, xj))\nax1.plot(xp, np.zeros(N+1), 'bo'); ax2.plot(xc, np.zeros(N+1), 'bo')\nax1.set_title('Uniform grid'); ax2.set_title('Chebyshev grid');\n\n\n\n\n\n\n\n\nUniform roots lead to large oscillations near the edges, whereas Chebyshev points leads to a polynomial with near uniform oscillations. Large oscillations in \\(p(x)\\) leads to large derivatives of \\(p(x)\\) near the edges and thus poor approximations. Remember the error term in Taylor expansions is \\(\\sim p^{(n)}(x)\\)."
  },
  {
    "objectID": "functions.html#compute-the-lagrange-polynomial-using-chebyshev-points",
    "href": "functions.html#compute-the-lagrange-polynomial-using-chebyshev-points",
    "title": "Function approximation with global functions",
    "section": "Compute the Lagrange polynomial using Chebyshev points",
    "text": "Compute the Lagrange polynomial using Chebyshev points\n\n\nxj = np.cos(np.arange(N+1)*np.pi/N)\nuj = sp.lambdify(x, u)(xj)\nell = Lagrangebasis(xj)\nL = Lagrangefunction(uj, ell)\nplt.figure(figsize=(6, 4))\nplt.plot(xj, uj, 'bo')\nplt.plot(yj, sp.lambdify(x, u)(yj))\nplt.gca().set_ylim(-0.5, 2.2);\nplt.plot(yj, sp.lambdify(x, L)(yj), 'r:');\n\n\n\n\n\n\n\n\n\n\nOnly small oscillations - interpolation is converging using more points!"
  },
  {
    "objectID": "functions.html#how-about-the-variational-method-and-uxfrac1125x2",
    "href": "functions.html#how-about-the-variational-method-and-uxfrac1125x2",
    "title": "Function approximation with global functions",
    "section": "How about the variational method and \\(u(x)=\\frac{1}{1+25x^2}\\)?",
    "text": "How about the variational method and \\(u(x)=\\frac{1}{1+25x^2}\\)?\nLets try Legendre polynomials. There is no need for mapping since the domain is \\([-1, 1]\\). And there are no mesh points! The Legendre coefficients are\n\\[\n\\hat{u}_j = \\frac{2j+1}{2}(u, P_j), \\quad j=0, 1, ...\n\\]\n\n\nu = 1/(1+25*x**2)\nuj = lambda u, j: (2*j+1) * inner(sp.legendre(j, x), u, (-1, 1))/2\nul = []\nfor n in range(40):\n  ul.append(uj(u, n).n())\n\nxj = np.linspace(-1, 1, 1000)\nuj = sp.lambdify(x, u)(xj)\nfig, (ax1, ax2) = plt.subplots(2, 1)\nax1.semilogy(ul, '+')\nax2.plot(xj, Legendre(ul[:17])(xj))\nax2.plot(yj, sp.lambdify(x, L)(yj), 'r:')\nax2.set_ylim(-0.5, 2.2);\nax2.legend(['Legendre', 'Lagrange on Chebyshev'])"
  },
  {
    "objectID": "functions.html#large-oscillations-in-p_nx-occur-when-the-interpolation-uses-a-uniform-mesh",
    "href": "functions.html#large-oscillations-in-p_nx-occur-when-the-interpolation-uses-a-uniform-mesh",
    "title": "Function approximation with global functions",
    "section": "Large oscillations in \\(p_N(x)\\) occur when the interpolation uses a uniform mesh",
    "text": "Large oscillations in \\(p_N(x)\\) occur when the interpolation uses a uniform mesh\nLets use both a uniform mesh and Chebyshev points:\n\\[\nx_j = \\cos(j \\pi / N)\n\\]\n\n\n\n\n\n\n\n\n\nConstruct \\(p_N(x)\\) using wither uniform mesh or Chebyshev nodes.."
  },
  {
    "objectID": "functions.html#high-order-interpolation-on-uniform-grids-is-bad",
    "href": "functions.html#high-order-interpolation-on-uniform-grids-is-bad",
    "title": "Function approximation with global functions",
    "section": "High order interpolation on uniform grids is bad",
    "text": "High order interpolation on uniform grids is bad\nLets consider a more difficult function\n\\[\nu(x) = \\frac{1}{1+25x^2}, \\quad x \\in [-1, 1],\n\\]\nand attempt to approximate it with Lagrange polynomials on a uniform grid.\nUse\n\\[\\begin{align}\nN &= 15 \\\\\nx_i &= -1 + \\frac{2i}{N}, \\quad i=0, 1, \\ldots, N \\\\\nu(x_i) &= u_N(x_i) = \\sum_{j=0}^N u(x_j) \\ell_j(x_i), \\quad i=0, 1, \\ldots, N\n\\end{align}\\]"
  },
  {
    "objectID": "functions.html#the-monic-p_nx-can-be-created-for-any-mesh",
    "href": "functions.html#the-monic-p_nx-can-be-created-for-any-mesh",
    "title": "Function approximation with global functions",
    "section": "The monic \\(p_N(x)\\) can be created for any mesh",
    "text": "The monic \\(p_N(x)\\) can be created for any mesh\nLets use both a uniform mesh and Chebyshev points:\n\\[\nx_j = -1 + \\frac{2j}{N} \\quad \\text{and} \\quad x_j = \\cos(j \\pi / N), \\quad j=0, 1, \\ldots, N\n\\]\n\n\n\n\n\n\n\n\n\nWe see that Chebyshev points are clustered near the edges\nLets construct \\(p_N(x)\\) using either the uniform mesh or Chebyshev nodes.."
  },
  {
    "objectID": "functions.html#plot-the-monic-polynomial-small-p_nxprod_j0n-x-x_j-at-uniform-and-chebyshev-mesh",
    "href": "functions.html#plot-the-monic-polynomial-small-p_nxprod_j0n-x-x_j-at-uniform-and-chebyshev-mesh",
    "title": "Function approximation with global functions",
    "section": "Plot the monic polynomial \\(\\small p_N(x)=\\prod_{j=0}^N (x-x_j)\\) at uniform and Chebyshev mesh",
    "text": "Plot the monic polynomial \\(\\small p_N(x)=\\prod_{j=0}^N (x-x_j)\\) at uniform and Chebyshev mesh\n\nxp = np.linspace(-1, 1, N+1); xc = np.cos(np.arange(N+1)*np.pi/N) \npp = np.poly(xp); pc = np.poly(xc)\nxj = np.linspace(-1, 1, 1000)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\nax1.plot(xj, np.polyval(pp, xj)); ax2.plot(xj, np.polyval(pc, xj))\nax1.plot(xp, np.zeros(N+1), 'bo'); ax2.plot(xc, np.zeros(N+1), 'bo')\nax1.set_title('Uniform grid'); ax2.set_title('Chebyshev grid');\n\n\n\n\n\n\n\n\n\nA uniform mesh leads to large oscillations near the edges, whereas Chebyshev points leads to a polynomial with near uniform oscillations.\nThe oscillations on the uniform mesh grow even larger when increasing \\(N\\)."
  },
  {
    "objectID": "functions.html#how-about-the-variational-method-and-small-uxfrac1125x2",
    "href": "functions.html#how-about-the-variational-method-and-small-uxfrac1125x2",
    "title": "Function approximation with global functions",
    "section": "How about the variational method and \\(\\small u(x)=\\frac{1}{1+25x^2}\\)?",
    "text": "How about the variational method and \\(\\small u(x)=\\frac{1}{1+25x^2}\\)?\nLets try Legendre polynomials. There is no need for mapping since the domain is \\([-1, 1]\\). And there are no mesh points! The Legendre coefficients are\n\\[\n\\hat{u}_j = \\frac{2j+1}{2}(u, P_j), \\quad j=0, 1, ...\n\\]\n\n\n\nu = 1/(1+25*x**2)\nuj = lambda u, j: (2*j+1) * inner(sp.legendre(j, x), u, (-1, 1))/2\nul = []\nfor n in range(40):\n  ul.append(uj(u, n).n())\n\nxj = np.linspace(-1, 1, 1000)\nuj = sp.lambdify(x, u)(xj)\nfig, (ax1, ax2) = plt.subplots(2, 1)\nax1.semilogy(ul, '+')\nax2.plot(xj, Legendre(ul[:17])(xj))\nax2.plot(yj, sp.lambdify(x, L)(yj), 'r:')\nax2.set_ylim(-0.5, 2.2);\nax2.legend(['Legendre', 'Lagrange on Chebyshev'])"
  },
  {
    "objectID": "functions.html#the-variational-method-and-small-uxfrac1125x2",
    "href": "functions.html#the-variational-method-and-small-uxfrac1125x2",
    "title": "Function approximation with global functions",
    "section": "The variational method and \\(\\small u(x)=\\frac{1}{1+25x^2}\\)",
    "text": "The variational method and \\(\\small u(x)=\\frac{1}{1+25x^2}\\)\nLets try Legendre polynomials. There is no need for mapping since the domain is \\([-1, 1]\\). And there are no mesh points! The Legendre coefficients are\n\\[\n\\hat{u}_j = \\frac{2j+1}{2}(u, P_j), \\quad j=0, 1, ...\n\\]\n\n\n\n# Use scipy to compute integral\nfrom numpy.polynomial import Legendre \nfrom scipy.integrate import quad \ndef innern(u, v):\n    uj = lambda xj: sp.lambdify(x, u)(xj)*v(xj)\n    return quad(uj, -1, 1)[0]\n\nu = 1/(1+25*x**2)\nuhat = lambda u, j: (2*j+1) * innern(u, Legendre.basis(j))/2\nul = [uhat(u, n) for n in range(40)]\n\nxj = np.linspace(-1, 1, 1000)\nuj = sp.lambdify(x, u)(xj)\nfig, (ax1, ax2) = plt.subplots(2, 1)\nax1.semilogy(abs(np.array(ul)), '+')\nax2.plot(xj, Legendre(ul[:17])(xj))\nax2.plot(yj, sp.lambdify(x, L)(yj), 'r:')\nax2.set_ylim(-0.5, 2.2);\nax1.legend(['Legendre coefficients magnitude'])\nax2.legend(['Legendre', 'Lagrange on Chebyshev'])"
  },
  {
    "objectID": "functions.html#boundary-issues",
    "href": "functions.html#boundary-issues",
    "title": "Function approximation with global functions",
    "section": "Boundary issues",
    "text": "Boundary issues\nLets consider a slightly different problem\n\\[\nu(x) = 10(x-1)^2 -1, \\quad x \\in [0, 1]\n\\]\nand attempt to find \\(u_N \\in V_N = \\text{span}\\{\\sin((j+1)\\pi x)\\}_{j=0}^N\\) with the Galerkin method.\nThe sines are orthogonal such that the mass matrix becomes diagonal\n\\[\n\\int_0^1 \\sin((j+1) \\pi x) \\sin((i + 1) \\pi x) dx = \\frac{1}{2} \\delta_{ij}.\n\\]\nHence we can easily get the coefficients with the Galerkin method\n\\[\n\\hat{u}_i = 2 \\left( u(x),  \\sin((i+1) \\pi x) \\right)_{L^2(0, 1)}.\n\\]"
  },
  {
    "objectID": "functions.html#implementation-1",
    "href": "functions.html#implementation-1",
    "title": "Function approximation with global functions",
    "section": "Implementation",
    "text": "Implementation\n\n\nu = 10*(x-1)**2-1\nuhat = lambda u, i: 2*inner(u, sp.sin((i+1)*sp.pi*x), (0, 1), (0, 1))\nul = []\nfor i in range(15):\n    ul.append(uhat(u, i).n())\n\nul = np.array(ul, dtype=float)\ndef uN(uh, xj):\n    N = len(xj)\n    uj = np.zeros(N)\n    for i, ui in enumerate(uh):\n        uj[:] += ui*np.sin((i+1)*np.pi*xj)\n    return uj\n\nxj = np.linspace(0, 1, 100)\nplt.figure(figsize=(6,4))\nplt.plot(xj, 10*(xj-1)**2-1, 'b')\nplt.plot(xj, uN(ul[:(3+1)], xj), 'g:')\nplt.plot(xj, uN(ul[:(12+1)], xj), 'r-')\nplt.legend(['$u(x)$', 'N=3', 'N=12']);\n\n\n\n\n\n\n\n\n\n\nWhat is wrong?\n\nAll basis functions are zero at the domain boundaries! And as such\n\\[\nu_N(0)=u_N(1)=0\n\\]"
  },
  {
    "objectID": "functions.html#solution-add-a-basis-function-with-nonzero-boundary-values",
    "href": "functions.html#solution-add-a-basis-function-with-nonzero-boundary-values",
    "title": "Function approximation with global functions",
    "section": "Solution, add a basis function with nonzero boundary values",
    "text": "Solution, add a basis function with nonzero boundary values\n\\[\nu_N(x) = B(x) + \\tilde{u}_N(x) = B(x) + \\sum_{j=0}^N \\hat{u}_j \\sin((j+1) \\pi x),\n\\]\nand use \\(\\tilde{u}_N\\in V_N\\) with homogeneous boundary values. \\(B(x)\\) can be\n\\[\nB(x) = u(1)x + u(0)(1-x)\n\\]\nsuch that\n\\[\nu_N(0)=B(0)=u(0) \\quad \\text{and} \\quad u_N(1)=B(1)=u(1)\n\\]\n\nThe variational problem becomes: Find \\(\\tilde{u}_N \\in V_N\\) such that\n\\[\n(B + \\tilde{u}_N - u, v) = 0 \\quad \\forall \\, v \\in V_N\n\\]"
  },
  {
    "objectID": "functions.html#sines-with-boundary-values",
    "href": "functions.html#sines-with-boundary-values",
    "title": "Function approximation with global functions",
    "section": "Sines with boundary values",
    "text": "Sines with boundary values\nFind \\(\\tilde{u}_N \\in V_N\\) such that \\[\n(B + \\tilde{u}_N - u, v) = 0 \\quad \\forall \\, v \\in V_N\n\\]\nInsert for \\(\\tilde{u}_N=\\sum_{j=0}^N \\hat{u}_j \\sin((j+1)\\pi x)\\) and \\(v=\\sin((i+1)\\pi x)\\)\n\\[\n\\left(\\sum_{j=0}^N \\hat{u}_j \\sin((j+1)\\pi x)+B-u, \\sin((i+1)\\pi x) \\right) = 0, \\quad i=0, 1, \\ldots, N\n\\]\nand using the orthogonality of the sines we get\n\\[\n\\hat{u}_i = 2 \\left(u-B, \\sin((i+1)\\pi x) \\right) \\quad i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#result-using-boundary-function-small-bx",
    "href": "functions.html#result-using-boundary-function-small-bx",
    "title": "Function approximation with global functions",
    "section": "Result using boundary function \\(\\small B(x)\\)",
    "text": "Result using boundary function \\(\\small B(x)\\)\n\n\nuc = []\nB = u.subs(x, 0)*(1-x) + u.subs(x, 1)*x\nfor i in range(15):\n    uc.append(uhat(u-B, i).n())\n\nuc = np.array(uc, dtype=float)\ndef uNc(uh, xj):\n    N = len(xj)\n    uj = u.subs(x, 0)*(1-xj) + u.subs(x, 1)*xj\n    for i, ui in enumerate(uh):\n        uj[:] += ui*np.sin((i+1)*np.pi*xj)\n    return uj\n\nplt.figure(figsize=(6,4))\nplt.plot(xj, 10*(xj-1)**2-1, 'b')\nplt.plot(xj, uNc(uc[:(3+1)], xj), 'g:')\nplt.plot(xj, uNc(uc[:(12+1)], xj), 'r--')\nplt.legend(['$u(x)$', 'N=3', 'N=12']);"
  },
  {
    "objectID": "functions.html#how-about-the-legendre-method",
    "href": "functions.html#how-about-the-legendre-method",
    "title": "Function approximation with global functions",
    "section": "How about the Legendre method?",
    "text": "How about the Legendre method?\nLegendre polynomials \\(P_j(x)\\) are such that the boundary values are nonzero\n\\[\nP_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1)=1\n\\]\nHence there is no need for boundary functions or adjustments\n\nuhat = lambda u, j: (2*j+1) * inner(u, sp.legendre(j, x), (0, 1))/2\nuL = [uhat(u, i) for i in range(6)]\nplt.figure(figsize=(8, 3.5))\nplt.plot(xj, Legendre(uL, domain=(0, 1))(xj), 'b')\nplt.plot(xj, 10*(xj-1)**2-1, 'r:'); plt.legend([r'$u_N(x)$', r'$u(x)$'])\nprint(uL)\n\n[7/3, -5, 5/3, 0, 0, 0]"
  },
  {
    "objectID": "functions.html#summary-of-variational-methods",
    "href": "functions.html#summary-of-variational-methods",
    "title": "Function approximation with global functions",
    "section": "Summary of variational methods",
    "text": "Summary of variational methods\nWe want to approximate \\(u(x)\\) with \\(u_N(x) \\in V_N\\)\n\\[\nu_N(x) \\approx u(x)\n\\]\nusing the inner product\n\\[\n(f, g) = \\int_a^b f(x)g(x)dx\n\\]\n\n\n\nLeast squares method\nFind \\(u_N \\in V_N\\) such that\n\\[\n\\frac{\\partial E}{\\partial \\hat{u}_j} = 0, \\quad j=0,1,\\ldots, N\n\\]\nwhere \\(E=(u-u_N, u-u_N)\\).\nMinimizing the \\(L^2\\) error norm.\n\nGalerkin method\nFind \\(u_N \\in V_N\\) such that\n\\[\n(u-u_N, v) = 0, \\quad \\forall \\, v \\in V_N\n\\]\nOr equivalently\n\\[\n(u-u_N, \\psi_j) = 0, \\quad j=0,1,\\ldots, N\n\\]\nError orthogonal to the test functions"
  },
  {
    "objectID": "functions.html#if-we-now-set-smallfracpartial-e-partial-hatu_j0-then",
    "href": "functions.html#if-we-now-set-smallfracpartial-e-partial-hatu_j0-then",
    "title": "Function approximation with global functions",
    "section": "If we now set \\(\\small\\frac{\\partial E}{ \\partial \\hat{u}_j}=0\\), then",
    "text": "If we now set \\(\\small\\frac{\\partial E}{ \\partial \\hat{u}_j}=0\\), then\n\\[\n\\frac{\\partial E}{ \\partial \\hat{u}_j} = -\\int 2\\left( u-\\sum_{k=0}^N \\hat{u}_k \\psi_k \\right) \\psi_j dx = 0\n\\]\n\\[\n\\longrightarrow \\int u \\psi_j dx = \\sum_{k=0}^N\\int \\psi_k \\psi_j dx \\hat{u}_k, \\quad j=0,1, \\ldots, N\n\\]\ngives us \\(N+1\\) linear equations for the \\(N+1\\) unknown \\(\\{\\hat{u}_k\\}_{k=0}^N\\)\n\n\nBack to the example: \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1, 2]\\)\nFind \\(u_N \\in \\mathbb{P}_1\\) (meaning find \\(\\hat{u}_0\\) and \\(\\hat{u}_1\\)) such that\n\\[\n\\frac{\\partial E}{\\partial \\hat{u}_0}=0 \\text{ and } \\frac{\\partial E}{\\partial \\hat{u}_1}=0\n\\]\nTwo equations for two unknowns."
  },
  {
    "objectID": "functions.html#affine-map",
    "href": "functions.html#affine-map",
    "title": "Function approximation with global functions",
    "section": "Affine map",
    "text": "Affine map\n\n\nEquations are defined in the real domain \\([a, b]\\)\nEquations are solved in the computational domain \\([A, B]\\)"
  },
  {
    "objectID": "functions.html#an-example-of-spectral-exponential-convergence",
    "href": "functions.html#an-example-of-spectral-exponential-convergence",
    "title": "Function approximation with global functions",
    "section": "An example of spectral (exponential) convergence",
    "text": "An example of spectral (exponential) convergence\nLets appriximate the smooth function\n\\[\nu = e^{\\cos(x)}, \\quad x \\in [-1, 1]\n\\]\nand monitor the \\(L^2\\) error as we increase \\(N\\).\n\n\ndef L2_error(uh, ue):\n    uej = sp.lambdify(x, ue)\n    err = []\n    for n in range(0, 40, 2):\n        uf = lambda xj: (Legendre(uh[:(n+1)])(xj)\n                         -uej(xj))**2\n        err.append(np.sqrt(quad(uf, -1, 1)[0]))\n    return err\nu = sp.exp(sp.cos(x))\n# Compute Legendre coefficients\nuh = [uhat(u, n) for n in range(0, 40)]\nplt.figure(figsize=(6, 3))\nplt.loglog(np.arange(0, 40, 2), L2_error(uh, u), 'ko')\nplt.title('$L^2 error$');plt.xlabel('N');\n\n\n\n\n\n\n\n\n\n\n\nExponentially fast convergence rate!\nError does not go down further due to machine precision."
  },
  {
    "objectID": "functions.html#solution-add-a-boundary-function-with-nonzero-boundary-values",
    "href": "functions.html#solution-add-a-boundary-function-with-nonzero-boundary-values",
    "title": "Function approximation with global functions",
    "section": "Solution, add a boundary function with nonzero boundary values",
    "text": "Solution, add a boundary function with nonzero boundary values\n\\[\nu_N(x) = B(x) + \\tilde{u}_N(x) = B(x) + \\sum_{j=0}^N \\hat{u}_j \\sin((j+1) \\pi x),\n\\]\nand use \\(\\tilde{u}_N\\in V_N\\) with homogeneous boundary values. \\(B(x)\\) can be\n\\[\nB(x) = u(1)x + u(0)(1-x)\n\\]\nsuch that\n\\[\nu_N(0)=B(0)=u(0) \\quad \\text{and} \\quad u_N(1)=B(1)=u(1)\n\\]\n\nThe variational problem becomes: Find \\(\\tilde{u}_N \\in V_N\\) such that\n\\[\n(B + \\tilde{u}_N - u, v) = 0 \\quad \\forall \\, v \\in V_N\n\\]"
  },
  {
    "objectID": "functions.html#linear-algebra-problem-for-the-galerkin-method",
    "href": "functions.html#linear-algebra-problem-for-the-galerkin-method",
    "title": "Function approximation with global functions",
    "section": "Linear algebra problem for the Galerkin method",
    "text": "Linear algebra problem for the Galerkin method\nWe have the \\(N+1\\) equations\n\\[\n(e, \\psi_i) = 0, \\quad  i = 0, 1, \\ldots, N.\n\\]\nInsert for \\(e=u-u_N=u-\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) to get\n\\[\n(u - \\sum_{j=0}^N \\hat{u}_j \\psi_j, \\psi_i) = 0, \\quad i =0, 1, \\ldots, N\n\\]\nand thus the linear algebra problem \\(A \\boldsymbol{x} = \\boldsymbol{b}\\):\n\\[\n\\sum_{j=0}^N \\underbrace{(\\psi_j, \\psi_i)}_{a_{ij}} \\, \\underbrace{\\hat{u}_j}_{x_j} = \\underbrace{(u, \\psi_i)}_{b_i}, \\quad i=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#legendre-polynomials-are-much-better-basis-functions",
    "href": "functions.html#legendre-polynomials-are-much-better-basis-functions",
    "title": "Function approximation with global functions",
    "section": "Legendre polynomials are much better basis functions",
    "text": "Legendre polynomials are much better basis functions\nLegendre polynomials are defined on the domain \\(\\Omega = [-1, 1]\\) as the recursion\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\nThe first 5 Legendre polynomials are plotted on the right"
  },
  {
    "objectID": "functions.html#if-the-physical-domain-a-b-is-different-from-the-reference-domain--1-1-then-we-need-to-map",
    "href": "functions.html#if-the-physical-domain-a-b-is-different-from-the-reference-domain--1-1-then-we-need-to-map",
    "title": "Function approximation with global functions",
    "section": "If the physical domain \\([a, b]\\) is different from the reference domain \\([-1, 1]\\), then we need to map",
    "text": "If the physical domain \\([a, b]\\) is different from the reference domain \\([-1, 1]\\), then we need to map\nMany well-known basis functions work only on a given reference domain\n\n\n\nSines and cosines \\([0, \\pi]\\)\nLegendre polynomials \\([-1, 1]\\)\n\n\n\nBernstein polynomials \\([0, 1]\\)\nChebyshev polynomials \\([-1, 1]\\)\n\n\n\n\nLet \\(X\\) be the coordinate in the computational (reference) domain \\([A, B]\\) and \\(x\\) be the coordinate in the true physical domain \\([a, b]\\). A linear (affine) mapping, from \\(X\\) to \\(x\\) (and back) is then\n\\[\nX \\in [A, B] \\quad \\text{and} \\quad x \\in [a, b]\n\\]\n\\[\nx = a + \\frac{b-a}{B-A}(X-A) \\quad \\text{and} \\quad  X = A + \\frac{B-A}{b-a}(x-a)\n\\]"
  },
  {
    "objectID": "functions.html#important-topics-of-todays-lecture",
    "href": "functions.html#important-topics-of-todays-lecture",
    "title": "Function approximation with global functions",
    "section": "Important topics of todays lecture",
    "text": "Important topics of todays lecture\n\nApproximation of smooth functions \\(u(x)\\) using global basis functions\nVariational methods\n\nThe least squares method\nThe Galerkin method\n\nThe collocation method\nRunge’s phenomenon\nLegendre polynomials\nSpectral convergence"
  },
  {
    "objectID": "functions.html#example-ux-10x-12-1-for-xin-12",
    "href": "functions.html#example-ux-10x-12-1-for-xin-12",
    "title": "Function approximation with global functions",
    "section": "Example: \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1,2]\\)",
    "text": "Example: \\(u(x) = 10(x-1)^2-1\\) for \\(x\\in [1,2]\\)\n\n\n\n\n\n\n\n\n\nLet \\(V_N = \\{1, x\\}\\) be the space of all straight lines.\nWhat is the best approximation \\(u_N \\in V_N\\) to the function \\(u(x)\\)?\n\nHow do we decide what is best? All we know is that\n\\[\nu_N(x) = \\hat{u}_0 + \\hat{u}_1 x.\n\\]\nHow to find the best \\(\\{\\hat{u}_0, \\hat{u}_1\\}\\)?"
  },
  {
    "objectID": "functions.html#we-get-the-linear-system-with-inner-products-over-the-reference-domain",
    "href": "functions.html#we-get-the-linear-system-with-inner-products-over-the-reference-domain",
    "title": "Function approximation with global functions",
    "section": "We get the linear system with inner products over the reference domain",
    "text": "We get the linear system with inner products over the reference domain\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 P_k(X)  P_j(X) \\, dX \\, \\hat{u}_k =  \\int_{-1}^1 u(x(X)) P_j(X) \\, dX, \\quad j=0, 1, \\ldots, N\n\\]\nwhich can be written as\n\\[\n\\sum_{k=0}^N  \\underbrace{(P_k(X), P_j(X))_{L^2(-1, 1)}}_{\\frac{2}{2j+1} \\delta_{kj}} \\, \\hat{u}_k = (u(x(X)), P_j(X))_{L^2(-1, 1)}\n\\]\nsuch that\n\\[\n\\hat{u}_j = \\frac{2j+1}{2} \\left( u(x(X)), P_j(X) \\right)_{L^2(-1, 1)} \\quad j=0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "functions.html#summary",
    "href": "functions.html#summary",
    "title": "Function approximation with global functions",
    "section": "Summary",
    "text": "Summary\n\n\nWe have been approximating \\(u(x)\\) by \\(u_N \\in V_N = \\text{span}\\{\\psi_j\\}_{j=0}^N\\), meaning that we have found \\(\\small u_N(x) = \\sum_{j=0}^N \\hat{u_j} \\psi_j(x)\\)\nWe have found the unknown expansion coefficients \\(\\{\\hat{u}_j\\}_{j=0}^N\\) using\n\nVariational methods\n\nThe least squares method\nThe Galerkin method\n\nCollocation - an interpolation method\n\nInterpolation on uniform grids is bad for large \\(N\\) due to Runge’s phenomenon\nBasis functions are often defined on a reference domain, and problems thus need mapping.\nSome basis functions, like sines, have zero boundary values and need an additional boundary function for convergence.\nLegendre polynomials lead to spectral convergence (for smooth functions \\(u(x)\\))."
  },
  {
    "objectID": "functions.html#the-galerkin-method-is-a-more-popular-variational-method-for-solving-differential-equations",
    "href": "functions.html#the-galerkin-method-is-a-more-popular-variational-method-for-solving-differential-equations",
    "title": "Function approximation with global functions",
    "section": "The Galerkin method is a more popular variational method for solving differential equations",
    "text": "The Galerkin method is a more popular variational method for solving differential equations\nHere the error is required to be orthogonal to the basis functions (for the \\(L^2\\) inner product)\n\\[\n(e, \\psi_j) = 0, \\quad  j = 0, 1, \\ldots, N.\n\\]\nAgain \\(N+1\\) equations for \\(N+1\\) unknowns.\n\nNot as much work as the least squares method, and even easier to implement in Sympy:\n\neq1 = sp.Eq(inner(err, 1, domain=(1, 2)), 0)\neq2 = sp.Eq(inner(err, x, domain=(1, 2)), 0)\ndisplay(eq1)\ndisplay(eq2) \n\n\\(\\displaystyle - u_{0} - \\frac{3 u_{1}}{2} + \\frac{7}{3} = 0\\)\n\n\n\\(\\displaystyle - \\frac{3 u_{0}}{2} - \\frac{7 u_{1}}{3} + \\frac{13}{3} = 0\\)\n\n\n\nsp.solve((eq1, eq2), (u0, u1))\n\n{u0: -38/3, u1: 10}"
  },
  {
    "objectID": "chebyshev.html#chebyshev-polynomials",
    "href": "chebyshev.html#chebyshev-polynomials",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Chebyshev polynomials",
    "text": "Chebyshev polynomials\nThe Chebyshev polynomials are an often preferred alternative to Legendre:\n\\[\nT_k(x) = \\cos(k \\cos^{-1}(x)), \\quad k = 0, 1, \\ldots \\quad x \\in [-1, 1]\n\\]\n\n\nAs recursion: \\[ \\small\n\\begin{align*}\nT_0(x) &= 1, \\\\\nT_1(x) &= x, \\\\\nT_2(x) &= 2x^2-1, \\\\\n&\\vdots \\\\\nT_{j+1}(x) &= 2xT_{j}(x) - T_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(T_N(x)\\) all extrema points (max and mins) and all roots are, respectively\n\\[\n\\begin{align}\nx_j &= \\cos\\left(\\frac{j \\pi}{N}\\right),  &j=0, 1, \\ldots, N \\\\\nx_j &= \\cos\\left( \\frac{(2j+1)\\pi}{2N}\\right),  &j=0, 1, \\ldots, N-1\n\\end{align}\n\\]"
  },
  {
    "objectID": "chebyshev.html#chebyshev-polynomials-as-a-basis",
    "href": "chebyshev.html#chebyshev-polynomials-as-a-basis",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Chebyshev polynomials as a basis",
    "text": "Chebyshev polynomials as a basis\nThe Chebyshev polynomials \\(\\{T_j\\}_{j=0}^N\\) also form a basis for \\(\\mathbb{P}_N\\). However, the Chebyshev polynomials are not orthogonal in the \\(L^2(-1, 1)\\) space!\n\\[\n(T_i, T_j)_{L^2(\\Omega)} \\ne \\|T_i\\|^2 \\delta_{ij}\n\\]\n\nThe Chebyshev polynomials are, on the other hand, orthogonal in a special weighted inner product space.\nWe define the weighted \\(L^2_{\\omega}(\\Omega)\\) inner product as\n\\[\n(f, g)_{L^2_{w}(\\Omega)} = \\int_{\\Omega} f(x)g(x)\\omega(x)d\\Omega,\n\\]\nwhich is more commonly written as \\((f, g)_{\\omega}\\). The weight function \\(\\omega(x)\\) is positive (almost everywhere) and a weighted norm is\n\\[\n\\|u\\|_{\\omega} = \\sqrt{(u, u)_{\\omega}}\n\\]"
  },
  {
    "objectID": "chebyshev.html#function-approximations-with-chebyshev-polynomials",
    "href": "chebyshev.html#function-approximations-with-chebyshev-polynomials",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Function approximations with Chebyshev polynomials",
    "text": "Function approximations with Chebyshev polynomials\nThe Chebyshev polynomials are orthogonal if \\(\\omega(x) = (1-x^2)^{-1/2}\\) and \\(x\\in [-1,1]\\). We get\n\\[\n(T_i, T_j)_{\\omega} = \\|T_i\\|_{\\omega}^2 \\delta_{ij}\n\\]\nwhere \\(\\|T_i\\|_{\\omega}^2 = \\frac{c_i \\pi}{2}\\) and \\(c_i=1\\) for \\(i&gt;0\\) and \\(c_0=2\\).\nThe Galerkin method for approximating a smooth function \\(u(x)\\) is now:\nFind \\(u_N \\in \\mathbb{P}_N\\) such that\n\\[\n(u-u_N, v)_{\\omega} = 0, \\quad \\forall \\, v \\in \\mathbb{P}_N\n\\]\n\nWe get the linear algebra problem by inserting for \\(v=T_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j T_j\\)\n\\[\n\\sum_{j=0}^N(T_j, T_i)_{\\omega} \\hat{u}_j = (u, T_i)_{\\omega} \\rightarrow \\hat{u}_i = \\frac{(u, T_i)_{\\omega}}{\\|T_i\\|_{\\omega}^2}, \\quad \\quad i=0,1,\\ldots, N\n\\]"
  },
  {
    "objectID": "chebyshev.html#mapping-to-reference-domain",
    "href": "chebyshev.html#mapping-to-reference-domain",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Mapping to reference domain",
    "text": "Mapping to reference domain\nWith a physical domain \\(x\\in [a, b]\\) and a reference \\(X\\in [-1, 1]\\), we now have the basis function\n\\[\n\\psi_i(x) = T_i(X(x)), \\quad i=0,1,\\ldots, N\n\\]\nand the inner product to compute is\n\\[\n(u(x)-u_N(x), \\psi_i(x))_{\\omega} = \\int_{a}^b (u(x)-u_N(x)) \\psi_i(x) \\omega(x) dx = 0, \\quad i=0, 1, \\ldots, N\n\\]\n\nAs for Legendre we use a change of variables \\(x\\rightarrow X\\), but there is also a weight function that requires mapping\n\\[\n\\omega(x) = \\tilde{\\omega}(X) = \\frac{1}{\\sqrt{1-X^2}}\n\\]"
  },
  {
    "objectID": "chebyshev.html#mapping",
    "href": "chebyshev.html#mapping",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Mapping",
    "text": "Mapping\n\\[\n\\sum_{j=0}^N \\int_{-1}^1 T_j(X)  T_i(x) \\omega(X) \\, \\frac{dx}{dX} \\, dX \\, \\hat{u}_j =  \\int_{-1}^1 u(x(X)) T_i(X) \\omega(X) \\, \\frac{dx}{dX} \\, dX\n\\]"
  },
  {
    "objectID": "chebyshev.html#the-mapped-problem-becomes",
    "href": "chebyshev.html#the-mapped-problem-becomes",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The mapped problem becomes",
    "text": "The mapped problem becomes\nfor all \\(i=0,1,\\ldots, N\\):\n\\[\n\\sum_{j=0}^N \\overbrace{\\int_{-1}^1 T_j(X)  T_i(X) \\tilde{\\omega}(X) \\, \\cancel{\\frac{dx}{dX}} \\, dX}^{\\|T_i\\|^2\\delta_{ij}} \\, \\hat{u}_j =  \\overbrace{\\int_{-1}^1 u(x(X)) T_i(X) \\tilde{\\omega}(X) \\, \\cancel{\\frac{dx}{dX}} \\, dX}^{(u(x(X)), T_i)_{\\omega}}\n\\]\nand finally (using \\(\\|T_i\\|_{\\omega}^2=\\frac{c_i \\pi}{2}\\))\n\\[\n\\hat{u}_i = \\frac{2}{c_i \\pi}\\left(u(x(X)), T_i\\right)_{L^2_{\\omega}(-1,1)}, \\quad i=0, 1, \\ldots, N\n\\]\nThe procedure is exactly like for Legendre polynomials, but with a weighted inner product using \\(L^2_{\\omega}(-1,1)\\) instead of \\(L^2(-1,1)\\)."
  },
  {
    "objectID": "chebyshev.html#the-weighted-inner-product-requires-some-extra-attention",
    "href": "chebyshev.html#the-weighted-inner-product-requires-some-extra-attention",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The weighted inner product requires some extra attention",
    "text": "The weighted inner product requires some extra attention\n\\[\n(f, T_i)_{\\omega} = \\int_{-1}^1 \\frac{f(x(X))T_i(X)}{\\sqrt{1-X^2}}dX\n\\]\n\nSince \\(T_i(X) = \\cos(i \\cos^{-1}(X))\\) a change of variables \\(X=\\cos \\theta\\) leads to \\(T_i(\\cos \\theta) = \\cos(i \\theta)\\). Using the change of variables for the integral:\n\\[\n(f, T_i)_{\\omega} = \\int_{\\pi}^{0} \\frac{f(x(\\cos \\theta))T_i(\\cos \\theta)}{\\sqrt{1-\\cos^2 \\theta}} \\frac{d \\cos \\theta}{d \\theta} d\\theta.\n\\]\n\n\nInsert for \\(1-\\cos^2\\theta = \\sin^2\\theta\\) and swap both the direction of the integration and the sign:\n\\[\n(f, T_i)_{\\omega}= \\int_{0}^{\\pi} f(x(\\cos \\theta))T_i(\\cos \\theta) d\\theta.\n\\]"
  },
  {
    "objectID": "chebyshev.html#weighted-inner-product-continued",
    "href": "chebyshev.html#weighted-inner-product-continued",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Weighted inner product continued",
    "text": "Weighted inner product continued\n\\[\n(f, T_i)_{\\omega}= \\int_{0}^{\\pi} f(x(\\cos \\theta))T_i(\\cos \\theta) d\\theta.\n\\]\nUsing \\(T_i(\\cos \\theta) = \\cos (i \\theta)\\) we get the much simpler integral\n\\[\n(f, T_i)_{\\omega} = \\int_{0}^{\\pi} f(x(\\cos \\theta))\\cos (i \\theta) d\\theta.\n\\]\nUsing this integral, we get the Chebyshev coefficients\n\\[\n\\hat{u}_i = \\frac{2}{c_i \\pi}\\int_{0}^{\\pi} u(x(\\cos \\theta)) \\cos(i \\theta) d\\theta, \\quad  i=0, 1, \\ldots, N\n\\]\nLets try this with an example."
  },
  {
    "objectID": "chebyshev.html#implementation",
    "href": "chebyshev.html#implementation",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Implementation",
    "text": "Implementation\n\nimport numpy as np\nimport sympy as sp\n\nx = sp.Symbol('x', real=True)\nk = sp.Symbol('k', integer=True, positive=True)\n\nTk = lambda k, x: sp.cos(k * sp.acos(x))\ncj = lambda j: 2 if j == 0 else 1\n\ndef innerw(u, v, domain, ref_domain=(-1, 1)):\n    A, B = ref_domain\n    a, b = domain\n    # map u(x(X)) to use reference coordinate X.\n    # Note that x is here ref coord.\n    us = u.subs(x, a + (b-a)*(x-A)/(B-A))\n    # Change variables x=cos(theta)\n    us = sp.simplify(us.subs(x, sp.cos(x)), inverse=True)\n    vs = sp.simplify(v.subs(x, sp.cos(x)), inverse=True)\n    return sp.integrate(us*vs, (x, 0, sp.pi))"
  },
  {
    "objectID": "chebyshev.html#implementation-of-the-weighted-inner-product",
    "href": "chebyshev.html#implementation-of-the-weighted-inner-product",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Implementation of the weighted inner product",
    "text": "Implementation of the weighted inner product\n\nx = sp.Symbol('x', real=True)\nk = sp.Symbol('k', integer=True, positive=True)\n\nTk = lambda k, x: sp.cos(k * sp.acos(x))\ncj = lambda j: 2 if j == 0 else 1\n\ndef innerw(u, v, domain, ref_domain=(-1, 1)):\n    A, B = ref_domain\n    a, b = domain\n    # map u(x(X)) to use reference coordinate X.\n    # Note that small x here in the end will be ref coord.\n    us = u.subs(x, a + (b-a)*(x-A)/(B-A))\n    # Change variables x=cos(theta)\n    us = sp.simplify(us.subs(x, sp.cos(x)), inverse=True) # X=cos(theta)\n    vs = sp.simplify(v.subs(x, sp.cos(x)), inverse=True)  # X=cos(theta)\n    return sp.integrate(us*vs, (x, 0, sp.pi))\n\n\n\n\n\n\n\nNote\n\n\nWe use the Sympy function simplify with inverse=True, which is required for Sympy to use that \\(\\cos^{-1}(\\cos x) = x\\), which is not necessarily true."
  },
  {
    "objectID": "chebyshev.html#try-with-small-10x-12-1-xin-12",
    "href": "chebyshev.html#try-with-small-10x-12-1-xin-12",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Try with \\(\\small 10(x-1)^2-1, x\\in [1,2]\\)",
    "text": "Try with \\(\\small 10(x-1)^2-1, x\\in [1,2]\\)\n\nfrom numpy.polynomial import Chebyshev\nu = 10*(x-1)**2-1\nuhat = lambda u, j: 2 / (cj(j) * sp.pi) * innerw(u, Tk(j, x), (1, 2))\n\nplt.figure(figsize=(8, 4))\nxj = np.linspace(1, 2, 100)\nuhj = [uhat(u, j) for j in range(6)]\nC2 = Chebyshev(uhj[:2], domain=(1, 2))\nC3 = Chebyshev(uhj[:3], domain=(1, 2))\nplt.plot(xj, sp.lambdify(x, u)(xj), 'b')\nplt.plot(xj, C2(xj), 'r:')\nplt.plot(xj, C3(xj), 'g--')\nplt.legend(['$10(x-1)^2-1$', f'{C2}', f'{C3}']);"
  },
  {
    "objectID": "chebyshev.html#try-with-small-ux10x-12-1-xin-12",
    "href": "chebyshev.html#try-with-small-ux10x-12-1-xin-12",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Try with \\(\\small u(x)=10(x-1)^2-1, x\\in [1,2]\\)",
    "text": "Try with \\(\\small u(x)=10(x-1)^2-1, x\\in [1,2]\\)\n\nfrom numpy.polynomial import Chebyshev\nu = 10*(x-1)**2-1\nuhat = lambda u, j: 2 / (cj(j) * sp.pi) * innerw(u, Tk(j, x), (1, 2))\nplt.figure(figsize=(8, 3.5))\nxj = np.linspace(1, 2, 100)\nuhj = [uhat(u, j) for j in range(6)]\nC2, C3 = Chebyshev(uhj[:2], domain=(1, 2)), Chebyshev(uhj[:3], domain=(1, 2))\nplt.plot(xj, sp.lambdify(x, u)(xj), 'b')\nplt.plot(xj, C2(xj), 'r:'); plt.plot(xj, C3(xj), 'g--')\nplt.plot(xj, 7/3+5*(-1+2*(xj-1)), 'm--')\nplt.legend(['$10(x-1)^2-1$', f'{C2}', f'{C3}', 'Legendre: 7/3+5$P_1(x)$']);\n\n\n\n\n\n\n\n\nDifferent from Legendre for the linear profile. But not by much. Why is it different?"
  },
  {
    "objectID": "chebyshev.html#try-more-difficult-function-with-numerical-integration",
    "href": "chebyshev.html#try-more-difficult-function-with-numerical-integration",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Try more difficult function with numerical integration",
    "text": "Try more difficult function with numerical integration\n\\[\nu(x) = e^{\\cos x}, \\quad x \\in [-1,1]\n\\]\nUse numerical integration and change of variables\n\nfrom scipy.integrate import quad\ndef innerwn(u, v, domain, ref_domain=(-1, 1)):\n    A, B = ref_domain\n    a, b = domain\n    us = u.subs(x, a + (b-a)*(x-A)/(B-A)) # u(x(X))\n    us = sp.simplify(us.subs(x, sp.cos(x)), inverse=True) # X=cos(theta)\n    vs = sp.simplify(v.subs(x, sp.cos(x)), inverse=True)  # X=cos(theta)\n    return quad(sp.lambdify(x, us*vs), 0, np.pi)[0]\nu = sp.exp(sp.cos(x))\n#uhat = lambda u, j: 2 / (cj(j) * sp.pi) * innerw(u, Tk(j, x), (-1, 1)) # slow\nuhatn = lambda u, j: 2 / (cj(j) * np.pi) * innerwn(u, Tk(j, x), (-1, 1))\n\nRemember, we are computing for \\(i=0, 1, \\ldots, N\\)\n\\[\n\\hat{u}_i = \\frac{2}{c_i \\pi} \\int_{-1}^1 u(x(X)) T_i(X) \\tilde{\\omega}(X) dX = \\frac{2}{c_i \\pi}\\int_{0}^{\\pi} u(x(\\cos \\theta)) \\cos(i \\theta) d\\theta\n\\]"
  },
  {
    "objectID": "chebyshev.html#compare-with-legendre",
    "href": "chebyshev.html#compare-with-legendre",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Compare with Legendre",
    "text": "Compare with Legendre\n\n\n# Compute Chebyshev coefficients:\nN = 25\nuc = [uhatn(u, n) for n in range(N)]\n\n# Compute Legendre coefficients:\nfrom numpy.polynomial import Legendre\ndef innern(u, v):\n    uj = lambda xj: sp.lambdify(x, u)(xj)*v(xj)\n    return quad(uj, -1, 1)[0]\nuhatj = lambda u, j: (2*j+1) * innern(u, Legendre.basis(j))/2\nul = [uhatj(u, n) for n in range(N)]\n\nplt.figure(figsize=(5, 3))\nplt.semilogy(np.arange(0, N, 2), uc[::2], '+',\n             np.arange(0, N, 2), ul[::2], 'ko', fillstyle='none')\nplt.title('Coefficients $\\hat{u}_j$')\nplt.legend(['Chebyshev', 'Legendre']);\n\n\n\n\n\n\n\n\n\n\nVery similar convergence. Chebyshev coefficients are slightly smaller than Legendre. How about the \\(L^2\\) error?"
  },
  {
    "objectID": "chebyshev.html#l2-error",
    "href": "chebyshev.html#l2-error",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "\\(L^2\\) error",
    "text": "\\(L^2\\) error\n\ndef L2_error(uh, ue, space=Legendre):\n    xj = np.linspace(-1, 1, 1000)\n    uej = sp.lambdify(x, ue)(xj)\n    err = []\n    for n in range(0, len(uh), 2):\n        uj = space(uh[:(n+1)])(xj).astype(float)\n        err.append(np.sqrt(np.trapz((uj-uej)**2, dx=xj[1]-xj[0])))\n    return err\n\nerrc = L2_error(uc, u, Chebyshev)\nerrl = L2_error(ul, u, Legendre)\n\nplt.figure(figsize=(5, 3))\nplt.loglog(np.arange(0, N, 2), errc, '+',\n           np.arange(0, N, 2), errl, 'ko', fillstyle='none')\nplt.legend(['Chebyshev', 'Legendre']);"
  },
  {
    "objectID": "chebyshev.html#the-least-squares-method",
    "href": "chebyshev.html#the-least-squares-method",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The least squares method",
    "text": "The least squares method\nThe least squares method is also similar, using \\(E_{\\omega}=\\|e\\|^2_{\\omega}\\):\nFind \\(u_N \\in \\mathbb{P}_N\\) such that\n\\[\n\\frac{\\partial E_{\\omega}}{\\partial \\hat{u}_j}=0, \\quad j=0,1, \\ldots, N\n\\]\n\nWe get the linear algebra problem using\n\\[\n\\frac{\\partial E_{\\omega}}{\\partial \\hat{u}_j} = \\frac{\\partial}{\\partial \\hat{u}_j} \\int_{-1}^1 e^2 \\omega dx = \\int_{-1}^1 2e \\frac{\\partial e}{\\partial \\hat{u}_j} \\omega dx\n\\]\nInsert for \\(e(x)=u(x)-u_N(x) = u(x)-\\sum_{k=0}^N \\hat{u}_k T_k\\) and you get exactly the same linear equations as for the Galerkin method."
  },
  {
    "objectID": "chebyshev.html#l2-error---esqrtint_-11-e2-dx-not-weighted",
    "href": "chebyshev.html#l2-error---esqrtint_-11-e2-dx-not-weighted",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "\\(L^2\\) error - \\(\\|e\\|=\\sqrt{\\int_{-1}^1 e^2 dx}\\) (not weighted)",
    "text": "\\(L^2\\) error - \\(\\|e\\|=\\sqrt{\\int_{-1}^1 e^2 dx}\\) (not weighted)\n\ndef L2_error(uh, ue, space=Legendre):\n    xj = np.linspace(-1, 1, 1000)\n    uej = sp.lambdify(x, ue)(xj)\n    err = []\n    for n in range(0, len(uh), 2):\n        uj = space(uh[:(n+1)])(xj).astype(float)\n        err.append(np.sqrt(np.trapz((uj-uej)**2, dx=xj[1]-xj[0])))\n    return err\n\nerrc = L2_error(uc, u, Chebyshev)\nerrl = L2_error(ul, u, Legendre)\n\nplt.figure(figsize=(5, 3))\nplt.loglog(np.arange(0, N, 2), errc, '+',\n           np.arange(0, N, 2), errl, 'ko', fillstyle='none')\nplt.legend(['Chebyshev', 'Legendre']);"
  },
  {
    "objectID": "chebyshev.html#approximations-in-2d",
    "href": "chebyshev.html#approximations-in-2d",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Approximations in 2D",
    "text": "Approximations in 2D\nWe can approximate a two-dimensional function \\(u(x, y)\\) using a two-dimensional function space \\(W\\), such that\n\\[\nu(x, y) \\approx u_{N}(x, y) = \\sum_{i=0}^M \\hat{u}_{i}\\Psi_{i}(x, y),\n\\]\nwhere \\(\\Psi_{i}(x, y)\\) is a two-dimensional basis function and \\(\\{\\Psi_i(x, y)\\}_{i=0}^M\\) is a basis for the function space \\(W = \\text{span}\\{\\Psi_i(x, y)\\}_{i=0}^M\\). Just like in one dimensional space we will write \\(u_N\\) for the series expansion that approximates \\(u\\).\nThere are not all that many two-dimensional basis functions and a more common approach is to use one basis function for the \\(x\\)-direction and another for the \\(y\\)-direction\n\\[\nu_N(x, y) = \\sum_{i=0}^M\\sum_{j=0}^N \\hat{u}_{ij}\\psi_{i}(x) \\varphi_j(y).\n\\]"
  },
  {
    "objectID": "chebyshev.html#section",
    "href": "chebyshev.html#section",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "",
    "text": "The discrete cosine transform of type 1 is defined to transform the real numbers \\((y_i)_{i=0}^N\\) into \\((Y_i)_{i=0}^N\\) such that\n\\[\nY_i = y_0 + (-1)^{i}y_N + 2\\sum_{j=1}^{N-1}y_j \\cos(ij\\pi / N), \\quad i=0,1,\\ldots, N\n\\]"
  },
  {
    "objectID": "chebyshev.html#section-1",
    "href": "chebyshev.html#section-1",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "",
    "text": "The function space \\(W\\) is the tensor product of \\(V_x\\) and \\(V_y\\). The function space \\(W\\) has a basis that is formed as the tensor product of the basis for \\(V_x\\) and \\(V_y\\). The 2D basis function \\(\\Psi_{ij}\\) is the tensor product of \\(\\psi_i(x)\\) and \\(\\varphi_j(y)\\)\n\\[\n\\Psi_{ij}(x, y) = \\psi_i(x) \\varphi_j(y).\n\\]\nThis is also called the outer product, and sometimes also the dyadic product."
  },
  {
    "objectID": "chebyshev.html#we-can-approximate-a-two-dimensional-function-ux-y-using-a-two-dimensional-function-space-w",
    "href": "chebyshev.html#we-can-approximate-a-two-dimensional-function-ux-y-using-a-two-dimensional-function-space-w",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "We can approximate a two-dimensional function \\(u(x, y)\\) using a two-dimensional function space \\(W\\)",
    "text": "We can approximate a two-dimensional function \\(u(x, y)\\) using a two-dimensional function space \\(W\\)\nIn 2D we will try to find \\(u_N(x,y) \\in W\\), which implies:\n\\[\nu(x, y) \\approx u_{N}(x, y) = \\sum_{i=0}^M \\hat{u}_{i}\\Psi_{i}(x, y),\n\\]\n\n\\(\\Psi_{i}(x, y)\\) is a two-dimensional basis function\n\\(\\{\\Psi_i\\}_{i=0}^M\\) is a basis\n\\(W = \\text{span}\\{\\Psi_i\\}_{i=0}^M\\) is a 2D function space."
  },
  {
    "objectID": "chebyshev.html#section-2",
    "href": "chebyshev.html#section-2",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "",
    "text": "The function space \\(W\\) is the tensor product of \\(V_x\\) and \\(V_y\\). The function space \\(W\\) has a basis that is formed as the tensor product of the basis for \\(V_x\\) and \\(V_y\\). The 2D basis function \\(\\Psi_{ij}\\) is the tensor product of \\(\\psi_i(x)\\) and \\(\\varphi_j(y)\\)\n\\[\n\\Psi_{ij}(x, y) = \\psi_i(x) \\varphi_j(y).\n\\]\nThis is also called the outer product, and sometimes also the dyadic product."
  },
  {
    "objectID": "chebyshev.html#use-one-basis-function-for-each-direction",
    "href": "chebyshev.html#use-one-basis-function-for-each-direction",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Use one basis function for each direction",
    "text": "Use one basis function for each direction\nThere are not all that many two-dimensional basis functions and a more common approach is to use one basis function for the \\(x\\)-direction and another for the \\(y\\)-direction\n\\[\nu_N(x, y) = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} \\hat{u}_{ij}\\psi_{i}(x) \\varphi_j(y).\n\\]\n\n\n\n\n\n\nNote\n\n\nThe unknowns \\(\\{\\hat{u}_{ij}\\}_{i,j=0}^{N_x,N_y}\\) are now in the form of a matrix. The total number of unknowns: \\(N+1=(N_x+1)\\cdot (N_y+1)\\).\n\n\n\n\nThe most straightforward approach is to use the same basis functions for both directions. For example, with a Chebyshev basis\n\\[\nu_N(x, y) = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} \\hat{u}_{ij}T_{i}(x)T_j(y).\n\\]"
  },
  {
    "objectID": "chebyshev.html#two-dimensional-functionspaces",
    "href": "chebyshev.html#two-dimensional-functionspaces",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Two-dimensional functionspaces",
    "text": "Two-dimensional functionspaces\nRemember lecture 6. Cartesian grids.\n\\[\n\\boldsymbol{x}=(0, \\Delta x, \\ldots, N_x\\Delta x) \\quad \\boldsymbol{y}=(0, \\Delta y, \\ldots, N_x\\Delta y), \\quad L_x=N_x \\Delta x, L_y=N_y \\Delta y\n\\]\n\\[\n\\text{2D-mesh} \\rightarrow \\boldsymbol{x} \\times \\boldsymbol{y} = \\{(x, y) | x \\in \\boldsymbol{x} \\text{ and } y \\in \\boldsymbol{y}\\}\n\\]"
  },
  {
    "objectID": "chebyshev.html#we-can-approximate-a-two-dimensional-function-ux-y-using-a-two-dimensional-function-space-w_n",
    "href": "chebyshev.html#we-can-approximate-a-two-dimensional-function-ux-y-using-a-two-dimensional-function-space-w_n",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "We can approximate a two-dimensional function \\(u(x, y)\\) using a two-dimensional function space \\(W_N\\)",
    "text": "We can approximate a two-dimensional function \\(u(x, y)\\) using a two-dimensional function space \\(W_N\\)\nIn 2D we will try to find \\(u_N(x,y) \\in W_N\\), which implies:\n\\[\nu(x, y) \\approx u_{N}(x, y) = \\sum_{i=0}^N \\hat{u}_{i}\\Psi_{i}(x, y),\n\\]\n\n\\(\\Psi_{i}(x, y)\\) is a two-dimensional basis function\n\\(\\{\\Psi_i\\}_{i=0}^N\\) is a basis\n\\(W_N = \\text{span}\\{\\Psi_i\\}_{i=0}^N\\) is a 2D function space."
  },
  {
    "objectID": "chebyshev.html#we-can-approximate-a-two-dimensional-function-small-ux-y-using-a-two-dimensional-function-space-small-w_n",
    "href": "chebyshev.html#we-can-approximate-a-two-dimensional-function-small-ux-y-using-a-two-dimensional-function-space-small-w_n",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "We can approximate a two-dimensional function \\(\\small u(x, y)\\) using a two-dimensional function space \\(\\small W_N\\)",
    "text": "We can approximate a two-dimensional function \\(\\small u(x, y)\\) using a two-dimensional function space \\(\\small W_N\\)\nIn 2D we will try to find \\(u_N(x,y) \\in W_N\\), which implies:\n\\[\nu(x, y) \\approx u_{N}(x, y) = \\sum_{i=0}^N \\hat{u}_{i}\\Psi_{i}(x, y),\n\\]\n\n\\(\\Psi_{i}(x, y)\\) is a two-dimensional basis function\n\\(\\{\\Psi_i\\}_{i=0}^N\\) is a basis\n\\(W_N = \\text{span}\\{\\Psi_i\\}_{i=0}^N\\) is a 2D function space."
  },
  {
    "objectID": "chebyshev.html#two-dimensional-function-spaces",
    "href": "chebyshev.html#two-dimensional-function-spaces",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Two-dimensional function spaces",
    "text": "Two-dimensional function spaces\nWe can define two one-dimensional function spaces for the two directions as\n\\[\nV_{N_x} = \\text{span}\\{\\psi_i\\}_{i=0}^{N_x} \\quad \\text{and} \\quad V_{N_y} = \\text{span}\\{\\varphi_i\\}_{i=0}^{N_y}\n\\]\nwith a 2D domain \\(\\Omega\\) created as Cartesian products of two 1D domains:\n\\[\n{I}_x = [a, b] \\quad \\text{and} \\quad {I}_y = [c, d] \\rightarrow \\Omega = I_x \\times I_y\n\\]\nA two-dimensional function space can then be created as\n\\[\nW_N = V_{N_x} \\otimes V_{N_y}, \\quad (x, y) \\in \\Omega.\n\\]\n\\(W_N\\) is the tensor product of \\(V_{N_x}\\) and \\(V_{N_y}\\)\n\nSimilarly,\n\\[\n\\Psi_{ij}(x, y) = \\psi_i(x) \\varphi_j(y)\n\\]\n\\(\\Psi_{ij}\\) is the tensor product (or outer product) of \\(\\psi_i\\) and \\(\\varphi_j\\)."
  },
  {
    "objectID": "chebyshev.html#the-cartesian-grid",
    "href": "chebyshev.html#the-cartesian-grid",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "The Cartesian grid",
    "text": "The Cartesian grid\nRemember lecture 6. Cartesian grids.\n\\[\n\\boldsymbol{x}=(0, \\Delta x, \\ldots, N_x\\Delta x) \\quad \\boldsymbol{y}=(0, \\Delta y, \\ldots, N_x\\Delta y), \\quad L_x=N_x \\Delta x, L_y=N_y \\Delta y\n\\]\n\\[\n\\text{2D-mesh} \\rightarrow \\boldsymbol{x} \\times \\boldsymbol{y} = \\{(x, y) | x \\in \\boldsymbol{x} \\text{ and } y \\in \\boldsymbol{y}\\}\n\\]"
  },
  {
    "objectID": "chebyshev.html#the-tensor-product-is-a-cartesian-product-with-multiplication",
    "href": "chebyshev.html#the-tensor-product-is-a-cartesian-product-with-multiplication",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The tensor product is a Cartesian product with multiplication",
    "text": "The tensor product is a Cartesian product with multiplication\nConsider the Cartesian product of the two sequences \\((1, 2, 3)\\) and \\((4, 5)\\) and compare with the tensor product\n\n\nCartesian product: \\[\n(1, 2, 3) \\times (4, 5) = \\begin{bmatrix}\n(1, 4) \\\\\n(1, 5) \\\\\n(2, 4) \\\\\n(2, 5) \\\\\n(3, 4) \\\\\n(3, 5) \\\\\n\\end{bmatrix}\n\\]\n\nTensor product: \\[\n(1, 2, 3) \\otimes (4, 5) = \\begin{bmatrix}\n1 \\cdot 4 \\\\\n1 \\cdot 5 \\\\\n2 \\cdot 4 \\\\\n2 \\cdot 5 \\\\\n3 \\cdot 4 \\\\\n3 \\cdot 5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n4 \\\\\n5 \\\\\n8 \\\\\n10 \\\\\n12 \\\\\n15\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "chebyshev.html#tensor-product-of-functions",
    "href": "chebyshev.html#tensor-product-of-functions",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Tensor product of functions",
    "text": "Tensor product of functions\n\n\nCartesian product: \\[\n(\\psi_0, \\psi_1) \\times (\\varphi_0, \\varphi_1) = \\begin{bmatrix}\n\\left(\\psi_0, \\varphi_0\\right) \\\\\n\\left(\\psi_0, \\varphi_1\\right) \\\\\n\\left(\\psi_1, \\varphi_0\\right) \\\\\n\\left(\\psi_1, \\varphi_1\\right)\n\\end{bmatrix}\n\\]\n\nTensor product: \\[\n(\\psi_0, \\psi_1) \\otimes (\\varphi_0, \\varphi_1) = \\begin{bmatrix}\n\\psi_0 \\cdot \\varphi_0 \\\\\n\\psi_0 \\cdot \\varphi_1 \\\\\n\\psi_1 \\cdot \\varphi_0 \\\\\n\\psi_1 \\cdot \\varphi_1\n\\end{bmatrix}\n\\]\n\n\nThis tensor product is the basis for \\(W_N\\):\n\\[\n\\{\\psi_0\\psi_0, \\psi_0\\psi_1, \\psi_1\\psi_0, \\psi_1\\psi_1\\}\n\\]\n\nwhich can also be arranged in matrix notation \\(\\{\\psi_i\\varphi_j\\}_{i,j=0}^{1,1}\\) (\\(i\\) is row, \\(j\\) is column)\n\\[\n(\\psi_0, \\psi_1) \\otimes (\\varphi_0, \\varphi_1) =  \n\\begin{bmatrix}\n\\psi_0 \\\\\n\\psi_1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\varphi_0 & \\varphi_1\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\psi_0 \\cdot \\varphi_0, \\psi_0 \\cdot \\varphi_1 \\\\\n\\psi_1 \\cdot \\varphi_0, \\psi_1 \\cdot \\varphi_1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "chebyshev.html#example-of-tensor-product-basis",
    "href": "chebyshev.html#example-of-tensor-product-basis",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Example of tensor product basis",
    "text": "Example of tensor product basis\nUse the space of all linear functions in both \\(x\\) and \\(y\\) directions\n\\[\nV_{N_x} = \\text{span}\\{1, x\\} \\quad \\text{and} \\quad V_{N_y} = \\text{span}\\{1, y\\}\n\\]\n\n\nCartesian product\n\\[\n(1, x) \\times (1, y) = \\begin{bmatrix}\n(1, 1) \\\\\n(1, y) \\\\\n(x, 1) \\\\\n(x, y)\n\\end{bmatrix}\n\\]\n\nTensor product\n\\[\n(1, x) \\otimes (1, y) = \\begin{bmatrix}\n1 \\\\\ny \\\\\nx \\\\\nxy\n\\end{bmatrix}\n\\]\n\n\n\nNumpy naturally arranges the outer product into matrix form:\n\n\ny = sp.Symbol('y')\nVx = np.array([1, x])\nVy = np.array([1, y])\nW = np.outer(Vx, Vy)\nprint(W)\n\n\n[[1 y]\n [x x*y]]"
  },
  {
    "objectID": "chebyshev.html#we-have-a-function-space-and-a-basis-now-its-time-to-approximate-small-uxy",
    "href": "chebyshev.html#we-have-a-function-space-and-a-basis-now-its-time-to-approximate-small-uxy",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "We have a function space and a basis, now it’s time to approximate \\(\\small u(x,y)\\)",
    "text": "We have a function space and a basis, now it’s time to approximate \\(\\small u(x,y)\\)\nThe variational methods require the \\(L^2(\\Omega)\\) inner product\n\\[\n\\begin{align*}\n(f, g)_{L^2(\\Omega)} &= \\int_{\\Omega} f g \\, d\\Omega, \\\\\n&= \\int_{I_x}\\int_{I_y} f(x,y)g(x,y)dxdy.\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe first line is identical to the definition used for the 1D case and is valid for any domain \\(\\Omega\\), not just Cartesian product domains. The only difference for 2D is that \\(f\\) and \\(g\\) now are functions of both \\(x\\) and \\(y\\) and the the integral over the domain is a double integral."
  },
  {
    "objectID": "chebyshev.html#galerkin-for-2d-approximations",
    "href": "chebyshev.html#galerkin-for-2d-approximations",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Galerkin for 2D approximations",
    "text": "Galerkin for 2D approximations\nWe want to approximate\n\\[\nu(x, y) \\approx u_N(x, y)\n\\]\nThe Galerkin method is then: find \\(u_N \\in W_N\\) such that\n\\[\n(u - u_N, v) = 0, \\quad \\forall \\, v \\in W_N \\tag{1}\n\\]\n\nIn order to solve the problem we just choose basis functions and solve (1). For example, use Legendre polynomials in both \\(x\\) and \\(y\\)-directions.\n\\[\nV_{N_x} = \\text{span}\\{P_i\\}_{i=0}^{N_x}, \\quad \\text{and} \\quad V_{N_y} = \\text{span}\\{P_j\\}_{j=0}^{N_y}\n\\]\n\\[\nW_N = V_{N_x} \\otimes V_{N_y} = \\text{span}\\{P_iP_j\\}_{i,j=0}^{N_x, N_y}\n\\]"
  },
  {
    "objectID": "chebyshev.html#example-small-ux-y-exp-x22y-0.52-x-y-in--1-1-times--1-1",
    "href": "chebyshev.html#example-small-ux-y-exp-x22y-0.52-x-y-in--1-1-times--1-1",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Example: \\(\\small u(x, y) = \\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1] \\times [-1, 1]\\)",
    "text": "Example: \\(\\small u(x, y) = \\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1] \\times [-1, 1]\\)\nThis means to compute\n\\[\n\\int_{-1}^1\\int_{-1}^1 \\left(u - \\sum_{i=0}^N \\sum_{j=0}^N \\hat{u}_{ij} P_i(x)P_j(y)\\right) P_m(x)P_n(y) dx dy = 0, \\quad \\forall (m, n) \\in \\mathcal{I}_N^2,\n\\]\nwhere the index set \\(\\mathcal{I}_N = \\{0, 1, \\ldots, N\\}\\) and \\(\\mathcal{I}_N^2 = \\mathcal{I}_N \\times \\mathcal{I}_N\\). The test function \\(v\\) is here the tensor product basis function \\(P_m(x)P_n(y)\\).\nEquation {eq}`eq-project2D` provides $(N+1)^2$ equations for the $(N+1)^2$ unknown in $\\hat{U} = (\\hat{u}_{ij})_{i,j=0}^N$."
  },
  {
    "objectID": "chebyshev.html#example",
    "href": "chebyshev.html#example",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Example:",
    "text": "Example:\n\\[ \\small\nu(x, y)=\\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1] \\times [-1, 1]\n\\]\nFind \\(u_N \\in W_N = V_{N_x} \\otimes V_{N_y}\\) using Legendre polynomials for both directions. With Galerkin: find \\(u_N \\in W_N\\) such that\n\\[\n(u-u_N,v)=0 \\quad \\forall \\, v \\in W_N\n\\]\n\nFind the matrix \\(U=\\{u_{ij}\\}_{i,j=0}^{N_x,N_y}\\), \\(u_{ij}=(u, P_iP_j)\\)\nFind the matrix \\(A=\\{a_{ij}\\}_{i,j=0}^{N_x,N_y}\\), \\(a_{ij}=\\|P_i\\|^2 \\delta_{ij}\\)\nCompute \\(\\hat{U} = A^{-1} U A^{-1}\\)\n\n\nimport scipy.sparse as sparse\nfrom scipy.integrate import dblquad\nue = sp.exp(-(x**2+2*(y-sp.S.Half)**2))\nuh = lambda i, j: dblquad(sp.lambdify((x, y), ue*sp.legendre(i, x)*sp.legendre(j, y)), -1, 1, -1, 1, epsabs=1e-12)[0]\nN = 8\nuij = np.zeros((N+1, N+1))\nfor i in range(N+1):\n    for j in range(N+1):\n        uij[i, j] = uh(i, j)\nA_inv = sparse.diags([(2*np.arange(N+1)+1)/2], [0], (N+1, N+1))\nuhat_ij = A_inv @ uij @ A_inv"
  },
  {
    "objectID": "chebyshev.html#the-inner-product-with-double-integral-becomes-a-bit-messy",
    "href": "chebyshev.html#the-inner-product-with-double-integral-becomes-a-bit-messy",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "The inner product with double integral becomes a bit messy",
    "text": "The inner product with double integral becomes a bit messy\nWe compute \\((u - u_N, v)\\) using\n\\[\nv = P_m(x)P_n(y) \\quad \\text{and} \\quad u_N = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y}\\hat{u}_{ij} P_i(x) P_j(y)\n\\]\n\nIt becomes a bit messy, with 4 different indices:\n\\[\n\\int_{-1}^1\\int_{-1}^1 \\left(u - \\sum_{i=0}^N \\sum_{j=0}^N \\hat{u}_{ij} P_i(x)P_j(y)\\right) P_m(x)P_n(y) dx dy\n\\]\nNote that the unknown coefficients \\(\\hat{u}_{ij}\\) are independent of space and we can simplify the double integrals by separating them into one integral for \\(x\\) and one for \\(y\\). We get\n\\[\n\\int_{-1}^1 \\int_{-1}^1 P_i(x)P_j(y) P_m(x)P_n(y) dx dy = \\underbrace{\\int_{-1}^1 P_i(x)P_m(x)dx}_{a_{mi}}  \\underbrace{\\int_{-1}^1 P_j(y) P_n(y) dy}_{a_{nj}}\n\\]"
  },
  {
    "objectID": "chebyshev.html#the-linear-algebra-problem-becomes",
    "href": "chebyshev.html#the-linear-algebra-problem-becomes",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "The linear algebra problem becomes",
    "text": "The linear algebra problem becomes\nUsing for the right hand side \\((u, v)\\) \\[\nu_{mn} = \\int_{-1}^1 \\int_{-1}^1 u(x, y) P_m(x)P_n(y) dx dy\n\\]\nwhere \\(U = \\{u_{mn}\\}_{m,n=0}^{N_x, N_y}\\) is a matrix of coefficients.\n\\((u - u_N, v) = 0\\) thus becomes the linear algebra problem\n\\[\n\\sum_{i=0}^{N_x} \\sum_{j=0}^{N_y} a_{mi}a_{nj} \\hat{u}_{ij} = u_{mn}, (m,n) \\in (0, \\ldots, N_x)\\times (0, \\ldots, N_y)\n\\]\nIn matrix form this is\n\\[\nA \\hat{U} A = U,\n\\]"
  },
  {
    "objectID": "chebyshev.html#we-now-compute-u---u_n-v-using",
    "href": "chebyshev.html#we-now-compute-u---u_n-v-using",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "We now compute \\((u - u_N, v)\\) using",
    "text": "We now compute \\((u - u_N, v)\\) using\n\\[\nv = P_m(x)P_n(y) \\quad \\text{and} \\quad u_N = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y}\\hat{u}_{ij} P_i(x) P_j(y)\n\\]\n\nIt becomes a bit messy, with 4 different indices:\n\\[\n\\int_{-1}^1\\int_{-1}^1 \\left(u - \\sum_{i=0}^N \\sum_{j=0}^N \\hat{u}_{ij} P_i(x)P_j(y)\\right) P_m(x)P_n(y) dx dy\n\\]\n\n\nNote that the unknown coefficients \\(\\hat{u}_{ij}\\) are independent of space and we can simplify the double integrals by separating them into one integral for \\(x\\) and one for \\(y\\). For example\n\\[\n\\int_{-1}^1 \\int_{-1}^1 P_i(x)P_j(y) P_m(x)P_n(y) dx dy = \\underbrace{\\int_{-1}^1 P_i(x)P_m(x)dx}_{a_{mi}}  \\underbrace{\\int_{-1}^1 P_j(y) P_n(y) dy}_{a_{nj}}\n\\]"
  },
  {
    "objectID": "chebyshev.html#solve-the-linear-algebra-problem",
    "href": "chebyshev.html#solve-the-linear-algebra-problem",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Solve the linear algebra problem",
    "text": "Solve the linear algebra problem\n\\[\n\\begin{align}\n\\sum_{i=0}^{N_x} \\sum_{j=0}^{N_y} a_{mi}a_{nj} \\hat{u}_{ij} &= u_{mn}, \\quad (m,n)\\in (0, \\ldots, N_x) \\times (0, \\ldots, N_y) \\\\\n\\longrightarrow A \\hat{U} A &= U\n\\end{align}\n\\]\nCan solve for \\(U\\) with the vec-trick (\\(\\text{vec}(A \\hat{U} A^T) = (A \\otimes A) \\text{vec}{(\\hat{U})}\\))\n\\[\n\\begin{align}\n(A \\otimes A) \\text{vec}(\\hat{U}) &= \\text{vec}(U) \\\\\n\\text{vec}(\\hat{U}) &= (A \\otimes A)^{-1} \\text{vec}(U)\n\\end{align}\n\\]\nHowever, since \\(A\\) here is a diagonal matrix and we only have one matrix \\((A\\hat{U}A)\\) it is actually much easier to just avoid the vectorization and solve directly\n\\[\n\\hat{U} = A^{-1} U A^{-1}.\n\\]"
  },
  {
    "objectID": "chebyshev.html#breaking-down-small-u-u_n-v",
    "href": "chebyshev.html#breaking-down-small-u-u_n-v",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Breaking down \\(\\small (u-u_N, v)\\)",
    "text": "Breaking down \\(\\small (u-u_N, v)\\)\n\\[\n\\text{With} \\quad v = P_m(x)P_n(y) \\quad \\text{and} \\quad u_N = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y}\\hat{u}_{ij} P_i(x) P_j(y)\n\\]\n\\[\n(u-u_N, v)=0 \\rightarrow \\int_{-1}^1\\int_{-1}^1 \\left(u - \\sum_{i=0}^N \\sum_{j=0}^N \\hat{u}_{ij} P_i(x)P_j(y)\\right) P_m(x)P_n(y) dx dy = 0\n\\]\n\\[\n(u, v) = \\int_{-1}^1 \\int_{-1}^1 u(x, y) P_m(x)P_n(y) dx dy = u_{mn}\n\\]\n\\[\n(u_N, v) := \\sum_{i=0}^{N_x} \\sum_{j=0}^{N_y} a_{mi}a_{nj} \\hat{u}_{ij}\n\\]\n\n\\[\n(u-u_N,v)=0 \\longrightarrow \\boxed{ \\sum_{i=0}^{N_x} \\sum_{j=0}^{N_y} a_{mi}a_{nj} \\hat{u}_{ij} = u_{mn}}, \\quad (m, n) = (0, \\ldots, N_x) \\times (0, \\ldots, N_y)\n\\]"
  },
  {
    "objectID": "chebyshev.html#evaluate-the-2d-solution",
    "href": "chebyshev.html#evaluate-the-2d-solution",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Evaluate the 2D solution",
    "text": "Evaluate the 2D solution\nWe have found \\(\\{\\hat{u}_{ij}\\}_{i,j=0}^{N_x,N_y}\\), so now we can evaluate\n\\[\nu_N(x, y) = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} \\hat{u}_{ij} P_i(x)P_j(y)\n\\]\nfor any \\(x, y\\), preferably within the domain \\([-1, 1] \\times [-1, 1]\\).\nHow to do this?\n\nA simple double for-loop will do, or on matrix-vector form to avoid the for-loop. Use \\(\\boldsymbol{P_x}=(P_0(x), \\ldots, P_{N_x})\\) and \\(\\boldsymbol{P_y}=(P_0(y), \\ldots, P_{N_y}(y))\\)\n\\[\n\\boldsymbol{P_x} \\hat{U} \\boldsymbol{P_{y}}^T =\n\\begin{bmatrix} P_0(x)&  \\ldots& P_{N_x}(x)\\end{bmatrix} \\begin{bmatrix} u_{0,0} & \\cdots & u_{0,N_y} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nu_{N_x, 0} & \\cdots & u_{N_x,N_y}\n\\end{bmatrix}\n\\begin{bmatrix}\nP_0(y) \\\\\n\\vdots \\\\\nP_{N_y}(y)\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "chebyshev.html#evaluate-for-a-computational-mesh",
    "href": "chebyshev.html#evaluate-for-a-computational-mesh",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Evaluate for a computational mesh",
    "text": "Evaluate for a computational mesh\nIt is very common to compute the solution on a 2D computational Cartesian grid \\(\\boldsymbol{x}= (x_0, x_1, \\ldots, x_{N_x})\\) and \\(\\boldsymbol{y}=(y_0, y_1, \\ldots, y_{N_y})\\):\n\\[\n\\boldsymbol{x} \\times \\boldsymbol{y} = \\{(x, y) | x \\in \\boldsymbol{x} \\text{ and } y \\in \\boldsymbol{y}\\}\n\\]\n\\[\nu_N(x_i, y_j) = \\sum_{m=0}^N \\sum_{n=0}^N \\hat{u}_{mn} P_m(x_i)P_n(y_j).\n\\]\nFour nested for-loops, or a triple matrix product\n\\[\n\\begin{bmatrix} P_0(x_0)&  \\ldots& P_{N_x}(x_0) \\\\\n\\vdots&  \\ddots & \\vdots \\\\\nP_{0}(x_{N_x})&  \\ldots & P_{N_x}(x_{N_x})\n\\end{bmatrix} \\begin{bmatrix} \\hat{u}_{0,0} & \\cdots & \\hat{u}_{0,N_y} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\hat{u}_{N_x, 0} & \\cdots & \\hat{u}_{N_x,N_y}\n\\end{bmatrix}\n\\begin{bmatrix}\nP_0(y_0)&  \\ldots& P_{0}(y_{N_y}) \\\\\n\\vdots&  \\ddots & \\vdots \\\\\nP_{N_y}(y_{0})&  \\ldots & P_{N_y}(y_{N_y})\n\\end{bmatrix}\n\\] If \\(\\boldsymbol{P_x} = \\{P_j(x_i)\\}_{i,j=0}^{N_x, N_x}\\) and \\(\\boldsymbol{P_y} = \\{P_j(y_i)\\}_{i,j=0}^{N_y, N_y}\\) this is simply:\n\\[\n\\boldsymbol{P_x} \\hat{U} \\boldsymbol{P_y}^T\n\\]"
  },
  {
    "objectID": "chebyshev.html#check-accuracy",
    "href": "chebyshev.html#check-accuracy",
    "title": "Function approximation with Chebyshev polynomials and in more dimensions",
    "section": "Check accuracy",
    "text": "Check accuracy\n\nxi = np.linspace(-1, 1, N+1)\nV = np.polynomial.legendre.legvander(xi, N)\nU = V @ uhat_ij @ V.T\nxij, yij = np.meshgrid(xi, xi, indexing='ij', sparse=True)\nueij = sp.lambdify((x, y), ue)(xij, yij)\nnp.linalg.norm(U-ueij)\n\n7.519704442136278e-10"
  },
  {
    "objectID": "chebyshev.html#check-accuracy-by-computing-the-ell2-error-norm",
    "href": "chebyshev.html#check-accuracy-by-computing-the-ell2-error-norm",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Check accuracy by computing the \\(\\ell^2\\) error norm",
    "text": "Check accuracy by computing the \\(\\ell^2\\) error norm\n\\[ \\small\n\\|u-u_N\\|_{\\ell^2} =\\sqrt{\\frac{4}{N_xN_y} \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} (u(x_i,y_j)-u_N(x_i,y_j))^2}\n\\]\n\n\ndef l2error(uh_ij): \n    N = uh_ij.shape[0]-1\n    xi = np.linspace(-1, 1, N+1)\n    U = eval2D(xi, xi, uh_ij)\n    xij, yij = np.meshgrid(xi, xi, indexing='ij', sparse=True)\n    ueij = sp.lambdify((x, y), ue)(xij, yij)\n    return np.linalg.norm(U-ueij)*(2/N)\n\ndef solve(N):\n    uij = np.zeros((N+1, N+1))\n    for i in range(N+1):\n        for j in range(N+1):\n            uij[i, j] = uh(i, j)\n    A_inv = sparse.diags([(2*np.arange(N+1)+1)/2], [0], (N+1, N+1))\n    return A_inv @ uij @ A_inv\n    \nerror = []\nfor n in range(4, 24, 2):\n    error.append(l2error(solve(n)))\nplt.figure(figsize=(5, 3))\nplt.semilogy(np.arange(4, 24, 2), error);"
  },
  {
    "objectID": "chebyshev.html#short-recap",
    "href": "chebyshev.html#short-recap",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Short recap",
    "text": "Short recap\nWe want to find an approximation to \\(u(x)\\) using\n\\[\nu(x) \\approx u_N(x) = \\sum_{k=0}^N \\hat{u}_k \\psi_k(x)\n\\]\n\n\n\nLeast squares method\nGalerkin method\nCollocation method (Lagrange interpolation)\n\n\n\n\\(\\psi_j\\) is a basis function\n\\(\\{\\psi_j\\}_{j=0}^N\\) is a basis\n\\(V_N = \\text{span}\\{\\psi_j\\}_{j=0}^N\\) is a function space\n\n\n\nThe variational methods make use of integrals over the domain. The \\(L^2(\\Omega)\\) inner product and norms are (in 1D, where \\(\\Omega=[a, b]\\))\n\\[\n(f, g)_{L^2(\\Omega)} = \\int_{\\Omega} f(x) g(x) \\, dx \\quad \\text{and} \\quad \\|f\\|_{L^2(\\Omega)} = \\sqrt{(f, f)_{L^2(\\Omega)}}\n\\]"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials",
    "href": "chebyshev.html#legendre-polynomials",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials",
    "text": "Legendre polynomials\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-are-very-good-global-basis-functions",
    "href": "chebyshev.html#legendre-polynomials-are-very-good-global-basis-functions",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials are very good global basis functions",
    "text": "Legendre polynomials are very good global basis functions\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=\\psi_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) and solve for \\(\\{\\hat{u}_j\\}_{j=0}^N\\) \\(\\rightarrow\\) Done!\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)"
  },
  {
    "objectID": "chebyshev.html#weight-function",
    "href": "chebyshev.html#weight-function",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Weight function",
    "text": "Weight function"
  },
  {
    "objectID": "chebyshev.html#the-weight-function-favours-the-edges",
    "href": "chebyshev.html#the-weight-function-favours-the-edges",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The weight function favours the edges",
    "text": "The weight function favours the edges\n\\[\n\\omega(x) = \\frac{1}{\\sqrt{1-x^2}}\n\\]\n\n\n\n\n\n\n\n\n\nSo the weighted Chebyshev approach has smaller errors towards the edges."
  },
  {
    "objectID": "chebyshev.html#evaluate",
    "href": "chebyshev.html#evaluate",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Evaluate",
    "text": "Evaluate\n\ndef eval2D(xi, yi, uhat):\n    Vx = np.polynomial.legendre.legvander(xi, uhat.shape[0]-1)\n    Vy = np.polynomial.legendre.legvander(yi, uhat.shape[1]-1) \n    return Vx @ uhat @ Vy.T\n\nN = 20\nxi = np.linspace(-1, 1, N+1)\nU = eval2D(xi, xi, uhat_ij)\nxij, yij = np.meshgrid(xi, xi, indexing='ij', sparse=False)\nplt.contourf(xij, yij, U)"
  },
  {
    "objectID": "chebyshev.html#implement-evaluate-in-2d",
    "href": "chebyshev.html#implement-evaluate-in-2d",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Implement evaluate in 2D",
    "text": "Implement evaluate in 2D\n\ndef eval2D(xi, yi, uhat):\n    Vx = np.polynomial.legendre.legvander(xi, uhat.shape[0]-1)\n    Vy = np.polynomial.legendre.legvander(yi, uhat.shape[1]-1) \n    return Vx @ uhat @ Vy.T\n\nN = 20\nxi = np.linspace(-1, 1, N+1)\nU = eval2D(xi, xi, uhat_ij)\nxij, yij = np.meshgrid(xi, xi, indexing='ij', sparse=False)\nplt.contourf(xij, yij, U)"
  },
  {
    "objectID": "chebyshev.html#helpful-tools",
    "href": "chebyshev.html#helpful-tools",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Helpful tools",
    "text": "Helpful tools\nChebfun and Shenfun\nLets approximate \\(u(x, y)=\\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1] \\times [-1, 1]\\) using Shenfun\n\nimport shenfun as sf \nue = sp.exp(-(x**2+2*(y-sp.S.Half)**2)) \nT = sf.FunctionSpace(20, 'C')\nW = sf.TensorProductSpace(sf.comm, (T, T))\nuN = sf.project(ue, W) # projection is Galerkin approximation\nxi, yj = W.local_mesh(True, kind='uniform')\nplt.contourf(xi, yj, uN.backward(mesh='uniform'))"
  },
  {
    "objectID": "chebyshev.html#some-helpful-tools",
    "href": "chebyshev.html#some-helpful-tools",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Some helpful tools",
    "text": "Some helpful tools\nChebfun and Shenfun\nApproximate \\(u(x, y)=\\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1]^2\\) using Shenfun:\n\nimport shenfun as sf \nue = sp.exp(-(x**2+2*(y-sp.S.Half)**2)) \nT = sf.FunctionSpace(20, 'C')\nW = sf.TensorProductSpace(sf.comm, (T, T))\nuN = sf.project(ue, W) # projection is Galerkin approximation\nxi, yj = W.local_mesh(True, kind='uniform')\nplt.contourf(xi, yj, uN.backward(mesh='uniform'))"
  },
  {
    "objectID": "chebyshev.html#some-helpful-tools-chebfun-and-shenfun",
    "href": "chebyshev.html#some-helpful-tools-chebfun-and-shenfun",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Some helpful tools: Chebfun and Shenfun",
    "text": "Some helpful tools: Chebfun and Shenfun"
  },
  {
    "objectID": "chebyshev.html#d-using-tensor-products",
    "href": "chebyshev.html#d-using-tensor-products",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "2D using tensor products",
    "text": "2D using tensor products\nApproximate \\(u(x, y)=\\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1]^2\\) using Shenfun:\n\nue = sp.exp(-(x**2+2*(y-sp.S.Half)**2)) \nT = sf.FunctionSpace(20, 'Chebyshev')\nW = sf.TensorProductSpace(sf.comm, (T, T))\nuN = sf.project(ue, W) # projection is Galerkin approximation\nxi, yj = W.local_mesh(True, kind='uniform')\nplt.contourf(xi, yj, uN.backward(mesh='uniform'))"
  },
  {
    "objectID": "chebyshev.html#plot-the-pointwise-error",
    "href": "chebyshev.html#plot-the-pointwise-error",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Plot the pointwise error",
    "text": "Plot the pointwise error\n\n\nxj = np.linspace(-1, 1, 1000)\nplt.figure(figsize=(5, 3))\nplt.plot(xj, uh(xj)-sp.lambdify(x, ue)(xj), 'r')\nplt.ticklabel_format(axis='y', style='sci', \n                     scilimits=(-5,-5))\nplt.gca().set_yticks([-1e-4, -5e-5, 0, 5e-5]);\n\n\n\n\n\n\n\n\n\n\nNote the oscillation in the error that is typical of a spactral method.\nWho thinks that Chebyshev can do better?\n\n\n\nV = sf.FunctionSpace(N+1, 'Chebyshev', \n                     domain=(-1, 1))\nuhc = sf.project(ue, V)\nplt.figure(figsize=(5, 3))\nplt.plot(xj, uhc(xj)-sp.lambdify(x, ue)(xj), 'r');"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-are-very-good-basis-functions",
    "href": "chebyshev.html#legendre-polynomials-are-very-good-basis-functions",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials are very good basis functions",
    "text": "Legendre polynomials are very good basis functions\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=\\psi_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) and solve for \\(\\{\\hat{u}_j\\}_{j=0}^N\\) \\(\\rightarrow\\) Done!\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)"
  },
  {
    "objectID": "chebyshev.html#shenfun-in-2d-using-tensor-products",
    "href": "chebyshev.html#shenfun-in-2d-using-tensor-products",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Shenfun in 2D using tensor products",
    "text": "Shenfun in 2D using tensor products\nApproximate \\(u(x, y)=\\exp(-(x^2+2(y-0.5)^2)), (x, y) \\in [-1, 1]^2\\):\n\nue = sp.exp(-(x**2+2*(y-sp.S.Half)**2)) \nT = sf.FunctionSpace(20, 'Chebyshev')\nW = sf.TensorProductSpace(sf.comm, (T, T))\nuN = sf.project(ue, W) # projection is Galerkin approximation\nxi, yj = W.local_mesh(True, kind='uniform')\nplt.contourf(xi, yj, uN.backward(mesh='uniform'))"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-are-very-good-basis-funcs",
    "href": "chebyshev.html#legendre-polynomials-are-very-good-basis-funcs",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials are very good basis funcs",
    "text": "Legendre polynomials are very good basis funcs\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=\\psi_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) and solve for \\(\\{\\hat{u}_j\\}_{j=0}^N\\) \\(\\rightarrow\\) Done!\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-are-a-very-good-basis",
    "href": "chebyshev.html#legendre-polynomials-are-a-very-good-basis",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials are a very good basis",
    "text": "Legendre polynomials are a very good basis\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=\\psi_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) and solve for \\(\\{\\hat{u}_j\\}_{j=0}^N\\) \\(\\rightarrow\\) Done!\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)\nFinding \\(u_N \\in V_N\\) with Galerkin is often called a projection of \\(u(x)\\) onto \\(V_N\\)"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-form-a-very-good-basis",
    "href": "chebyshev.html#legendre-polynomials-form-a-very-good-basis",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials form a very good basis",
    "text": "Legendre polynomials form a very good basis\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=P_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j P_j\\) and solve to get \\(\\hat{u}_i=\\frac{(u, P_i)}{\\|P_i\\|^2}, i=0,1, \\ldots, N\\)\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)\nThe Galerkin method is also be referred to as a projection of \\(u(x)\\) onto \\(V_N\\)"
  },
  {
    "objectID": "chebyshev.html#it-is-more-common-to-use-one-basis-function-for-each-direction",
    "href": "chebyshev.html#it-is-more-common-to-use-one-basis-function-for-each-direction",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "It is more common to use one basis function for each direction",
    "text": "It is more common to use one basis function for each direction\nThere are not all that many two-dimensional basis functions and a more common approach is to use one basis function for the \\(x\\)-direction and another for the \\(y\\)-direction\n\\[\nu_N(x, y) = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} \\hat{u}_{ij}\\psi_{i}(x) \\varphi_j(y).\n\\]\n\n\n\n\n\n\nNote\n\n\nThe unknowns \\(\\{\\hat{u}_{ij}\\}_{i,j=0}^{N_x,N_y}\\) are now in the form of a matrix. The total number of unknowns: \\(N+1=(N_x+1)\\cdot (N_y+1)\\).\n\n\n\n\nThe most straightforward approach is to use the same basis functions for both directions. For example, with a Chebyshev basis\n\\[\nu_N(x, y) = \\sum_{i=0}^{N_x}\\sum_{j=0}^{N_y} \\hat{u}_{ij}T_{i}(x)T_j(y).\n\\]"
  },
  {
    "objectID": "chebyshev.html#try-shenfun",
    "href": "chebyshev.html#try-shenfun",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Try Shenfun",
    "text": "Try Shenfun\nApproximate with Legendre polynomials through Shenfun and the Galerkin method\n\\[\nu(x)=\\frac{1}{1+25x^2} \\quad x \\in [-1, 1] \\rightarrow \\hat{u}_j = \\frac{2j+1}{2}(u, P_j)\n\\]\n\n\nimport shenfun as sf  \nN = 50\nue = 1./(1+25*x**2)\nV = sf.FunctionSpace(N+1, 'Legendre', domain=(-1, 1))\nv = sf.TestFunction(V)\nuh = (2*np.arange(N+1)+1)/2*sf.inner(ue, v) \nplt.figure(figsize=(6, 3))\nplt.plot(V.mesh(), uh.backward(), 'b', V.mesh(), sp.lambdify(x, ue)(V.mesh()), 'ro')\nplt.legend(['Legendre', 'Exact'])\n\n\n\n\n\n\n\n\n\n\nNote the implementation. Choose FunctionSpace and compute Legendre coefficients using the inner product, with \\(v=P_j\\) as a TestFunction for the function space V."
  },
  {
    "objectID": "chebyshev.html#fast-chebyshev-transforms",
    "href": "chebyshev.html#fast-chebyshev-transforms",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Fast Chebyshev transforms",
    "text": "Fast Chebyshev transforms\nOne of the reasons why Chebyshev polynomials are so popular is the fact that you can transform fast between spectral and physical space\n\\[\nu_N(x_i) = \\sum_{j=0}^N \\hat{u}_j T_j(x_i), \\quad i=0, 1, \\ldots, N\n\\]\nHere\n\\[\n\\begin{matrix}\n\\text{Physical points} & \\boldsymbol{u} = (u_N(x_i))_{i=0}^N \\\\\n\\text{Spectral points} & \\boldsymbol{\\hat{u}} = (\\hat{u}_i)_{i=0}^N\n\\end{matrix}\n\\]\nSlow implementation: Using \\(\\boldsymbol{T} = (T_j(x_i))_{i,j=0}^{N, N}\\) we get\n\\[\n\\boldsymbol{u} = \\boldsymbol{T} \\boldsymbol{\\hat{u}}\n\\]\nwhich is computed in \\((2N+1)(N+1)\\) floating point operations, which scales as \\(\\mathcal{O}(N^2)\\)."
  },
  {
    "objectID": "chebyshev.html#slow-chebyshev-transforms-implementation",
    "href": "chebyshev.html#slow-chebyshev-transforms-implementation",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Slow Chebyshev transforms implementation",
    "text": "Slow Chebyshev transforms implementation\n\nN = 10\nT = np.polynomial.chebyshev.chebvander(np.cos(np.arange(N)*np.pi/(N-1)), N)\nuhat = np.random.random(N+1)\nu = T @ uhat\nprint(u)\n\n[ 5.99738384 -0.95502078  2.18640766 -1.44865349  1.45577777 -1.36972851\n -0.56772977  1.17448687 -1.32189502 -0.6117369 ]\n\n\nSlow because you use \\(\\mathcal{O}(N^2)\\) floating point operations and memory demanding because you need a matrix \\(\\boldsymbol{T} \\in \\mathbb{R}^{(N+1)\\times (N+1)}\\).\nLets describe a faster way to compute \\(\\boldsymbol{u}=(u_N(x_i))_{i=0}^N\\) from \\(\\boldsymbol{\\hat{u}}=(\\hat{u}_i)_{i=0}^N\\)"
  },
  {
    "objectID": "chebyshev.html#fast-cosine-transform",
    "href": "chebyshev.html#fast-cosine-transform",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Fast cosine transform",
    "text": "Fast cosine transform\nLet\n\\[\nx_i = \\cos(i \\pi / N), \\quad i=0, 1, \\ldots, N\n\\]\nsuch that for \\(i=0, 1, \\ldots, N\\):\n\\[\n\\begin{align}\nu_N(x_i) &= \\sum_{j=0}^N \\hat{u}_j T_j(x_i)\\\\\nu_N(x_i) &= \\sum_{j=0}^N \\hat{u}_j \\cos(j i \\pi /N) \\\\\nu_N(x_i) &= \\hat{u}_0 + (-1)^{i}\\hat{u}_N + \\sum_{j=1}^{N-1} \\hat{u}_j \\cos(j i \\pi /N)\n\\end{align}\n\\]"
  },
  {
    "objectID": "chebyshev.html#the-discrete-cosine-transform",
    "href": "chebyshev.html#the-discrete-cosine-transform",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "The discrete cosine transform",
    "text": "The discrete cosine transform\nThe discrete cosine transform of type 1 is defined to transform the real numbers \\(\\boldsymbol{y}=(y_i)_{i=0}^N\\) into \\(\\boldsymbol{Y}=(Y_i)_{i=0}^N\\) such that\n\\[\nY_i = y_0 + (-1)^{i}y_N + 2\\sum_{j=1}^{N-1}y_j \\cos(ij\\pi / N), \\quad i=0,1,\\ldots, N\n\\]\nThis operation can be evaluated in \\(\\mathcal{O}(N \\log_2 N)\\) floating point operations, using the Fast Fourier Transform (FFT). Vectorized:\n\\[\n\\boldsymbol{Y} = DCT^1(\\boldsymbol{y})\n\\]\nThe DCT is found in scipy and we will now use it to compute a fast Chebyshev transform."
  },
  {
    "objectID": "chebyshev.html#fast-chebyshev-transform",
    "href": "chebyshev.html#fast-chebyshev-transform",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Fast Chebyshev transform",
    "text": "Fast Chebyshev transform\nWe have the \\(DCT^1\\) for any \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{y}\\) \\[ \\small\nY_i = y_0 + (-1)^{i}y_N + 2\\sum_{j=1}^{N-1}y_j \\cos(ij\\pi / N), \\quad i=0,1,\\ldots, N\n\\]\nWe want to compute the following using the fast \\(DCT^1\\) \\[ \\small\nu_N(x_i) = \\hat{u}_0 + (-1)^{i}\\hat{u}_N + \\sum_{j=1}^{N-1} \\hat{u}_j \\cos(j i \\pi /N), \\quad i=0,1,\\ldots, N \\tag{1}\n\\]\n\nRearrange (1) my multiplying by 2: \\[ \\small\n2u_N(x_i)-\\hat{u}_0-(-1)^{i}\\hat{u}_N = \\overbrace{\\hat{u}_0 + (-1)^{i}\\hat{u}_N + 2\\sum_{j=1}^{N-1}\\hat{u}_j \\cos(ij\\pi / N)}^{DCT^1(\\boldsymbol{\\hat{u}})_i}\n\\]\n\\[ \\small\nu_N(x_i) = \\frac{DCT^1(\\boldsymbol{\\hat{u}})_i + \\hat{u}_0 + (-1)^{i}\\hat{u}_N}{2}\n\\]"
  },
  {
    "objectID": "chebyshev.html#fast-implementation",
    "href": "chebyshev.html#fast-implementation",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Fast implementation",
    "text": "Fast implementation\n\n\n\\[\n\\boldsymbol{u} = \\frac{DCT^1(\\boldsymbol{\\hat{u}}) + \\hat{u}_0 + I_m \\hat{u}_N}{2}\n\\]\nwhere\n\\[\nI_m = ((-1)^{i})_{i=0}^N\n\\]\n\n\nimport scipy\n\ndef evaluate_cheb_1(uhat):\n  N = len(uhat)\n  uj = scipy.fft.dct(uhat, type=1)\n  uj += uhat[0]\n  uj[::2] += uhat[-1]\n  uj[1::2] -= uhat[-1]\n  uj *= 0.5\n  return uj\n\nN = 1000\nxi = np.cos(np.arange(N+1)*np.pi/N)\nT = np.polynomial.chebyshev.chebvander(xi, N)\nuhat = np.ones(N+1)\nuj = T @ uhat\nuj_fast = evaluate_cheb_1(uhat)\nassert np.allclose(uj, uj_fast) \n\n\n\n\n\n\nTiming of regular transform:\n\n%timeit -q -o -n 10 uj = T @ uhat\n\n&lt;TimeitResult : 246 µs ± 170 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)&gt;\n\n\n\nTiming of fast transform:\n\n%timeit -q -o -n 10 uj_fast = evaluate_cheb_1(uhat)\n\n&lt;TimeitResult : 18.7 µs ± 15.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)&gt;"
  },
  {
    "objectID": "chebyshev.html#legendre-polynomials-form-a-good-basis-for-mathbbp_n",
    "href": "chebyshev.html#legendre-polynomials-form-a-good-basis-for-mathbbp_n",
    "title": "Function approximation with Chebyshev polynomials and in 2 dimensions",
    "section": "Legendre polynomials form a good basis for \\(\\mathbb{P}_N\\)",
    "text": "Legendre polynomials form a good basis for \\(\\mathbb{P}_N\\)\n\n\n\\[ \\small\n\\begin{align*}\nP_0(x) &= 1, \\\\\nP_1(x) &= x, \\\\\nP_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n&\\vdots \\\\\n(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin method to approximate \\(u(x) \\approx u_N(x)\\) with Legendre polynomials:\nFind \\(u_N \\in V_N (= \\text{span}\\{P_j\\}_{i=0}^N=\\mathbb{P}_N)\\) such that\n\\[\n(u-u_N, v)_{L^2(\\Omega)} = 0, \\quad \\forall \\, v \\in V_N\n\\]\n\n\n\nInsert for \\(v=P_i\\) and \\(u_N=\\sum_{j=0}^N \\hat{u}_j P_j\\) and solve to get \\(\\hat{u}_i=\\frac{(u, P_i)}{\\|P_i\\|^2}, i=0,1, \\ldots, N\\)\nRequires mapping if \\(\\Omega \\ne [-1, 1]\\)\nThe Galerkin method is also be referred to as a projection of \\(u(x)\\) onto \\(V_N\\)"
  },
  {
    "objectID": "fem.html#section",
    "href": "fem.html#section",
    "title": "Function approximation by the finite element method",
    "section": "",
    "text": "Finite element software\nDeveloped originally at Chalmers University of Technology and UiO\nVery flexible and easy to use\nSolves PDEs with many different finite elements, including Lagrange"
  },
  {
    "objectID": "fem.html#short-recap",
    "href": "fem.html#short-recap",
    "title": "Function approximation by the finite element method",
    "section": "Short recap",
    "text": "Short recap\nWe have considered the approximation of functions \\(u(x), x \\in \\Omega =[a, b]\\) using \\(u(x) \\approx u_N(x)\\) and\n\\[\nu_N(x) = \\sum_{i=0}^N \\hat{u}_i \\psi_i(x)\n\\]\n\n\\(\\psi_i(x)\\) have been global basis functions, defined on all of \\(\\Omega = [a, b]\\)\n\\(\\{\\hat{u}_i\\}_{i=0}^N\\) are the unknowns\n\nWe have found \\(\\{\\hat{u}_i\\}_{i=0}^N\\) using\n\nThe least squares method (variational)\nThe Galerkin method (variational)\nThe Collocation method (interpolation)"
  },
  {
    "objectID": "fem.html#advantages-and-disadvantages-of-the-global-methods",
    "href": "fem.html#advantages-and-disadvantages-of-the-global-methods",
    "title": "Function approximation by the finite element method",
    "section": "Advantages and disadvantages of the global methods",
    "text": "Advantages and disadvantages of the global methods\n\n\nAdvantages\n\nSpectral accuracy\nEfficient for orthogonal basis functions\n\n\nDisadvantages\n\nOnly feasible for simple meshes, like lines and rectangles\nInefficient for non-orthogonal basis functions\n\n\n\nImpossible to use for, e.g."
  },
  {
    "objectID": "fem.html#the-finite-element-method",
    "href": "fem.html#the-finite-element-method",
    "title": "Function approximation by the finite element method",
    "section": "The finite element method",
    "text": "The finite element method\nThe finite element method is a variational method using local basis functions\n\n\n5 global basis functions\n\n\n\n\n\n\n\n\n\n\n2 local basis functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe Galerkin formulation is the same whether you use a global approach with Legendre polynomials or a local FEM with piecewise polynomials. The difference lies all in the function spaces and the choice of basis.\n\n\n\nFind \\(u_N \\in V_N (=\\text{span}\\{\\psi_j\\}_{j=0}^N)\\) such that\n\\[\n(u-u_N, v) = 0\\quad \\forall \\, v \\in V_N\n\\]"
  },
  {
    "objectID": "fem.html#advantages-and-disadvantages-of-the-global-variational-methods",
    "href": "fem.html#advantages-and-disadvantages-of-the-global-variational-methods",
    "title": "Function approximation by the finite element method",
    "section": "Advantages and disadvantages of the global variational methods",
    "text": "Advantages and disadvantages of the global variational methods\n\n\nAdvantages\n\nSpectral accuracy\nEfficient for orthogonal basis functions\nNo mesh\n\n\nDisadvantages\n\nMainly feasible for simple domains, like lines and rectangles\nInefficient for non-orthogonal basis functions\n\n\n\nImpossible to use for unstructured meshes, like"
  },
  {
    "objectID": "fem.html#the-finite-element-method-is-a-variational-method-using-local-basis-functions",
    "href": "fem.html#the-finite-element-method-is-a-variational-method-using-local-basis-functions",
    "title": "Function approximation by the finite element method",
    "section": "The finite element method is a variational method using local basis functions",
    "text": "The finite element method is a variational method using local basis functions\n\n\n5 global basis functions\n\n\n\n\n\n\n\n\n\n\n2 local piecewise linear basis functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Galerkin formulation is the same whether you use a global approach with Legendre polynomials or a local FEM with piecewise linear polynomials. The difference lies all in the function spaces and the choice of basis.\n\n\n\nFind \\(u_N \\in V_N (=\\text{span}\\{\\psi_j\\}_{j=0}^N)\\) such that\n\\[\n(u-u_N, v) = 0\\quad \\forall \\, v \\in V_N\n\\]"
  },
  {
    "objectID": "fem.html#the-finite-element-mesh",
    "href": "fem.html#the-finite-element-mesh",
    "title": "Function approximation by the finite element method",
    "section": "The finite element mesh",
    "text": "The finite element mesh\nThe domain \\(\\Omega\\) is divided into \\(N_e\\) smaller, non-overlapping, subdomains \\(\\Omega^{(e)}\\), such that\n\\[\n\\Omega = \\bigcup_{e=0}^{N_e-1} \\Omega^{(e)}\n\\]\n\n\nThe smaller subdomains between the blue lines are referred to as elements.\nThe red dots are referred to as nodes, just like for interpolation methods."
  },
  {
    "objectID": "fem.html#there-can-be-nodes-inside-elements",
    "href": "fem.html#there-can-be-nodes-inside-elements",
    "title": "Function approximation by the finite element method",
    "section": "There can be nodes inside elements",
    "text": "There can be nodes inside elements\nAn element may contain many nodes.\n\nThe figure shows a mesh with 5 non-uniform nodes and 2 non-uniform elements"
  },
  {
    "objectID": "fem.html#there-can-be-many-nodes-inside-each-element",
    "href": "fem.html#there-can-be-many-nodes-inside-each-element",
    "title": "Function approximation by the finite element method",
    "section": "There can be many nodes inside each element",
    "text": "There can be many nodes inside each element\n\nThe figure shows a mesh with 5 non-uniform nodes and 2 non-uniform elements"
  },
  {
    "objectID": "fem.html#there-may-be-many-nodes-inside-each-element",
    "href": "fem.html#there-may-be-many-nodes-inside-each-element",
    "title": "Function approximation by the finite element method",
    "section": "There may be many nodes inside each element",
    "text": "There may be many nodes inside each element\n\nThe figure shows a mesh with 5 non-uniform nodes and 2 non-uniform elements\nUsing more nodes inside each element is how the FEM can achieve higher order accuracy"
  },
  {
    "objectID": "fem.html#finite-element-basis-functions",
    "href": "fem.html#finite-element-basis-functions",
    "title": "Function approximation by the finite element method",
    "section": "Finite element basis functions",
    "text": "Finite element basis functions\n\nAn element with no internal nodes can at best use piecewise linear basis functions\n\\[\n\\psi_j(x) = \\begin{cases}\n\\frac{x-x_{j-1}}{x_{j}-x_{j-1}} \\quad &x \\in [x_{j-1}, x_{j}],\\\\\n\\frac{x-x_{j+1}}{x_{j}-x_{j+1}} \\quad &x \\in [x_{j}, x_{j+1}],\\\\\n0 \\quad &\\text{otherwise},\n\\end{cases}\n\\]"
  },
  {
    "objectID": "fem.html#finite-element-assembly",
    "href": "fem.html#finite-element-assembly",
    "title": "Function approximation by the finite element method",
    "section": "Finite element assembly",
    "text": "Finite element assembly\nUse a continuous piecewise linear function space \\(V_N=\\text{span}\\{\\psi_j\\}_{j=0}^N\\), where\n\\[\n\\psi_j(x) = \\begin{cases}\n\\frac{x-x_{j-1}}{x_{j}-x_{j-1}} \\quad &x \\in [x_{j-1}, x_{j}],\\\\\n\\frac{x-x_{j+1}}{x_{j}-x_{j+1}} \\quad &x \\in [x_{j}, x_{j+1}],\\\\\n0 \\quad &\\text{otherwise},\n\\end{cases}\n\\]\nTo approximate a function \\(u(x), x \\in \\Omega = [a, b]\\), we can now:\nFind \\(u_N \\in V_N\\) such that\n\\[\n(u-u_N, v) = 0 \\quad \\forall \\, v \\in V_N\n\\]\nWe can still use \\(v=\\psi_i\\) and \\(u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x)\\), exactly like for the global Galerkin method and obtain:\n\\[\n\\sum_{j=0}^N (\\psi_j, \\psi_i) \\hat{u}_j = (u, \\psi_i), \\quad i=0,1,\\ldots, N\n\\]"
  },
  {
    "objectID": "fem.html#finite-element-assembly-ctd",
    "href": "fem.html#finite-element-assembly-ctd",
    "title": "Function approximation by the finite element method",
    "section": "Finite element assembly ctd",
    "text": "Finite element assembly ctd\nThe mass matrix \\(A = (a_{ij})_{i,j=0}^N\\) is\n\\[\na_{ij} = (\\psi_j, \\psi_i) = \\int_{\\Omega}\\psi_j, \\psi_i dx, \\quad (i, j) \\in (0, \\ldots, N) \\times (0, \\ldots, N)\n\\]\nHowever, since each basis function is only non-zero on at most two elements, we usually assemble elementwise\n\\[\na_{ij} = \\sum_{e=0}^{N_e-1} \\tilde{a}_{ij}^{(e)} =  \\sum_{e=0}^{N_e-1} \\int_{\\Omega^{(e)}} \\psi_j \\psi_i dx\n\\]\nWe define the element mass matrix \\(A^{(e)} = (\\tilde{a}^{(e)}_{ij})_{i,j=0}^N\\) as\n\\[\n\\tilde{a}^{(e)}_{ij} = \\int_{\\Omega^{(e)}} \\psi_j \\psi_i dx, \\quad\n\\]"
  },
  {
    "objectID": "fem.html#the-element-mass-matrix",
    "href": "fem.html#the-element-mass-matrix",
    "title": "Function approximation by the finite element method",
    "section": "The element mass matrix",
    "text": "The element mass matrix\nThe mass matrix \\(A = (a_{ij})_{i,j=0}^N\\) is\n\\[\na_{ij} = (\\psi_j, \\psi_i) = \\int_{\\Omega}\\psi_j \\psi_i dx, \\quad (i, j) \\in (0, \\ldots, N)^2\n\\]\nHowever, since each basis function is only non-zero on at most two elements, we usually assemble elementwise and add up (this works very well on unstructured meshes!)\n\\[\na_{ij} = \\sum_{e=0}^{N_e-1} {a}_{ij}^{(e)} =  \\sum_{e=0}^{N_e-1} \\int_{\\Omega^{(e)}} \\psi_j \\psi_i dx\n\\]\nWe define the element mass matrix \\(A^{(e)} = ({a}^{(e)}_{ij})_{i,j=0}^N\\) as\n\\[\n{a}^{(e)}_{ij} = \\int_{\\Omega^{(e)}} \\psi_j \\psi_i dx, \\quad (i, j) \\in (0, \\ldots, N)^2\n\\]\n\n\n\nThe finite element method is much more difficult to implement than global methods, because of the local basis functions and unstructured mesh. Yet, the unstructured mesh and local basis functions make the method much more flexible."
  },
  {
    "objectID": "fem.html#the-element-mass-matrix-is-highly-sparse",
    "href": "fem.html#the-element-mass-matrix-is-highly-sparse",
    "title": "Function approximation by the finite element method",
    "section": "The element mass matrix is highly sparse",
    "text": "The element mass matrix is highly sparse\n\\[\n{a}^{(e)}_{ij} = \\int_{\\Omega^{(e)}} \\psi_j \\psi_i dx, \\quad\n\\]\nFor piecewise linear basis functions there are only 2 non-zero basis functions per element. See element \\(\\Omega^{(2)}\\)\n\n\n\n\n\n\n\n\nThe matrix \\({A}^{(2)}\\) will have only 4 non-zero items. So it is really a waste of memory using an \\((N+1)\\times (N+1)\\) matrix."
  },
  {
    "objectID": "fem.html#use-a-local-dense-element-mass-matrix",
    "href": "fem.html#use-a-local-dense-element-mass-matrix",
    "title": "Function approximation by the finite element method",
    "section": "Use a local dense element mass matrix",
    "text": "Use a local dense element mass matrix\nWith \\(d+1\\) nonzero basis functions on element \\(e\\) all the non-zero items of \\(A^{(e)}\\) can be stored in the dense matrix:\n\\[\n\\tilde{A}^{(e)} = (\\tilde{a}_{rs}^{(e)})_{r,s=0}^d\n\\]\n\\[\n\\tilde{a}_{rs}^{(e)} = \\int_{\\Omega^{(e)}} \\psi_{q(e,r)} \\psi_{q(e,r)} dx\n\\]\n\n\n\n\n\n\nNote\n\n\nThe matrix \\(\\tilde{A}^{(e)}\\) contains the same nonzero items as \\(A^{(e)}\\), but \\(\\tilde{A}^{(e)}\\in \\mathbb{R}^{(d+1) \\times (d+1)}\\) is dense, whereas \\(A^{(e)} \\in \\mathbb{R}^{(N+1)\\times (N+1)}\\) is highly sparse."
  },
  {
    "objectID": "fem.html#local-to-global-mapping",
    "href": "fem.html#local-to-global-mapping",
    "title": "Function approximation by the finite element method",
    "section": "Local to global mapping",
    "text": "Local to global mapping\n\n\n\n\n\n\nThe 4 smaller matrices represent \\(\\tilde{A}^{(0)}, \\tilde{A}^{(1)}, \\tilde{A}^{(2)}\\) and \\(\\tilde{A}^{(3)}\\)\nThe large matrix is \\(A\\). For each element \\(e\\) the finite element assembly is to add up \\[\n\\quad a_{q(e,r),q(e,s)} \\mathrel{+}= \\tilde{a}^{(e)}_{r,s}\n\\]"
  },
  {
    "objectID": "fem.html#mapping-to-reference-domain",
    "href": "fem.html#mapping-to-reference-domain",
    "title": "Function approximation by the finite element method",
    "section": "Mapping to reference domain",
    "text": "Mapping to reference domain\nIn assembling the matrix \\(A\\) we need to compute the element matrix \\(\\tilde{A}^{(e)}\\) many times. Is this really necessary? The integrals\n\\[\n\\int_{\\Omega^{(e)}} \\psi_{q(e,r)} \\psi_{q(e,s)} d\\Omega,\n\\]\ndiffer only in the domain, whereas the shape of the basis functions is the same regardless of domain. The piecewise linear basis functions are always straight lines.\n\nLet us map all elements to a reference domain \\(\\Omega^r = [-1, 1]\\). The affine map from \\(x \\in \\Omega^{(e)} = [x_{q(e,0)}, x_{q(e, d)}] = [x_L, x_R]\\) to \\(X \\in \\Omega^r\\) can be written for any element as\n\\[\nx = \\frac{1}{2}(x_L+x_R) + \\frac{1}{2}(x_R-x_L)X\n\\]\nMapping back and forth is as usual\n\\[\nX(x) \\quad \\text{or} \\quad x(X)\n\\]"
  },
  {
    "objectID": "fem.html#mapping-finite-element-basis-functions",
    "href": "fem.html#mapping-finite-element-basis-functions",
    "title": "Function approximation by the finite element method",
    "section": "Mapping finite element basis functions",
    "text": "Mapping finite element basis functions\nThe basis functions \\(\\psi_{q(e,r)}(x)\\) are commonly mapped to the Lagrangian basis functions\n\\[\n\\psi_{q(e,r)}(x) = \\ell_r(X) = \\prod_{\\substack{0 \\le s \\le d \\\\ s \\ne r}} \\frac{X-X_s}{X_r-X_s}\n\\]\nwhere\n\\[\nX_r = -1 + \\frac{2r}{d}, \\quad r = 0, 1, \\ldots, d\n\\]\nand for piecewise linear basis functions (\\(d=1\\)) we get the following basis functions on the reference domain:\n\\[\n\\ell_0(X) = \\frac{1}{2}(1-X) \\quad \\text{and} \\quad \\ell_1(X) = \\frac{1}{2}(1+X)\n\\]"
  },
  {
    "objectID": "fem.html#quadratic-elements-d2",
    "href": "fem.html#quadratic-elements-d2",
    "title": "Function approximation by the finite element method",
    "section": "Quadratic elements (\\(d=2\\))",
    "text": "Quadratic elements (\\(d=2\\))\nFor quadratic elements the Lagrange basis functions on the reference domain are\n\\[\n(X_0, X_1, X_2) = (-1, 0, 1)\n\\]\n\\[\n\\ell_0(X) = \\frac{1}{2}X(1-X), \\quad\n\\ell_1(X) = (1-X^2), \\quad\n\\ell_2(X) = \\frac{1}{2}X(1+X)\n\\]"
  },
  {
    "objectID": "fem.html#d3",
    "href": "fem.html#d3",
    "title": "Function approximation by the finite element method",
    "section": "d=3",
    "text": "d=3\n\\[\nX_0, X_1, X_2, X_3 = -1, -1/3, 1/3, 1\n\\]\n\\[\n\\begin{align}\\small\n\\ell_0(X) &= -\\frac{9}{16}(X-1)(X-{1}/{3})(X+{1}/{3}) \\\\\n\\ell_1(X) &= \\frac{27}{16}(X-1)(X-{1}/{3})(X+{1}) \\\\\n\\ell_2(X) &= -\\frac{27}{16}(X-1)(X+{1}/{3})(X+1) \\\\\n\\ell_3(X) &= \\frac{9}{16}(X-{1}/{3})(X+{1}/{3})(X+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "fem.html#d3-and-x_0-x_1-x_2-x_3--1--13-13-1",
    "href": "fem.html#d3-and-x_0-x_1-x_2-x_3--1--13-13-1",
    "title": "Function approximation by the finite element method",
    "section": "d=3 and \\(X_0, X_1, X_2, X_3 = -1, -1/3, 1/3, 1\\)",
    "text": "d=3 and \\(X_0, X_1, X_2, X_3 = -1, -1/3, 1/3, 1\\)\n\\[\n\\begin{align}\\small\n\\ell_0(X) &= -\\frac{9}{16}(X-1)(X-{1}/{3})(X+{1}/{3}) \\\\\n\\ell_1(X) &= \\frac{27}{16}(X-1)(X-{1}/{3})(X+{1}) \\\\\n\\ell_2(X) &= -\\frac{27}{16}(X-1)(X+{1}/{3})(X+1) \\\\\n\\ell_3(X) &= \\frac{9}{16}(X-{1}/{3})(X+{1}/{3})(X+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "fem.html#d3-and-small-x_0-x_1-x_2-x_3--1--13-13-1",
    "href": "fem.html#d3-and-small-x_0-x_1-x_2-x_3--1--13-13-1",
    "title": "Function approximation by the finite element method",
    "section": "d=3 and \\(\\small (X_0, X_1, X_2, X_3) = (-1, -1/3, 1/3, 1)\\)",
    "text": "d=3 and \\(\\small (X_0, X_1, X_2, X_3) = (-1, -1/3, 1/3, 1)\\)\n\\[\n\\begin{matrix}\n\\ell_0(X) = -\\frac{9}{16}(X-1)(X-\\tfrac{1}{3})(X+\\tfrac{1}{3}) & \\ell_1(X) = \\frac{27}{16}(X-1)(X-\\tfrac{1}{3})(X+{1}) \\\\\n\\ell_2(X) = -\\frac{27}{16}(X-1)(X+\\tfrac{1}{3})(X+1) & \\ell_3(X) = \\frac{9}{16}(X-\\tfrac{1}{3})(X+\\tfrac{1}{3})(X+1)\n\\end{matrix}\n\\]"
  },
  {
    "objectID": "fem.html#back-to-the-element-matrix",
    "href": "fem.html#back-to-the-element-matrix",
    "title": "Function approximation by the finite element method",
    "section": "Back to the element matrix",
    "text": "Back to the element matrix\nUse a change of variables (\\(x\\rightarrow X\\) and \\(\\psi_{q(e,r)}(x)=\\ell_r(X)\\)) for the inner product:\n\\[\n\\begin{align*}\n\\tilde{a}^{(e)}_{rs} &= \\int_{\\Omega^{(e)}} \\psi_{q(e,r)}(x) \\psi_{q(e,s)}(x) d\\Omega, \\\\\n&= \\int_{x_L}^{x_R} \\psi_{q(e,r)}(x) \\psi_{q(e,s)}(x) dx, \\\\\n&= \\int_{-1}^1 \\ell_{r}(X) \\ell_{s}(X) \\frac{dx}{dX} dX,\n\\end{align*}\n\\]\nwhere \\(dx/dX = h(e)/2\\) and \\(h(e)=x_{q(e, d)}-x_{q(e, 0)}=x_R-x_L\\), such that for any element, regardless of order \\(d\\), we can compute the elements of the element matrix as\n\\[\n\\tilde{a}_{rs}^{(e)} = \\frac{h(e)}{2}\\int_{-1}^1 \\ell_{r}(X) \\ell_{s}(X) dX\n\\]\nNote that the integral does not depend on element number \\(e\\)!"
  },
  {
    "objectID": "fem.html#ma",
    "href": "fem.html#ma",
    "title": "Function approximation by the finite element method",
    "section": "ma",
    "text": "ma\nHence, instead of computing the element matrix for linear polynomials as\n\\[\n\\tilde{A}^{(e)} = \\begin{bmatrix}\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 0)} \\psi_{q(e, 0)} dx &\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 0)} \\psi_{q(e, 1)} dx \\\\\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 1)} \\psi_{q(e, 0)} dx &\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 1)} \\psi_{q(e, 1)} dx\n\\end{bmatrix},\n\\]\nwe can simply use\n\\[\n\\tilde{A}^{(e)} = \\frac{h}{2}\\begin{bmatrix}\n\\int_{-1}^1 \\ell_{0} \\ell_{0} dX &\n\\int_{-1}^1 \\ell_{0} \\ell_{1} dX \\\\\n\\int_{-1}^1 \\ell_{1} \\ell_{0} dX &\n\\int_{-1}^1 \\ell_{1} \\ell_{1} dX\n\\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "fem.html#since-the-integral-does-not-depend-on-the-element",
    "href": "fem.html#since-the-integral-does-not-depend-on-the-element",
    "title": "Function approximation by the finite element method",
    "section": "Since the integral does not depend on the element",
    "text": "Since the integral does not depend on the element\nthen, instead of computing for each (linear) element:\n\\[\n\\tilde{A}^{(e)} = \\begin{bmatrix}\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 0)} \\psi_{q(e, 0)} dx &\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 0)} \\psi_{q(e, 1)} dx \\\\\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 1)} \\psi_{q(e, 0)} dx &\n\\int_{\\Omega^{(e)}} \\psi_{q(e, 1)} \\psi_{q(e, 1)} dx\n\\end{bmatrix},\n\\]\nwe can simply use:\n\\[\n\\tilde{A}^{(e)} = \\frac{h(e)}{2}\\begin{bmatrix}\n\\int_{-1}^1 \\ell_{0} \\ell_{0} dX &\n\\int_{-1}^1 \\ell_{0} \\ell_{1} dX \\\\\n\\int_{-1}^1 \\ell_{1} \\ell_{0} dX &\n\\int_{-1}^1 \\ell_{1} \\ell_{1} dX\n\\end{bmatrix}.\n\\]\nwith merely a different \\(h(e)\\) for each element.\nSimilarly for higher \\(d\\)."
  },
  {
    "objectID": "fem.html#sympy-implementation",
    "href": "fem.html#sympy-implementation",
    "title": "Function approximation by the finite element method",
    "section": "Sympy implementation",
    "text": "Sympy implementation\nLinear (\\(d=1\\))\n\nh = sp.Symbol('h')\nl = Lagrangebasis([-1, 1])\nae = lambda r, s: sp.integrate(l[r]*l[s], (x, -1, 1))\nA1e = h/2*sp.Matrix([[ae(0, 0), ae(0, 1)],[ae(1, 0), ae(1, 1)]])\nA1e\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{h}{3} & \\frac{h}{6}\\\\\\frac{h}{6} & \\frac{h}{3}\\end{matrix}\\right]\\)\n\n\nQuadratic (\\(d=2\\))\n\nl = Lagrangebasis([-1, 0, 1])\nae = lambda r, s: sp.integrate(l[r]*l[s], (x, -1, 1))\nA1e = h/2*sp.Matrix(np.array([[ae(i, j) for i in range(3) for j in range(3)]]).reshape(3, 3))\nA1e\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{2 h}{15} & \\frac{h}{15} & - \\frac{h}{30}\\\\\\frac{h}{15} & \\frac{8 h}{15} & \\frac{h}{15}\\\\- \\frac{h}{30} & \\frac{h}{15} & \\frac{2 h}{15}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "fem.html#sympy-implementation-element-mass-matrix",
    "href": "fem.html#sympy-implementation-element-mass-matrix",
    "title": "Function approximation by the finite element method",
    "section": "Sympy implementation element mass matrix",
    "text": "Sympy implementation element mass matrix\nLinear (\\(d=1\\))\n\nh = sp.Symbol('h')\nl = Lagrangebasis([-1, 1])\nae = lambda r, s: sp.integrate(l[r]*l[s], (x, -1, 1))\nA1e = h/2*sp.Matrix([[ae(0, 0), ae(0, 1)],[ae(1, 0), ae(1, 1)]])\nA1e\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{h}{3} & \\frac{h}{6}\\\\\\frac{h}{6} & \\frac{h}{3}\\end{matrix}\\right]\\)\n\n\nQuadratic (\\(d=2\\))\n\nl = Lagrangebasis([-1, 0, 1])\nae = lambda r, s: sp.integrate(l[r]*l[s], (x, -1, 1))\nA2e = h/2*sp.Matrix(np.array([[ae(i, j) for i in range(3) for j in range(3)]]).reshape(3, 3))\nA2e\n\n\\(\\displaystyle \\left[\\begin{matrix}\\frac{2 h}{15} & \\frac{h}{15} & - \\frac{h}{30}\\\\\\frac{h}{15} & \\frac{8 h}{15} & \\frac{h}{15}\\\\- \\frac{h}{30} & \\frac{h}{15} & \\frac{2 h}{15}\\end{matrix}\\right]\\)"
  },
  {
    "objectID": "fem.html#complete-assembly-implementation",
    "href": "fem.html#complete-assembly-implementation",
    "title": "Function approximation by the finite element method",
    "section": "Complete assembly implementation",
    "text": "Complete assembly implementation\n\nAe = [A1e, A2e] # previously computed\n\ndef get_element_boundaries(xj, e, d=1):\n    return xj[d*e], xj[d*(e+1)]\n\ndef get_element_length(xj, e, d=1):\n    xL, xR = get_element_boundaries(xj, e, d=d)\n    return xR-xL\n\ndef local_to_global_map(e, r=None, d=1): # q(e, r)\n    if r is None:\n        return slice(d*e, d*(e+1)+1)\n    return d*e+r\n\ndef assemble_mass(xj, d=1):\n    N = len(xj)-1\n    Ne = N//d\n    A = np.zeros((N+1, N+1))\n    for elem in range(Ne):\n        hj = get_element_length(xj, elem, d=d)\n        s0 = local_to_global_map(elem, d=d)\n        A[s0, s0] += np.array(Ae[d-1].subs(h, hj), dtype=float)\n    return A\n\n\n\nN = 4\nxj = np.linspace(1, 2, N+1)\nA = assemble_mass(xj, d=1)\nprint(A)\n\n\n[[0.0833 0.0417 0.     0.     0.    ]\n [0.0417 0.1667 0.0417 0.     0.    ]\n [0.     0.0417 0.1667 0.0417 0.    ]\n [0.     0.     0.0417 0.1667 0.0417]\n [0.     0.     0.     0.0417 0.0833]]"
  },
  {
    "objectID": "fem.html#higher-order-mass-matrix",
    "href": "fem.html#higher-order-mass-matrix",
    "title": "Function approximation by the finite element method",
    "section": "Higher order mass matrix",
    "text": "Higher order mass matrix\n\nN = 8\nxj = np.linspace(1, 2, N+1)\nA = assemble_mass(xj, d=2)\nprint(A)\n\n[[ 0.0333  0.0167 -0.0083  0.      0.      0.      0.      0.      0.    ]\n [ 0.0167  0.1333  0.0167  0.      0.      0.      0.      0.      0.    ]\n [-0.0083  0.0167  0.0667  0.0167 -0.0083  0.      0.      0.      0.    ]\n [ 0.      0.      0.0167  0.1333  0.0167  0.      0.      0.      0.    ]\n [ 0.      0.     -0.0083  0.0167  0.0667  0.0167 -0.0083  0.      0.    ]\n [ 0.      0.      0.      0.      0.0167  0.1333  0.0167  0.      0.    ]\n [ 0.      0.      0.      0.     -0.0083  0.0167  0.0667  0.0167 -0.0083]\n [ 0.      0.      0.      0.      0.      0.      0.0167  0.1333  0.0167]\n [ 0.      0.      0.      0.      0.      0.     -0.0083  0.0167  0.0333]]\n\n\n\n\nSparsity pattern:\n\n\n\n\n\n\nNote\n\n\n\nThe internal nodes represent rows with only 3 nonzero items. The nodes on the boundary between two elements have rows containing 5 nonzero items.\nThe mass matrix is not diagonal, but it is sparse."
  },
  {
    "objectID": "fem.html#finite-element-assembly-of-a-vector",
    "href": "fem.html#finite-element-assembly-of-a-vector",
    "title": "Function approximation by the finite element method",
    "section": "Finite element assembly of a vector",
    "text": "Finite element assembly of a vector\nIn solving for\n\\[\n\\sum_{j=0}^N(\\psi_j, \\psi_i) \\hat{u}_j = (u, \\psi_i), \\quad i=0,1,\\ldots, N\n\\]\nwe also need the right hand side\n\\[\nb_i = (u, \\psi_i), \\quad i = 0, 1, \\ldots, N\n\\]\nThis inner product can also be evaluated elementwise, and mapped just like the mass matrix. We define the element vector similarly as the element matrix\n\\[\nb_i^{(e)} = \\int_{\\Omega^{(e)}} u(x) \\psi_{i}(x) dx, \\quad i = 0, 1, \\ldots, N\n\\]\n\\(b_i^{(e)}\\) will be highly sparse."
  },
  {
    "objectID": "fem.html#define-a-dense-local-vector",
    "href": "fem.html#define-a-dense-local-vector",
    "title": "Function approximation by the finite element method",
    "section": "Define a dense local vector",
    "text": "Define a dense local vector\n\\[\n\\tilde{b}^{(e)}_r = (u, \\psi_{q(e, r)}) = \\int_{\\Omega^{(e)}} u(x) \\psi_{q(e, r)}(x) dx, \\quad r = 0, 1, \\ldots, d\n\\]\nUsing as before \\(\\psi_{q(e, r)}(x)=\\ell_r(X)\\) we get a mapping to the reference domain\n\\[\n\\tilde{b}^{(e)}_r = \\frac{h(e)}{2}\\int_{-1}^1 u(x(X)) \\ell_{r}(X) dX, \\quad r = 0, 1, \\ldots, d\n\\]\n\n\n\n\n\n\nNote\n\n\nThe vector \\(\\boldsymbol{\\tilde{b}}^{(e)}\\) needs to be assembled with an integral for each element because of \\(u(x(X))\\)\n\n\n\nAssemble by adding up for all elements \\(e=0,1, \\ldots, N_e-1\\) and \\(r=0,1, \\ldots, d\\)\n\\[\nb_{q(e, r)} \\mathrel{+}= \\tilde{b}_r^{(e)}\n\\]"
  },
  {
    "objectID": "fem.html#implementation-small-boldsymbolbe",
    "href": "fem.html#implementation-small-boldsymbolbe",
    "title": "Function approximation by the finite element method",
    "section": "Implementation \\(\\small \\boldsymbol{b}^{(e)}\\)",
    "text": "Implementation \\(\\small \\boldsymbol{b}^{(e)}\\)\n\ndef map_true_domain(xj, e, d=1, x=x): # return x(X)\n    xL, xR = get_element_boundaries(xj, e, d=d)\n    hj = get_element_length(xj, e, d=d)\n    return (xL+xR)/2+hj*x/2\n\ndef map_reference_domain(xj, e, d=1, x=x): # return X(x)\n    xL, xR = get_element_boundaries(xj, e, d=d)\n    hj = get_element_length(xj, e, d=d)\n    return (2*x-(xL+xR))/hj\n\ndef map_u_true_domain(u, xj, e, d=1, x=x): # return u(x(X))\n    return u.subs(x, map_true_domain(xj, e, d=d, x=x))\n\ndef assemble_b(u, xj, d=1):\n    l = Lagrangebasis(np.linspace(-1, 1, d+1), sympy=False)\n    N = len(xj)-1\n    Ne = N//d\n    b = np.zeros(N+1)\n    for elem in range(Ne):\n        hj = get_element_length(xj, elem, d=d)\n        us = sp.lambdify(x, map_u_true_domain(u, xj, elem, d=d))\n        integ = lambda xj, r: us(xj)*l[r](xj)\n        for r in range(d+1):\n            b[local_to_global_map(elem, r, d)] += hj/2*quad(integ, -1, 1, args=(r,))[0]\n    return b\n\n\n\n\n\n\n\n\nNote\n\n\nWe need to perform an integral by calling quad for each \\(r\\) in each element."
  },
  {
    "objectID": "fem.html#example-small-ux-10x-12-1-x-in-1-2",
    "href": "fem.html#example-small-ux-10x-12-1-x-in-1-2",
    "title": "Function approximation by the finite element method",
    "section": "Example: \\(\\small u(x) = 10(x-1)^2-1, x \\in [1, 2]\\)",
    "text": "Example: \\(\\small u(x) = 10(x-1)^2-1, x \\in [1, 2]\\)\nUse the previously implemented assemble_mass and assemble_b to find the approximation of \\(u(x)\\) using piecewise linear functions and FEM:\n\n\ndef assemble(u, N, domain=(-1, 1), d=1, xj=None):\n    mesh = np.linspace(domain[0], domain[1], N+1) if xj is None else xj\n    A = assemble_mass(mesh, d=d)\n    b = assemble_b(u, mesh, d=d)\n    return A, b\n\nN = 4\nxj = np.linspace(1, 2, N+1)\nA, b = assemble(10*(x-1)**2-1, N, d=1, xj=xj)\nuh = np.linalg.inv(A) @ b\nyj = np.linspace(1, 2, 1000)\nplt.figure(figsize=(6, 3.5))\nplt.plot(xj, uh, 'b-o', yj, 10*(yj-1)**2-1, 'r--')\nplt.legend(['FEM', 'Exact']);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince we are using piecewise linear polynomials we can simply plot uh = \\((u_N(x_i))_{i=0}^N=(\\hat{u}_i)_{i=0}^N\\) and matplotlib will correctly fill in a linear profile between the points.\nThe FEM solution \\(u_N(x_i) \\ne u(x_i)\\)"
  },
  {
    "objectID": "fem.html#second-order-d2",
    "href": "fem.html#second-order-d2",
    "title": "Function approximation by the finite element method",
    "section": "Second order \\((d=2)\\)",
    "text": "Second order \\((d=2)\\)\nCheck that a non-uniform mesh works as well:\n\\[\nx_{2i} = 1+(\\cos(2 \\pi i / N) + 1)/2 \\quad \\text{and} \\quad x_{2i+1} = \\frac{x_{2i}+x_{2(i+1)}}{2}\n\\]\n\n\nN = 6\nxj = np.zeros(N+1)\nxj[::2] = 1 + (np.cos(np.arange(N//2+1)*np.pi*2/N)[::-1] + 1)/2\nxj[1::2] = 0.5*(xj[:-1:2]+xj[2::2])\nA, b = assemble(10*(x-1)**2-1, N, d=2, xj=xj)\nuh = np.linalg.inv(A) @ b\nyj = np.linspace(1, 2, 1000)\nplt.figure(figsize=(6, 4))\nplt.plot(xj, uh, '-bo', yj, 10*(yj-1)**2-1, 'r--')\nplt.plot(xj[1::2], uh[1::2], 'go')\nplt.legend(['FEM 2nd order', 'Exact', 'Internal points']);\n\n\n\n\n\n\n\n\n\n\nWhy still linear interpolation? We need to use the higher order \\(u_N(x) = \\sum_{j=0}^N\\hat{u}_j \\psi_j(x)\\) between mesh points! \\(\\rightarrow\\) FEM evaluation"
  },
  {
    "objectID": "fem.html#finite-element-evaluation",
    "href": "fem.html#finite-element-evaluation",
    "title": "Function approximation by the finite element method",
    "section": "Finite element evaluation",
    "text": "Finite element evaluation\nThe finite element solution differs from the finite difference solution in that the solution is automatically defined everywhere within the domain.\n\\[\nu_N(x) = \\sum_{j=0}^N\\hat{u}_j \\psi_j(x)\n\\]\nHowever, most basis functions will be zero at any location \\(x\\). We need to find which element \\(x\\) belongs to! And then evaluate only with non-zero basisfunctions\n\\[\nu_N(x) = \\sum_{r=0}^d \\hat{u}_{q(e, r)} \\ell_{r}(X), \\quad x \\in \\Omega^{(e)}\n\\]\n\ndef fe_evaluate(uh, p, xj, d=1):\n    l = Lagrangebasis(np.linspace(-1, 1, d+1), sympy=False)\n    elem = max(0, np.argmax(p &lt;= xj[::d])-1) # find element containing p\n    Xx = map_reference_domain(xj, elem, d=d, x=p)\n    return Lagrangefunction(uh[d*elem:d*(elem+1)+1], l)(Xx)\n\nfe_evaluate(uh, 1.2, xj, d=2), 10*(1.2-1)**2-1\n\n(-0.600000000000000, -0.6000000000000002)"
  },
  {
    "objectID": "fem.html#vectorized",
    "href": "fem.html#vectorized",
    "title": "Function approximation by the finite element method",
    "section": "Vectorized",
    "text": "Vectorized\n\ndef fe_evaluate_v(uh, pv, xj, d=1):\n    l = Lagrangebasis(np.linspace(-1, 1, d+1), sympy=False)\n    # Find points on element boundaries\n    elem = (np.argmax((pv &lt;= xj[::d, None]), axis=0)-1).clip(min=0)\n    xL = xj[:-1:d] # All left element boundaries\n    xR = xj[d::d]  # All right element boundaries\n    xm = (xL+xR)/2 # middle of all elements\n    hj = (xR-xL)   # length of all elements\n    Xx = 2*(pv-xm[elem])/hj[elem] # map pv to reference space all elements\n    dofs = np.array([uh[e*d+np.arange(d+1)] for e in elem], dtype=float)\n    V = np.array([lr(Xx) for lr in l], dtype=float) # All basis functions evaluated for all points\n    return np.sum(dofs * V.T, axis=1)\n\n\ndisplay(fe_evaluate_v(uh, np.array([1, 1.3]), xj, d=2))\ndisplay((10*(1-1)**2-1, 10*(1.3-1)**2-1)) \n\narray([-1. , -0.1])\n\n\n(-1, -0.09999999999999976)"
  },
  {
    "objectID": "fem.html#vectorized-for-n_d-points",
    "href": "fem.html#vectorized-for-n_d-points",
    "title": "Function approximation by the finite element method",
    "section": "Vectorized for \\(N_d\\) points",
    "text": "Vectorized for \\(N_d\\) points\n\\[\nu_N(x_i) = \\sum_{r=0}^d \\hat{u}_{q(e, r)} \\ell_{r}(X(x_i)), \\quad x \\in \\Omega^{(e)}, \\quad i=0, 1, \\ldots, N_d-1\n\\]\n\ndef fe_evaluate_v(uh, pv, xj, d=1):\n    l = Lagrangebasis(np.linspace(-1, 1, d+1), sympy=False)\n    # Find points inside elements\n    elem = (np.argmax((pv &lt;= xj[::d, None]), axis=0)-1).clip(min=0)\n    xL = xj[:-1:d] # All left element boundaries\n    xR = xj[d::d]  # All right element boundaries\n    xm = (xL+xR)/2 # middle of all elements\n    hj = (xR-xL)   # length of all elements\n    Xx = 2*(pv-xm[elem])/hj[elem] # map pv to reference space all elements\n    dofs = np.array([uh[e*d+np.arange(d+1)] for e in elem], dtype=float)\n    V = np.array([lr(Xx) for lr in l], dtype=float) # All basis functions evaluated for all points\n    return np.sum(dofs * V.T, axis=1)\n\n\ndisplay(fe_evaluate_v(uh, np.array([1, 1.3]), xj, d=2))\ndisplay((10*(1-1)**2-1, 10*(1.3-1)**2-1)) \n\narray([-1. , -0.1])\n\n\n(-1, -0.09999999999999976)"
  },
  {
    "objectID": "fem.html#evaluate-fem-for-n_d-points",
    "href": "fem.html#evaluate-fem-for-n_d-points",
    "title": "Function approximation by the finite element method",
    "section": "Evaluate FEM for \\(N_d\\) points",
    "text": "Evaluate FEM for \\(N_d\\) points\n\\[\nu_N(x_i) = \\sum_{r=0}^d \\hat{u}_{q(e, r)} \\ell_{r}(X(x_i)), \\quad x \\in \\Omega^{(e)}, \\quad i=0, 1, \\ldots, N_d-1\n\\]\nJust loop over scalar code for each point\n\ndef fe_evaluate_v(uh, pv, xj, d=1):\n    uj = np.zeros(len(pv))\n    for i, p in enumerate(pv):\n        uj[i] = fe_evaluate(uh, p, xj, d)\n\n\nAlternatively, use vectorization, but not really straightforward:\n\ndef fe_evaluate_v(uh, pv, xj, d=1):\n    l = Lagrangebasis(np.linspace(-1, 1, d+1), sympy=False)\n    # Find points inside elements\n    elem = (np.argmax((pv &lt;= xj[::d, None]), axis=0)-1).clip(min=0)\n    xL = xj[:-1:d] # All left element boundaries\n    xR = xj[d::d]  # All right element boundaries\n    xm = (xL+xR)/2 # middle of all elements\n    hj = (xR-xL)   # length of all elements\n    Xx = 2*(pv-xm[elem])/hj[elem] # map pv to reference space all elements\n    dofs = np.array([uh[e*d+np.arange(d+1)] for e in elem], dtype=float)\n    V = np.array([lr(Xx) for lr in l], dtype=float) # All basis functions evaluated for all points\n    return np.sum(dofs * V.T, axis=1)"
  },
  {
    "objectID": "fem.html#more-difficult-example-small-uxecos-x",
    "href": "fem.html#more-difficult-example-small-uxecos-x",
    "title": "Function approximation by the finite element method",
    "section": "More difficult example: \\(\\small u(x)=e^{\\cos x}\\)",
    "text": "More difficult example: \\(\\small u(x)=e^{\\cos x}\\)\nCompute \\(L^2(\\Omega)\\) error and compare with global Chebyshev and Legendre methods\n\n\n\ndef L2_error(uh, ue, xj, d=1):\n    yj = np.linspace(-1, 1, 4*len(xj))\n    uhj = fe_evaluate_v(uh, yj, xj, d=d)\n    uej = ue(yj)\n    return np.sqrt(np.trapz((uhj-uej)**2, dx=yj[1]-yj[0]))\n\nu = sp.exp(sp.cos(x))\nue = sp.lambdify(x, u)\nerr = []\nerr2 = []\nfor n in range(2, 30, 4):\n    N = 2*n\n    xj = np.linspace(-1, 1, N+1)\n    A, b = assemble(u, N, (-1, 1), 1)\n    uh = np.linalg.inv(A) @ b\n    A2, b2 = assemble(u, N, (-1, 1), 2)\n    uh2 = np.linalg.inv(A2) @ b2\n    err.append(L2_error(uh, ue, xj, 1))\n    err2.append(L2_error(uh2, ue, xj, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThis illustrates nicely spectral versus finite order accuracy. With \\(d=1\\) the FEM obtains second order accuracy and the error disappears as the linear (in the loglog-plot) green curve with slope \\(-2\\) (from error \\(\\sim N^{-2}\\)). The spectral error on the other hand disappears exponentially as \\(\\sim e^{-\\mu N}\\), faster than any finite order."
  },
  {
    "objectID": "fem.html#piecewise-linear-basis-functions-leads-to-piecewise-linear-small-u_nx",
    "href": "fem.html#piecewise-linear-basis-functions-leads-to-piecewise-linear-small-u_nx",
    "title": "Function approximation by the finite element method",
    "section": "Piecewise linear basis functions leads to piecewise linear \\(\\small u_N(x)\\)",
    "text": "Piecewise linear basis functions leads to piecewise linear \\(\\small u_N(x)\\)\n\n\n\n\n\n\n\n\n\nBut with FEM the solution is defined everywhere in the domain \\(\\Omega\\), not just in mesh points, like for interpolation methods."
  },
  {
    "objectID": "fem.html#piecewise-linear-basis-functions-leads-to-piecewise-linear-approximations-small-u_nx",
    "href": "fem.html#piecewise-linear-basis-functions-leads-to-piecewise-linear-approximations-small-u_nx",
    "title": "Function approximation by the finite element method",
    "section": "Piecewise linear basis functions leads to piecewise linear approximations \\(\\small u_N(x)\\)",
    "text": "Piecewise linear basis functions leads to piecewise linear approximations \\(\\small u_N(x)\\)\n\n\n\n\n\n\n\n\n\n\nWith FEM the solution \\(u_N(x)\\) is defined everywhere in the domain \\(\\Omega\\) and not just in mesh points, as for interpolation methods.\n\\(u_N(x) = \\sum_{j=0}^N\\hat{u}_j \\psi_j(x), \\quad x \\in \\Omega\\)"
  },
  {
    "objectID": "fem.html#piecewise-linear-basis-functions-lead-to-piecewise-linear-approximations-small-u_nx",
    "href": "fem.html#piecewise-linear-basis-functions-lead-to-piecewise-linear-approximations-small-u_nx",
    "title": "Function approximation by the finite element method",
    "section": "Piecewise linear basis functions lead to piecewise linear approximations \\(\\small u_N(x)\\)",
    "text": "Piecewise linear basis functions lead to piecewise linear approximations \\(\\small u_N(x)\\)\n\n\n\n\n\n\n\n\n\n\nWith FEM \\(u_N(x)\\) is defined everywhere in the domain \\(\\Omega\\) and not just in mesh points.\nInterpolation is not needed since \\(u_N(x) = \\sum_{j=0}^N\\hat{u}_j \\psi_j(x), \\quad x \\in \\Omega\\)."
  },
  {
    "objectID": "fem.html#the-finite-element-method-is-especially-well-suited-for-unstructured-meshes-in-complex-geometries",
    "href": "fem.html#the-finite-element-method-is-especially-well-suited-for-unstructured-meshes-in-complex-geometries",
    "title": "Function approximation by the finite element method",
    "section": "The finite element method is especially well suited for unstructured meshes in complex geometries",
    "text": "The finite element method is especially well suited for unstructured meshes in complex geometries\n\n\nStructured mesh\n\n\n\n\n\n\n\n\n\n\nUnstructured mesh\n\n\n\nBut in this course we will learn FEM using simple structured meshes."
  },
  {
    "objectID": "fem.html#the-fem-is-a-variational-method",
    "href": "fem.html#the-fem-is-a-variational-method",
    "title": "Function approximation by the finite element method",
    "section": "The FEM is a variational method",
    "text": "The FEM is a variational method\nUse a continuous piecewise linear function space \\(V_N=\\text{span}\\{\\psi_j\\}_{j=0}^N\\), where\n\\[\n\\psi_j(x) = \\begin{cases}\n\\frac{x-x_{j-1}}{x_{j}-x_{j-1}} \\quad &x \\in [x_{j-1}, x_{j}]\\\\\n\\frac{x-x_{j+1}}{x_{j}-x_{j+1}} \\quad &x \\in [x_{j}, x_{j+1}]\\\\\n0 \\quad &\\text{otherwise}\n\\end{cases}\n\\]\nTo approximate a function \\(u(x), x \\in \\Omega = [a, b]\\), we can now use the variational Galerkin method: Find \\(u_N \\in V_N\\) such that\n\\[\n(u-u_N, v) = 0 \\quad \\forall \\, v \\in V_N\n\\]\nWe can still use \\(v=\\psi_i\\) and \\(u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x)\\), exactly like for the global Galerkin method and obtain:\n\\[\n\\sum_{j=0}^N (\\psi_j, \\psi_i) \\hat{u}_j = (u, \\psi_i), \\quad i=0,1,\\ldots, N\n\\]"
  },
  {
    "objectID": "fem.html#the-local-to-global-map-qe-r-is-structured",
    "href": "fem.html#the-local-to-global-map-qe-r-is-structured",
    "title": "Function approximation by the finite element method",
    "section": "The local-to-global map \\(q(e, r)\\) is structured",
    "text": "The local-to-global map \\(q(e, r)\\) is structured\n\\[\nq(e, r) = de+r\n\\]"
  },
  {
    "objectID": "fem.html#the-local-to-global-map-qe-r-is-here-structured",
    "href": "fem.html#the-local-to-global-map-qe-r-is-here-structured",
    "title": "Function approximation by the finite element method",
    "section": "The local-to-global map \\(q(e, r)\\) is here structured",
    "text": "The local-to-global map \\(q(e, r)\\) is here structured\n\\[\nq(e, r) = de+r\n\\]\n\n\n\n\n\n\n\n\n\nNormally, the local-to-global map needs to be stored in a list or dictionary mapping element number to global nodes (\\(r\\) numbering is implicit):\n\\[\nq = \\Big \\{ \\begin{matrix}\n0: \\begin{bmatrix}\n0 \\\\\n1 \\\\\n2\n\\end{bmatrix} &\n1: \\begin{bmatrix}\n2 \\\\\n3 \\\\\n4\n\\end{bmatrix}\n&\n2: \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix}\n\\end{matrix}\n\\Big \\}\n\\]"
  },
  {
    "objectID": "fem.html#local-to-global-mapping-in-assembly-of-a",
    "href": "fem.html#local-to-global-mapping-in-assembly-of-a",
    "title": "Function approximation by the finite element method",
    "section": "Local to global mapping in assembly of \\(A\\)",
    "text": "Local to global mapping in assembly of \\(A\\)\n\n\n\n\n\nThe 4 smaller matrices represent \\(\\tilde{A}^{(0)}, \\tilde{A}^{(1)}, \\tilde{A}^{(2)}\\) and \\(\\tilde{A}^{(3)}\\)\nFinite element assembly: add up for \\(e=0,1,\\ldots, N_e-1\\) and \\((r,s) \\in (0,1,\\ldots, d)^2\\)\n\\[\n\\quad a_{q(e,r),q(e,s)} \\mathrel{+}= \\tilde{a}^{(e)}_{r,s}\n\\]"
  },
  {
    "objectID": "fem.html#fenics",
    "href": "fem.html#fenics",
    "title": "Function approximation by the finite element method",
    "section": "FEniCS",
    "text": "FEniCS"
  },
  {
    "objectID": "fem.html#first-example---function-approximation",
    "href": "fem.html#first-example---function-approximation",
    "title": "Function approximation by the finite element method",
    "section": "First example - function approximation",
    "text": "First example - function approximation\nimport numpy as np\nfrom mpi4py import MPI\nfrom dolfinx import mesh, fem, cpp\nfrom dolfinx.fem.petsc import LinearProblem\nimport ufl \nfrom ufl import dx, inner\n\nmsh = mesh.create_interval(MPI.COMM_SELF, 8, (-1, 1))\nV = fem.functionspace(msh, (\"Lagrange\", 1))\nV = fem.functionspace(msh, ufl_element)\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\nxp = ufl.SpatialCoordinate(msh)\nue = ufl.exp(ufl.cos(xp[0]))\na = inner(u, v) * dx\nL = inner(ue, v) * dx\nproblem = LinearProblem(a, L)\nuh = problem.solve()"
  },
  {
    "objectID": "fem.html#first-example---function-approximation-using-piecewise-linear-lagrange-elements",
    "href": "fem.html#first-example---function-approximation-using-piecewise-linear-lagrange-elements",
    "title": "Function approximation by the finite element method",
    "section": "First example - function approximation using piecewise linear Lagrange elements",
    "text": "First example - function approximation using piecewise linear Lagrange elements\nfrom mpi4py import MPI\nfrom dolfinx import mesh, fem, cpp\nfrom dolfinx.fem.petsc import LinearProblem\nimport ufl \nfrom ufl import dx, inner\n\nmsh = mesh.create_interval(MPI.COMM_SELF, 4, (-1, 1))\nV = fem.functionspace(msh, (\"Lagrange\", 1))\nu = ufl.TrialFunction(V)\nv = ufl.TestFunction(V)\nxp = ufl.SpatialCoordinate(msh)\nue = ufl.exp(ufl.cos(xp[0]))\na = inner(u, v) * dx\nL = inner(ue, v) * dx\nproblem = LinearProblem(a, L)\nuh = problem.solve()\n\nAlternatively assemble and solve linear problem yourself:\nfrom scipy.sparse.linalg import spsolve\nA = fem.assemble_matrix(fem.form(a))\nb = fem.assemble_vector(fem.form(L))\nuh = fem.Function(V)\nuh.x.array[:] = spsolve(A.to_scipy(), b.array)"
  },
  {
    "objectID": "fem.html#result-4-piecewise-linear-basis-functions",
    "href": "fem.html#result-4-piecewise-linear-basis-functions",
    "title": "Function approximation by the finite element method",
    "section": "Result 4 piecewise linear basis functions",
    "text": "Result 4 piecewise linear basis functions\n\n\nFEniCS\nN = 100\nxj = np.zeros((N, 3))\nxj[:, 0] = np.linspace(-1, 1, N)\ndata = cpp.geometry.determine_point_ownership(msh._cpp_object, xj, 1e-8)\nplt.plot(xj[:, 0], uh.eval(xj, data.dest_cells), 'b')\nplt.plot(xj[:, 0], sp.lambdify(x, sp.exp(sp.cos(x)))(xj[:, 0]), 'r')\n\n\n\n\n\n\nOur implementation\n\nN = 4 \nA1, b1 = assemble(u, N, (-1, 1), 1)\nuN = np.linalg.inv(A1) @ b1\nplt.figure(figsize=(5.5, 3.8))\nplt.plot(np.linspace(-1, 1, N+1), uN, 'b')\nxj = np.linspace(-1, 1, 100)\nplt.plot(xj, sp.lambdify(x, u)(xj), 'r')"
  },
  {
    "objectID": "fem.html#fenics-can-also-use-legendre-polynomials",
    "href": "fem.html#fenics-can-also-use-legendre-polynomials",
    "title": "Function approximation by the finite element method",
    "section": "FEniCS can also use Legendre polynomials",
    "text": "FEniCS can also use Legendre polynomials"
  },
  {
    "objectID": "fem.html#the-same-result-for-the-same-matrix",
    "href": "fem.html#the-same-result-for-the-same-matrix",
    "title": "Function approximation by the finite element method",
    "section": "The same result for the same matrix",
    "text": "The same result for the same matrix\nFEniCS uses exactly the same method with piecewise linear basis functions as we have described using Sympy/Numpy and as such we get exactly the same matrix\n\n\nFEniCS\nA.to_dense()\n\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\nb.array\n\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\nuh.x.array\n\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])\n\n\n\nSympy/Numpy\n\nA1\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\n\nb1\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\n\nuN\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])"
  },
  {
    "objectID": "fem.html#exactly-the-same-result-for-the-same-matrix",
    "href": "fem.html#exactly-the-same-result-for-the-same-matrix",
    "title": "Function approximation by the finite element method",
    "section": "Exactly the same result for the same matrix",
    "text": "Exactly the same result for the same matrix\nFEniCS uses exactly the same method with piecewise linear basis functions as we have described using Sympy/Numpy and as such we get exactly the same matrix\n\n\nFEniCS\nA.to_dense()\n\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\nb.array\n\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\nuh.x.array\n\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])\n\n\n\nSympy/Numpy\n\nA1\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\n\nb1\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\n\nuN\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])"
  },
  {
    "objectID": "fem.html#exactly-the-same-result-for-the-same-method",
    "href": "fem.html#exactly-the-same-result-for-the-same-method",
    "title": "Function approximation by the finite element method",
    "section": "Exactly the same result for the same method",
    "text": "Exactly the same result for the same method\nFEniCS uses exactly the same method with piecewise linear basis functions as we have described using Sympy/Numpy and as such we get exactly the same matrix/vectors:\n\n\nFEniCS\nA.to_dense()\n\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\nb.array\n\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\nuh.x.array\n\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])\n\n\n\nSympy/Numpy\n\nA1\n\narray([[0.1667, 0.0833, 0.    , 0.    , 0.    ],\n       [0.0833, 0.3333, 0.0833, 0.    , 0.    ],\n       [0.    , 0.0833, 0.3333, 0.0833, 0.    ],\n       [0.    , 0.    , 0.0833, 0.3333, 0.0833],\n       [0.    , 0.    , 0.    , 0.0833, 0.1667]])\n\n\n\nb1\n\narray([0.4892, 1.1865, 1.3317, 1.1865, 0.4892])\n\n\n\nuN\n\narray([1.7169, 2.4361, 2.7772, 2.4361, 1.7169])"
  },
  {
    "objectID": "fem.html#installation",
    "href": "fem.html#installation",
    "title": "Function approximation by the finite element method",
    "section": "Installation",
    "text": "Installation\nSee https://github.com/FEniCS/dolfinx\nAnaconda\nconda create -c conda-forge --name fenics fenics-dolfinx mpich pyvista\nLinux\nsudo add-apt-repository ppa:fenics-packages/fenics\nsudo apt update\nsudo apt install fenicsx\nDocker\ndocker run -ti dolfinx/dolfinx:stable"
  },
  {
    "objectID": "fem.html#for-installation-see-httpsgithub.comfenicsdolfinx",
    "href": "fem.html#for-installation-see-httpsgithub.comfenicsdolfinx",
    "title": "Function approximation by the finite element method",
    "section": "For installation see https://github.com/FEniCS/dolfinx",
    "text": "For installation see https://github.com/FEniCS/dolfinx\nAnaconda\nconda create -c conda-forge --name fenics fenics-dolfinx mpich pyvista\nLinux\nsudo add-apt-repository ppa:fenics-packages/fenics\nsudo apt update\nsudo apt install fenicsx\nDocker\ndocker run -ti dolfinx/dolfinx:stable"
  },
  {
    "objectID": "fem.html#for-installation-httpsgithub.comfenicsdolfinx",
    "href": "fem.html#for-installation-httpsgithub.comfenicsdolfinx",
    "title": "Function approximation by the finite element method",
    "section": "For installation: https://github.com/FEniCS/dolfinx",
    "text": "For installation: https://github.com/FEniCS/dolfinx\nAnaconda\nconda create -c conda-forge --name fenics fenics-dolfinx mpich pyvista\nLinux\nsudo add-apt-repository ppa:fenics-packages/fenics\nsudo apt update\nsudo apt install fenicsx\nDocker\ndocker run -ti dolfinx/dolfinx:stable"
  },
  {
    "objectID": "fem.html#define-a-local-to-global-map-qe-r",
    "href": "fem.html#define-a-local-to-global-map-qe-r",
    "title": "Function approximation by the finite element method",
    "section": "Define a local-to-global map \\(q(e, r)\\)",
    "text": "Define a local-to-global map \\(q(e, r)\\)\n\\[\nq(e, r) = de+r\n\\]\n\n\n\n\n\n\n\n\n\nMapping local index \\(r \\in (0, \\ldots, d)\\) on global element \\(e\\) to the global index \\(q(e, r) \\in (0, 1, \\ldots, N)\\). There are \\(d+1\\) nodes per element.\n\nFor unstructured meshed \\(q(e, r)\\) needs to be stored explicitly (\\(r\\) numbering is implicit):\n\\[\nq = \\Big \\{ \\begin{matrix}\n0: \\begin{bmatrix}\n0 \\\\\n1 \\\\\n2\n\\end{bmatrix} &\n1: \\begin{bmatrix}\n2 \\\\\n3 \\\\\n4\n\\end{bmatrix}\n&\n2: \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix}\n\\end{matrix}\n\\Big \\}\n\\]"
  },
  {
    "objectID": "fem.html#summary",
    "href": "fem.html#summary",
    "title": "Function approximation by the finite element method",
    "section": "Summary",
    "text": "Summary\n\n\nThe finite element method (FEM) is a variational method using local basis functions.\nThe FEM uses the same Galerkin method as the methods using global basis functions.\nThe FEM is assembled by running over all elements and assembling local matrices and vectors that are subsequently added to global matrices and vectors.\nSince all assembly work is performed elementwise, the FEM is very well suited for unstructured meshes."
  },
  {
    "objectID": "pdes.html#the-method-of-weighted-residuals",
    "href": "pdes.html#the-method-of-weighted-residuals",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The method of weighted residuals",
    "text": "The method of weighted residuals\nis defined such that\n\\[\n(\\mathcal{R}_N, v) = 0 \\quad \\forall \\, v \\in W\n\\]\nfor some (possibly different) functionspace \\(W\\).\nThis is a generic method, where the choice of \\(V_N\\) and \\(W\\) fully determines the method. Note the similarity to function approximation\n\\[\n(u_N - u, v) = 0\n\\]\nNow we have instead\n\\[\n(\\mathcal{L}(u_N)-f, v)=0\n\\]"
  },
  {
    "objectID": "pdes.html#we-will-now-learn-to-approximate-ux-such-that-it-approximates-a-linear-equation",
    "href": "pdes.html#we-will-now-learn-to-approximate-ux-such-that-it-approximates-a-linear-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "We will now learn to approximate \\(u(x)\\) such that it approximates a linear equation",
    "text": "We will now learn to approximate \\(u(x)\\) such that it approximates a linear equation\nConsider any differential equation with operator \\(\\mathcal{L}\\)\n\\[\n\\mathcal{L}(u) = f\n\\]\nFor example\n\\[\n\\begin{align*}\nu  &= f \\\\\nu' &= f \\\\\nu'' &= f \\\\\nu'' +\\alpha u' + \\lambda u  &= f \\\\\n\\frac{d}{dx}\\left(\\alpha \\frac{d u}{dx}\\right) &= f\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pdes.html#we-will-now-learn-to-approximate-ux-by-u_nx-such-that-it-approximates-a-linear-equation",
    "href": "pdes.html#we-will-now-learn-to-approximate-ux-by-u_nx-such-that-it-approximates-a-linear-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "We will now learn to approximate \\(u(x)\\) by \\(u_N(x)\\) such that it approximates a linear equation",
    "text": "We will now learn to approximate \\(u(x)\\) by \\(u_N(x)\\) such that it approximates a linear equation\nDefine any linear differential equation as\n\\[\n\\mathcal{L}(u) = f\n\\]\nwhere the generic operator \\(\\mathcal{L}\\) can represent anything, like\n\\[\n\\begin{align*}\nu  &= f \\\\\nu' &= f \\\\\nu'' &= f \\\\\nu'' +\\alpha u' + \\lambda u  &= f \\\\\n\\frac{d}{dx}\\left(\\alpha \\frac{d u}{dx}\\right) &= f\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pdes.html#we-will-now-learn-to-approximate-small-ux-by-small-u_nx-such-that-it-approximates-a-linear-equation",
    "href": "pdes.html#we-will-now-learn-to-approximate-small-ux-by-small-u_nx-such-that-it-approximates-a-linear-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "We will now learn to approximate \\(\\small u(x)\\) by \\(\\small u_N(x)\\) such that it approximates a linear equation",
    "text": "We will now learn to approximate \\(\\small u(x)\\) by \\(\\small u_N(x)\\) such that it approximates a linear equation\n\\[\n\\mathcal{L}(u) = f\n\\]\nwhere the generic operator \\(\\mathcal{L}(u)\\) can represent anything, like\n\\[\n\\begin{align*}\nu  &= f \\\\\nu' &= f \\\\\nu'' &= f \\\\\nu'' +\\alpha u' + \\lambda u  &= f \\\\\n\\frac{d}{dx}\\left(\\alpha \\frac{d u}{dx}\\right) &= f\n\\end{align*}\n\\]\nUp until now we have only considered function approximations (\\(u=f\\)) such that\n\\[\nu(x) \\approx u_N(x)\n\\]"
  },
  {
    "objectID": "pdes.html#the-residual",
    "href": "pdes.html#the-residual",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The residual",
    "text": "The residual\nDefine a residual that we ultimately want to be zero\n\\[\n\\mathcal{R} = \\mathcal{L}(u)-f\n\\]\nUse as always an approximation to \\(u(x)\\) that is \\(u_N \\in V_N\\) for some functionspace \\(V_N\\)\n\\[\nu_N(x) = \\sum_{j=0}^N\\hat{u}_j \\psi_j\n\\]\nand create a “numerical” residual\n\\[\n\\mathcal{R}_N = \\mathcal{L}(u_N)-f\n\\]\nwhere the unknowns are still \\(\\{\\hat{u}_j\\}_{j=0}^N\\)."
  },
  {
    "objectID": "pdes.html#the-galerkin-method",
    "href": "pdes.html#the-galerkin-method",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The Galerkin method",
    "text": "The Galerkin method\nis as such a MWR where \\(W = V_N\\).\nLikewise, it can be shown that the least squares method\n\\[\n\\frac{\\partial (\\mathcal{R_N}, \\mathcal{R}_N)}{\\partial \\hat{u}_j} = 0, \\quad j = 0, 1, \\ldots, N,\n\\]"
  },
  {
    "objectID": "pdes.html#the-method-of-weighted-residuals-mwr",
    "href": "pdes.html#the-method-of-weighted-residuals-mwr",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The method of weighted residuals (MWR)",
    "text": "The method of weighted residuals (MWR)\nis defined such that the residual must satisfy\n\\[\n(\\mathcal{R}_N, v) = 0 \\quad \\forall \\, v \\in W\n\\]\nfor some (possibly different) functionspace \\(W\\).\nThis is a generic method, where the choice of \\(V_N\\) and \\(W\\) fully determines the method.\n\nNote the similarity to function approximation\n\\[\n(u_N - u, v) = (e, v) = 0\n\\]\nNow we have instead the slightly more complicated\n\\[\n(\\mathcal{L}(u_N)-f, v) = (\\mathcal{R}_N, v) = 0\n\\]"
  },
  {
    "objectID": "pdes.html#the-galerkin-method-is-a-mwr-with-wv_n",
    "href": "pdes.html#the-galerkin-method-is-a-mwr-with-wv_n",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The Galerkin method is a MWR with \\(W=V_N\\)",
    "text": "The Galerkin method is a MWR with \\(W=V_N\\)\nFind \\(u_N \\in V_N\\) such that\n\\[\n(\\mathcal{R}_N-f, v)=(\\mathcal{L}(u_N)-f, v)=0, \\quad \\forall \\, v \\in V_N\n\\]\nThe residual (or error) is orthogonal to all the test functions. We can also write this as\n\\[\n(\\mathcal{L}(u_N)-f, \\psi_j)=0, \\quad j=0,1,\\ldots, N\n\\]\n\n\nThe least squares method is a MWR with \\(\\small W=\\text{span}\\{\\frac{\\partial \\mathcal{R}_N}{\\partial \\hat{u}_j}\\}_{j=0}^N\\)\nsince \\[\n\\frac{\\partial (\\mathcal{R_N}, \\mathcal{R}_N)}{\\partial \\hat{u}_j} = 0, \\quad j = 0, 1, \\ldots, N\n\\]\ncan be written as\n\\[\n\\left(\\mathcal{R}_N, \\frac{\\partial \\mathcal{R}_N}{\\partial \\hat{u}_j}\\right)=0, \\quad j = 0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "pdes.html#the-collocation-method",
    "href": "pdes.html#the-collocation-method",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The collocation method",
    "text": "The collocation method\n\\[\n\\mathcal{R}_N(x_j) = 0, \\quad j = 0, 1, \\ldots, N\n\\]\ncan also be understood as a MWR. If we write the inner products as\n\\[\n(\\mathcal{R}_N, \\psi_j) = 0, \\quad j=0, 1, \\ldots, N\n\\]\nand use Dirac’s delta function as test functions \\(\\psi_j = \\delta(x-x_j)\\), then\n\\[\n(\\mathcal{R}_N, \\psi_j) = \\int_{\\Omega} \\mathcal{R}_N(x) \\delta(x-x_j) dx = \\mathcal{R}_N(x_j)\n\\]"
  },
  {
    "objectID": "pdes.html#the-collocation-method-is-to-find",
    "href": "pdes.html#the-collocation-method-is-to-find",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The collocation method is to find",
    "text": "The collocation method is to find\n\\[\n\\mathcal{R}_N(x_j) = 0, \\quad j = 0, 1, \\ldots, N\n\\]\nfor some chosen mesh points \\(\\{x_j\\}_{j=0}^N\\).\n\nIf we write the inner products \\((\\mathcal{R}_N, v)\\) as\n\\[\n(\\mathcal{R}_N, \\psi_j) = 0, \\quad j=0, 1, \\ldots, N\n\\]\nand use Dirac’s delta function as test functions \\(\\psi_j = \\delta(x-x_j)\\), then\n\\[\n(\\mathcal{R}_N, \\psi_j) = \\int_{\\Omega} \\mathcal{R}_N(x) \\delta(x-x_j) dx = \\mathcal{R}_N(x_j)\n\\]\nSo the collocation method can technically also be considered a MWR!"
  },
  {
    "objectID": "pdes.html#the-mwr-is-thus",
    "href": "pdes.html#the-mwr-is-thus",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The MWR is thus",
    "text": "The MWR is thus\nFind \\(u_N \\in V_N\\) such that\n\\[\n(\\mathcal{R}_N, v) = 0, \\quad \\forall v \\in W\n\\]\n\\(N+1\\) equations for \\(N+1\\) unknowns!\nFind \\(\\{\\hat{u}_j\\}_{j=0}^N\\) by choosing \\(N+1\\) test functions for \\(W\\). Choose test functions (basis functions for \\(W\\)) using either one of:\n\nGalerkin\nLeast squares\nCollocation"
  },
  {
    "objectID": "pdes.html#first-example---poissons-equation",
    "href": "pdes.html#first-example---poissons-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "First example - Poisson’s equation",
    "text": "First example - Poisson’s equation\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu(-1) &= u(1) = 0\n\\end{align}\n\\]\nFind \\(u_N \\in V_N\\) such that\n\\[\n(u_N''-f, v) = 0, \\quad \\forall \\, v \\in W\n\\]\nHow to choose \\(V_N\\) and \\(W\\)? How do we satisfy the 2 boundary conditions?\n\nIf we choose all the trial functions \\(\\psi_j\\) such that\n\\[\n\\psi_j(\\pm 1) = 0\n\\]\nthen, regardless the values of \\(\\{\\hat{u}_j\\}_{j=0}^N\\)\n\\[\nu_N(\\pm 1) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(\\pm 1) = 0\n\\]"
  },
  {
    "objectID": "pdes.html#possible-choices-for-trial-functions-in-v_n",
    "href": "pdes.html#possible-choices-for-trial-functions-in-v_n",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Possible choices for trial functions in \\(V_N\\)",
    "text": "Possible choices for trial functions in \\(V_N\\)\nThe domain is \\([-1, 1]\\), so a sensible choice for \\(j=0, 1, \\ldots\\) is\n\\[\n\\psi_j(x) = \\sin(\\pi (j+1) (x+1)/2)\n\\]\nThe \\((j+1)\\) is there because we start at \\(j=0\\) and \\(\\sin(0)=0\\) is not a basis function. The sines are alternating odd/even functions.\n\n\n\n\n\n\n\n\n\n\n\n\nIn comparison the (unmapped) sine functions \\(\\sin(\\pi(j+1)x), x \\in [-1, 1]\\) are odd for all integer \\(j \\ge 0\\)"
  },
  {
    "objectID": "pdes.html#section-1",
    "href": "pdes.html#section-1",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "",
    "text": "Remember that\n\\[\nP_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1) = 1.\n\\]\nHence for any \\(j\\) all basis functions are zero\n\\[\n\\begin{align}\nP_j(-1) - P_{j+2}(-1) &= (-1)^j - (-1)^{j+2} = 0 \\\\\nP_j(1)-P_{j+2}(1) & =1-1=0\n\\end{align}\n\\]"
  },
  {
    "objectID": "pdes.html#another-basis-can-be-created-from-legendre-polynomials",
    "href": "pdes.html#another-basis-can-be-created-from-legendre-polynomials",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Another basis can be created from Legendre polynomials",
    "text": "Another basis can be created from Legendre polynomials\n\n\n\\[\n\\psi_j(x) = P_j(x) - P_{j+2}(x)\n\\]\nAlternating odd/even\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that\n\\[\nP_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1) = 1.\n\\]\nHence for any \\(j\\) all basis functions are zero\n\\[\n\\begin{align}\nP_j(-1) - P_{j+2}(-1) &= (-1)^j - (-1)^{j+2} = 0 \\\\\nP_j(1)-P_{j+2}(1) & =1-1=0\n\\end{align}\n\\]"
  },
  {
    "objectID": "pdes.html#another-alternating-oddeven-basis-can-be-created-from-legendre-polynomials",
    "href": "pdes.html#another-alternating-oddeven-basis-can-be-created-from-legendre-polynomials",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Another alternating odd/even basis can be created from Legendre polynomials",
    "text": "Another alternating odd/even basis can be created from Legendre polynomials\n\n\n\\[\n\\begin{align}\n\\psi_j(x) &= P_j(x) - P_{j+2}(x)\\\\\n\\psi_j(\\pm 1) &= 0\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that\n\\[\nP_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1) = 1.\n\\]\nHence for any \\(j\\) all basis functions are zero\n\\[\n\\begin{align}\nP_j(-1) - P_{j+2}(-1) &= (-1)^j - (-1)^{j+2} = 0 \\\\\nP_j(1)-P_{j+2}(1) & =1-1=0\n\\end{align}\n\\]"
  },
  {
    "objectID": "pdes.html#first-choice-for-trial-functions-in-v_n",
    "href": "pdes.html#first-choice-for-trial-functions-in-v_n",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "First choice for trial functions in \\(V_N\\)",
    "text": "First choice for trial functions in \\(V_N\\)\nThe domain is \\([-1, 1]\\), so a sensible choice for \\(j\\ge 0\\) is\n\\[\n\\psi_j(x) = \\sin(\\pi (j+1) (x+1)/2)\n\\]\nThe \\((j+1)\\) is there because we start at \\(j=0\\) and \\(\\sin(0)=0\\) is not a basis function. These sines are also alternating odd/even functions.\n\n\n\n\n\n\n\n\n\n\n\n\nIn comparison the (unmapped) sine functions \\(\\sin(\\pi(j+1)x), x \\in [-1, 1]\\) are odd for all integer \\(j \\ge 0\\)"
  },
  {
    "objectID": "pdes.html#solve-poissons-equation",
    "href": "pdes.html#solve-poissons-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve Poisson’s equation",
    "text": "Solve Poisson’s equation\nInsert for \\(u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j\\) and \\(v=\\psi_i\\) and obtain the linear algebra problem\n\\[\n\\sum_{j=0}^N \\left(\\psi''_j, \\psi_i  \\right) \\hat{u}_j = (f, \\psi_i), \\quad i = 0, 1, \\ldots, N\n\\]\n\n\nConsider using integration by parts\n\\[\n\\int_{a}^b u' v dx = -\\int_a^b u v' dx + [u v]_{a}^b\n\\]\nSet \\(u=u'\\) to obtain\n\\[\n\\int_{a}^b u'' v dx = -\\int_a^b u' v' dx + [u' v]_{a}^b\n\\]\n\n\n\\[\n\\longrightarrow \\left(\\psi''_j, \\psi_i  \\right) = -\\left( \\psi'_j, \\psi'_i \\right) + [\\psi'_j \\psi_i]_{-1}^1\n\\]"
  },
  {
    "objectID": "pdes.html#poissons-equation-with-integration-by-parts",
    "href": "pdes.html#poissons-equation-with-integration-by-parts",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Poisson’s equation with integration by parts",
    "text": "Poisson’s equation with integration by parts\nSince \\(\\psi_j(\\pm 1) = 0\\) for all \\(j\\ge 0\\) we get that\n\\[\n\\left(\\psi''_j, \\psi_i  \\right) = -\\left( \\psi'_j, \\psi'_i \\right) + \\cancel{[\\psi'_j \\psi_i]_{-1}^1}\n\\]\nHence Poisson’s equation gets two alternative forms\n\\[\n\\sum_{j=0}^N \\left(\\psi''_j, \\psi_i  \\right) \\hat{u}_j = (f, \\psi_i), \\quad i = 0, 1, \\ldots, N \\tag{1}\n\\]\n\\[\n\\sum_{j=0}^N \\left(\\psi'_j, \\psi'_i  \\right) \\hat{u}_j = -(f, \\psi_i), \\quad i = 0, 1, \\ldots, N \\tag{2}\n\\]\n\n\n\n\n\n\nNote\n\n\nThe integration by parts is not really necessary here, as it is actually just as easy to compute \\(\\left(\\psi''_j, \\psi_i  \\right)\\) as \\(\\left(\\psi'_j, \\psi'_i  \\right)\\)!"
  },
  {
    "objectID": "pdes.html#find-the-stiffness-matrix-psi_j-psi_i",
    "href": "pdes.html#find-the-stiffness-matrix-psi_j-psi_i",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Find the stiffness matrix \\((\\psi''_j, \\psi_i)\\)",
    "text": "Find the stiffness matrix \\((\\psi''_j, \\psi_i)\\)\n\\[\n\\begin{align*}\n(\\psi''_j, \\psi_i) &= \\Big((\\sin( \\pi (j+1)  (x+1)/2))'', \\, \\sin(\\pi (i+1)(x+1)/2) \\Big) \\\\\n    &= -\\frac{(j+1)^2\\pi^2}{4} \\Big(\\sin(\\pi (j+1)(x+1)/2), \\, \\sin(\\pi (i+1) (x+1)/2) \\Big) \\\\\n    &= -\\frac{(j+1)^2 \\pi^2}{4} \\delta_{ij}\n\\end{align*}\n\\]\n\nSolve problem\n\\[\n\\sum_{j=0}^N \\left(\\psi''_j, \\psi_i  \\right) \\hat{u}_j = (f, \\psi_i), \\quad i = 0, 1, \\ldots, N\n\\]\n\\[\n\\longrightarrow \\hat{u}_i = \\frac{-4}{(i+1)^2 \\pi^2}\\Big(f,\\, \\sin( \\pi (i+1)(x+1)/2)\\Big), \\quad i = 0, 1, \\ldots, N\n\\]"
  },
  {
    "objectID": "pdes.html#implementation-using-the-method-of-manufactured-solution",
    "href": "pdes.html#implementation-using-the-method-of-manufactured-solution",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation using the method of manufactured solution",
    "text": "Implementation using the method of manufactured solution\n\nfrom scipy.integrate import quad\nx = sp.Symbol('x')\nue = (1-x**2)*sp.exp(sp.cos(sp.pi*(x-0.5)))\nf = ue.diff(x, 2) # manufactured f=u''\nuhat = lambda j: -(4/(j+1)**2/np.pi**2)*quad(sp.lambdify(x, f*sp.sin((j+1)*sp.pi*(x+1)/2)), -1, 1)[0]\nuh = [uhat(k) for k in range(200)]\nxj = np.linspace(-1, 1, 201)\nsines = np.sin(np.pi/2*(np.arange(len(uh))[None, :]+1)*(xj[:, None]+1))\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\nuej = sp.lambdify(x, ue)(xj)\nax1.plot(xj, uej, 'b', xj, sines[:, :1] @ np.array(uh)[:1], 'k:',\n         xj, sines @ np.array(uh), 'r--')\nax2.loglog(abs(np.array(uh)))\nax1.legend(['Exact', 'One coefficient', f'{200} coefficients'])\nax1.set_title('Solution');ax2.set_title(r'$|\\hat{u}|$');"
  },
  {
    "objectID": "pdes.html#section",
    "href": "pdes.html#section",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "",
    "text": "These weights are readily available from scipy.interpolate.BarycentricInterpolator. We can then obtain the derivative matrix \\(d_{ij} = \\ell'_j(x_i)\\) as\n\\[\n\\begin{align*}\nd_{ij} &= \\frac{w_j}{w_i(x_i-x_j)}, \\quad i \\, \\neq j, \\\\\nd_{ii} &= -\\sum_{\\substack{j=0 \\\\ j \\ne i}}^N d_{ij}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pdes.html#solve-poissons-equation-with-composite-legendre-basis",
    "href": "pdes.html#solve-poissons-equation-with-composite-legendre-basis",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve Poisson’s equation with composite Legendre basis",
    "text": "Solve Poisson’s equation with composite Legendre basis\nThe Legendre polynomials come with a lot of formulas, where two are \\[\n(P_j, P_i) = \\frac{2}{2i+1}\\delta_{ij} \\quad \\text{and} \\quad (2i+3)P_{i+1} = P'_{i+2}-P'_{i}\n\\]\nThe second is very useful for computing the diagonal (!) stiffness matrix\n\\[\n\\begin{align}\n(\\psi'_j, \\psi'_i) &= (P'_j-P'_{j+2}, P'_i-P'_{i+2}) \\\\\n&= (-(2j+3) P_{j+1}, -(2i+3)P_{i+1}) \\\\\n&= (2i+3)^2 (P_{j+1}, P_{i+1}) \\\\\n&= (2i+3)^2 \\frac{2}{2(i+1)+1} \\delta_{i+1,j+1} \\\\\n&= (4i+6)\\delta_{ij}\n\\end{align}\n\\]\n\n\\[\n\\text{Solve Poisson's equation: } \\longrightarrow \\hat{u}_i = \\frac{-\\left(f, \\psi_i\\right)}{4i+6}\n\\]"
  },
  {
    "objectID": "pdes.html#implementation",
    "href": "pdes.html#implementation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation",
    "text": "Implementation\n\nfrom numpy.polynomial import Legendre as Leg\npsi = lambda j: Leg.basis(j)-Leg.basis(j+2) \nfl = sp.lambdify(x, f)\ndef uv(xj, j): return psi(j)(xj) * fl(xj)\nuhat = lambda j: (-1/(4*j+6))*quad(uv, -1, 1, args=(j,))[0]\nN = 60\nuL = [uhat(j) for j in range(N)]\nj = sp.Symbol('j', integer=True, positive=True)\nV = np.polynomial.legendre.legvander(xj, N+1)\nPs = V[:, :-2] - V[:, 2:] \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\nax1.plot(xj, uej, 'b',\n         xj, Ps[:, :1] @ np.array(uL)[:1], 'k:',\n         xj, Ps @ np.array(uL), 'r--')\nax2.loglog(abs(np.array(uL)), '*')\nax1.legend(['Exact', 'One coefficient', f'{N} coefficients'])\nax1.set_title('Solution')\nax2.set_title(r'$|\\hat{u}|$');"
  },
  {
    "objectID": "pdes.html#l2omega-error",
    "href": "pdes.html#l2omega-error",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "\\(L^2(\\Omega)\\) error",
    "text": "\\(L^2(\\Omega)\\) error\n\\[\nL^2(\\Omega) = \\sqrt{\\int_{-1}^1(u-u_e)^2dx}\n\\]\n\n\nuh = np.array(uh) \nuL = np.array(uL) \nerror = np.zeros((2, N))\nfor n in range(N):\n    us = sines[:, :n] @ uh[:n]\n    ul = Ps[:, :n] @ uL[:n]\n    error[0, n] = np.trapz((us-uej)**2, dx=(xj[1]-xj[0]))\n    error[1, n] = np.trapz((ul-uej)**2, dx=(xj[1]-xj[0]))\nplt.figure(figsize=(6, 2.5))\nplt.loglog(error.T)\nplt.legend(['Sines', 'Legendre'])\n\n\n\n\n\n\n\n\n\n\nWhy are the Legendre basis functions better than the sines?\n\nAll the sine basis functions \\(\\psi_j=\\sin(\\pi(j+1)(x+1)/2)\\) have have even derivatives equal to zero at the boundaries, unlike the manufactures solution…\n\\[\n\\frac{d^{2n} \\psi_j}{dx^{2n}}(\\pm 1) = 0, \\quad n=0, 1, \\ldots\n\\]"
  },
  {
    "objectID": "pdes.html#l2omega-error-for-sines-and-legendre",
    "href": "pdes.html#l2omega-error-for-sines-and-legendre",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "\\(L^2(\\Omega)\\) error for sines and Legendre",
    "text": "\\(L^2(\\Omega)\\) error for sines and Legendre\n\\[\nL^2(\\Omega) = \\sqrt{\\int_{-1}^1(u-u_e)^2dx}\n\\]\n\n\nuh = np.array(uh) \nuL = np.array(uL) \nerror = np.zeros((2, N))\nfor n in range(N):\n    us = sines[:, :n] @ uh[:n]\n    ul = Ps[:, :n] @ uL[:n]\n    error[0, n] = np.trapz((us-uej)**2, dx=(xj[1]-xj[0]))\n    error[1, n] = np.trapz((ul-uej)**2, dx=(xj[1]-xj[0]))\nplt.figure(figsize=(6, 2.5))\nplt.loglog(error.T)\nplt.legend(['Sines', 'Legendre'])\n\n\n\n\n\n\n\n\n\n\nWhy are the Legendre basis functions better than the sines?\n\nAll the sine basis functions \\(\\psi_j=\\sin(\\pi(j+1)(x+1)/2)\\) have even derivatives equal to zero at the boundaries, unlike the chosen manufactured solution…\n\\[\n\\frac{d^{2n} \\psi_j}{dx^{2n}}(\\pm 1) = 0 \\rightarrow \\frac{d^{2n}u_N}{dx^{2n}}(\\pm 1)=0, \\quad n=0, 1, \\ldots\n\\]"
  },
  {
    "objectID": "pdes.html#implementation-using-shenfun",
    "href": "pdes.html#implementation-using-shenfun",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation using Shenfun",
    "text": "Implementation using Shenfun\n\nfrom shenfun import FunctionSpace, TestFunction, TrialFunction, inner, Dx\n\nN = 60\nVN = FunctionSpace(N+3, 'L', bc=(0, 0)) # Chooses {P_j-P_{j+2}} basis\nu = TrialFunction(VN)\nv = TestFunction(VN)\nS = inner(Dx(u, 0, 1), Dx(v, 0, 1))\nb = inner(-f, v)\nuh = S.solve(b.copy())\nfig = plt.figure(figsize=(6, 3))\nplt.loglog(np.arange(0, N+1, 2), abs(uh[:-2:2]), '*');"
  },
  {
    "objectID": "pdes.html#inhomogeneous-poisson",
    "href": "pdes.html#inhomogeneous-poisson",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Inhomogeneous Poisson",
    "text": "Inhomogeneous Poisson\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu(-1) &= a, u(1) = b\n\\end{align}\n\\]\nHow to handle the inhomogeneous boundary conditions?\n\nUse homogeneous \\(\\tilde{u}_N \\in V_N\\) and a boundary function \\(B(x)\\)\n\\[\nu_N(x) = B(x) + \\tilde{u}_N(x)\n\\]\nwhere \\(B(-1) = a\\) and \\(B(1) = b\\) such that\n\\[\nu_N(-1)=B(-1)=a \\quad \\text{and} \\quad u_N(1) = B(1) = b\n\\]\nA function that satisfies this in the current domain is\n\\[\nB(x) = \\frac{b}{2}(1+x) + \\frac{a}{2}(1-x)\n\\]"
  },
  {
    "objectID": "pdes.html#solve-poisson",
    "href": "pdes.html#solve-poisson",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve Poisson",
    "text": "Solve Poisson\nInsert for \\(u_N\\) into \\((R_N, v) = 0\\):\n\\[\n\\Big( (B(x)+\\tilde{u}_N)'' - f, v \\Big) = 0\n\\]\nSince \\(B(x)\\) is linear \\(B''=0\\) and we get the homogeneous problem\n\\[\n\\Big( \\tilde{u}^{''}_N - f, v \\Big) = 0\n\\]\nSolve exactly as before for \\(\\tilde{u}_N\\) and the solution will be in the end\n\\[\nu_N(x) = B(x) + \\tilde{u}_N\n\\]"
  },
  {
    "objectID": "pdes.html#implementation-1",
    "href": "pdes.html#implementation-1",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation",
    "text": "Implementation\n\n\nue = sp.exp(sp.cos(x-0.5))\nf = ue.diff(x, 2)\nfl = sp.lambdify(x, f)\ndef uv(xj, j): return psi(j)(xj) * fl(xj)\nuhat = lambda j: (-1/(4*j+6))*quad(uv, -1, 1, args=(j,))[0]\nN = 30\nutilde = [uhat(k) for k in range(N)]\na, b = ue.subs(x, -1), ue.subs(x, 1)\nB = b*(1+x)/2 + a*(1-x)/2\nM = 50\nxj = np.linspace(-1, 1, M+1)\nV = np.polynomial.legendre.legvander(xj, N+1)\nPs = V[:, :-2] - V[:, 2:]\nBs = sp.lambdify(x, B)(xj)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6))\nax1.plot(xj, sp.lambdify(x, ue)(xj), 'b',\n         xj, Ps[:, :1] @ np.array(utilde)[:1] + Bs, 'k:',\n         xj, Ps @ np.array(utilde) + Bs, 'r--')\nax2.loglog(abs(np.array(utilde)), '*')\nax1.legend(['Exact', 'One coefficient', f'{N} coefficients'])\nax1.set_title('Solution')\nax2.set_title(r'$|\\hat{u}|$');"
  },
  {
    "objectID": "pdes.html#neumann-boundary-conditions",
    "href": "pdes.html#neumann-boundary-conditions",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Neumann boundary conditions",
    "text": "Neumann boundary conditions\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu'(\\pm 1) &= 0\n\\end{align}\n\\]\nThis problem is ill-defined because if \\(u\\) is a solution, then \\(u + c\\), where \\(c\\) is a constant, is also a solution!\nIf \\(u(x)\\) satisfies the above problem, then\n\\[\n(u+c)'' = u'' + \\cancel{c''} = f\\quad \\text{and} \\quad (u+c)'(\\pm 1) = u'(\\pm 1) = 0\n\\]\nWe need an additional constraint! One possibility is then to require\n\\[\n(u, 1) = \\int_{\\Omega} u(x) dx = c\n\\]"
  },
  {
    "objectID": "pdes.html#the-well-defined-neumann-problem",
    "href": "pdes.html#the-well-defined-neumann-problem",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The well-defined Neumann problem",
    "text": "The well-defined Neumann problem\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu'(\\pm 1) &= 0 \\\\\n(u, 1) &= c\n\\end{align}\n\\]"
  },
  {
    "objectID": "pdes.html#a-well-defined-neumann-problem",
    "href": "pdes.html#a-well-defined-neumann-problem",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "A well-defined Neumann problem",
    "text": "A well-defined Neumann problem\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu'(\\pm 1) &= 0 \\\\\n(u, 1) &= c\n\\end{align}\n\\]\nHow about basis functions?\n\nIf we choose basis functions \\(\\psi_j\\) that satisfy\n\\[\n\\psi'_j(\\pm 1) = 0, \\quad j=0, 1, \\ldots\n\\]\nthen\n\\[\nu'_N(\\pm 1) = \\sum_{j=0}^N \\hat{u}_j \\psi'_j(\\pm 1) = 0\n\\]"
  },
  {
    "objectID": "pdes.html#neumann-basis-functions",
    "href": "pdes.html#neumann-basis-functions",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Neumann basis functions",
    "text": "Neumann basis functions\nSimplest possibility\n\\[\n\\psi_j = \\cos(\\pi j (x+1) / 2)\n\\]\nEasy to see that \\(\\psi'_j(x) = -j/2\\sin(j(x+1)/2)\\) and thus \\(\\psi'_j(\\pm 1) = 0\\). However, we also get that all odd derivatives are zero\n\\[\n\\frac{d^{2n+1} \\psi_j}{dx^{2n+1}}(\\pm 1) = 0, \\quad n=0, 1, \\ldots\n\\]\n\nLets try to find a basis function using Legendre polynomials instead\n\\[\n\\psi_j = P_j + b(j) P_{j+1} + a(j) P_{j+2}\n\\]\nand try to find \\(a(j)\\) and \\(b(j)\\) such that \\(\\psi'_j(\\pm 1) = 0\\)."
  },
  {
    "objectID": "pdes.html#legendre-neumann-basis-function",
    "href": "pdes.html#legendre-neumann-basis-function",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Legendre Neumann basis function",
    "text": "Legendre Neumann basis function\n\\[\n\\psi_j = P_j + b(j)P_{j+1} + a(j) P_{j+2}\n\\]\n\\(\\text{Using boundary conditions:}  \\quad P'_j(-1) = \\frac{j(j+1)}{2}(-1)^j \\quad \\text{and} \\quad P'_j(1) = \\frac{j(j+1)}{2}\\)\nWe have two conditions and two unknowns\n\\[\\small\n\\begin{align}\n\\psi'_j(-1) &= P'_j(-1) + b(j)P'_{j+1}(-1) + a(j) P'_{j+2}(-1) \\\\\n&= \\left(\\frac{j(j+1)}{2}-b(j)\\frac{(j+1)(j+2)}{2} +a(j)\\frac{(j+2)(j+3)}{2}\\right)(-1)^j = 0\n\\end{align}\n\\]\n\\[ \\small\n\\psi'_j(1) = \\left(\\frac{j(j+1)}{2} + b(j) \\frac{(j+1)(j+2)}{2}+a(j)\\frac{(j+2)(j+3)}{2}\\right) = 0\n\\]\nSolve the two equations to find \\(a(j), b(j)\\) and thus the Neumann basis function \\(\\psi_j\\):\n\\[\nb(j)=0, \\, a(j) = - \\frac{j(j+1)}{(j+2)(j+3)} \\longrightarrow \\boxed{\\psi_j = P_j - \\frac{j(j+1)}{(j+2)(j+3)} P_{j+2}}\n\\]"
  },
  {
    "objectID": "pdes.html#solve-neumann-problem",
    "href": "pdes.html#solve-neumann-problem",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve Neumann problem",
    "text": "Solve Neumann problem\nUse the functionspace\n\\[\nV_N = \\text{span}\\Big \\{P_j - \\frac{j(j+1)}{(j+2)(j+3)} P_{j+2} \\Big \\}_{j=0}^N\n\\]\nand try to find \\(u_N \\in V_N\\).\nHowever, we remember also the constraint and that\n\\[\n(u, 1) = c \\rightarrow (u_N, P_0)= c\n\\]\nsince \\(\\psi_0 = P_0 = 1\\). Insert for \\(u_N\\) and use orthogonality of Legendre polynomials to get\n\\[\n\\Big(\\sum_{j=0}^N \\hat{u}_j (P_j - \\frac{j(j+1)}{(j+2)(j+3)} P_{j+2}), P_0\\Big) = (P_0, P_0) \\hat{u}_0 = 2 \\hat{u}_0 = c\n\\]\nSo we already know that \\(\\hat{u}_0=c/2\\) and only have unknowns \\(\\{\\hat{u}_{j}\\}_{j=1}^N\\) left!"
  },
  {
    "objectID": "pdes.html#solve-neumann-with-galerkin",
    "href": "pdes.html#solve-neumann-with-galerkin",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve Neumann with Galerkin",
    "text": "Solve Neumann with Galerkin\nDefine\n\\[\n\\tilde{V}_N = \\text{span}\\Big\\{P_j - \\frac{j(j+1)}{(j+2)(j+3)} P_{j+2}\\Big\\}_{j=1}^N (= V_N \\backslash \\{P_0\\})\n\\]\nWith Galerkin: Find \\(\\tilde{u}_N \\in \\tilde{V}_N (= \\sum_{j=1}^N \\hat{u}_j \\psi_j)\\) such that\n\\[\n(\\tilde{u}^{''}_N - f, v) = 0, \\quad \\forall \\, v \\in \\tilde{V}_N\n\\]\nand use in the end\n\\[\nu_N = \\hat{u}_0 + \\tilde{u}_N = \\sum_{j=0}^N \\hat{u}_j \\psi_j\n\\]"
  },
  {
    "objectID": "pdes.html#stiffness-matrix-for-neumann",
    "href": "pdes.html#stiffness-matrix-for-neumann",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Stiffness matrix for Neumann",
    "text": "Stiffness matrix for Neumann\n\\[\n(\\psi^{''}_j, \\psi_i) = -(\\psi^{'}_j, \\psi^{'}_i)\n\\]"
  },
  {
    "objectID": "pdes.html#the-linear-algebra-problem",
    "href": "pdes.html#the-linear-algebra-problem",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The linear algebra problem",
    "text": "The linear algebra problem\nWe need to solve\n\\[\n\\sum_{j=1}^N(\\psi^{''}_j, \\psi_i) \\hat{u}_j = (f, \\psi_i), \\quad i=1,2, \\ldots, N\n\\]\nThe stiffness matrix for Neumann\n\\[\n(\\psi^{''}_j, \\psi_i) = -(\\psi^{'}_j, \\psi^{'}_i) = (\\psi_j, \\psi^{''}_i)\n\\]\nis fortunately diagonal (derivation later) and we can easily solve for \\(\\{\\hat{u}_i\\}_{i=1}^N\\)\n\\[\n(\\psi^{''}_j, \\psi_i) = a(j) (4j+6) \\delta_{ij}\n\\]\n\\[\n\\longrightarrow \\hat{u}_i = \\frac{(f, \\psi_i)}{a(i)(4i+6)}, \\quad i = 1, 2, \\ldots, N\n\\]"
  },
  {
    "objectID": "pdes.html#derivation-of-psi_j-psi_i",
    "href": "pdes.html#derivation-of-psi_j-psi_i",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Derivation of \\((\\psi^{''}_j, \\psi_i)\\)",
    "text": "Derivation of \\((\\psi^{''}_j, \\psi_i)\\)\nThere is a series expansion for the second derivative \\(P^{''}_j\\)\n\\[\nP^{''}_j = \\sum_{\\substack{k=0 \\\\ k+j \\text{ even}}}^{j-2}c(k, j) P_k, \\, \\text{where}\\, c(k, j) = (k+1/2)(j(j+1)-k(k+1)) \\tag{1}\n\\]\nHence \\(P^{''}_N+a(N)P^{''}_{N+2}\\) is a Legendre series ending at \\(a(N)c(N-2, N)P_N\\). Consider\n\\[\n\\Big(P^{''}_j+a(j)P^{''}_{j+2}, \\, P_i + a(i)P_{i+2} \\Big)\n\\]\n\nBased on the orthogonality \\((P_i, P_j)=\\frac{2}{2j+1}\\delta_{ij}\\) and (1) we get that\n\nIf \\(i&gt;j\\) then \\(\\small(P^{''}_j+a(j)P^{''}_{j+2}, P_i + a(i)P_{i+2})=0\\) since \\(\\small P^{''}_{j+2}=\\sum_{k=0}^j c(k,j) P_k\\)\nIf \\(i&lt; j\\) then \\((P^{''}_j+a(j)P^{''}_{j+2}, P_i)=0\\) due to symmetry \\((\\psi^{''}_j, \\psi_i) = (\\psi_j, \\psi^{''}_i)\\)\n\nHence \\(\\Big(P^{''}_j+a(j)P^{''}_{j+2}, \\, P_i + a(i)P_{i+2} \\Big)\\) is diagonal!"
  },
  {
    "objectID": "pdes.html#compute-psi_i-psi_i",
    "href": "pdes.html#compute-psi_i-psi_i",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Compute \\((\\psi^{''}_i, \\psi_i)\\)",
    "text": "Compute \\((\\psi^{''}_i, \\psi_i)\\)\nUsing again the expression \\(P^{''}_i = \\sum_{k=0}^{i-2} c(k, i) P_{k}\\)\n\\[ \\small\n\\begin{multline}\n\\Big(P^{''}_i+a(i)P^{''}_{i+2}, P_i+a(i)P_{i+2}\\Big) = \\\\ \\cancel{(P^{''}_i, P_i)} + \\cancel{a(i)(P^{''}_i, P_{i+2})} + a(i)(P^{''}_{i+2}, P_i) + \\cancel{a^2(i)(P^{''}_{i+2}, P_{i+2})}\n\\end{multline}\n\\]\nAll cancellations because of orthogonality and \\(P^{''}_i = \\sum_{k=0}^{i-2} (\\cdots) P_{k}\\)\n\\[ \\small\n\\begin{align}\na(i)(P^{''}_{i+2}, P_i) &= a(i) \\sum_{\\substack{k=0 \\\\ k+i \\text{ even}}}^{i} \\Big( (k+1/2)((i+2)(i+3)-k(k+1))P_k, \\, P_i \\Big) \\\\\n&= a(i)(i+1/2)((i+2)(i+3)-i(i+1)) (L_i, L_i) \\\\\n&= a(i)(4i+6)\n\\end{align}\n\\]\nHence we get the stiffness matrix\n\\[\n(\\psi^{''}_j, \\psi_i) = a(i) (4i+6) \\delta_{ij}\n\\]"
  },
  {
    "objectID": "pdes.html#implementation-2",
    "href": "pdes.html#implementation-2",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation",
    "text": "Implementation\nUse manufactured solution that we know satisfies the boundary conditions\n\\[\nu(x) = \\int (1-x^2)\\cos (x-1/2)dx\n\\]\n\nue = sp.integrate((1-x**2)*sp.cos(x-sp.S.Half), x)\nf = ue.diff(x, 2) # manufactured f\nc = sp.integrate(ue, (x, -1, 1)).n() # constraint c\npsi = lambda j: Leg.basis(j)-j*(j+1)/((j+2)*(j+3))*Leg.basis(j+2)\nfj = sp.lambdify(x, f)\ndef uv(xj, j): return psi(j)(xj) * fj(xj)\ndef a(j): return -j*(j+1)/((j+2)*(j+3))\nuhat = lambda j: 1/(a(j)*(4*j+6))*quad(uv, -1, 1, args=(j,))[0]\nN = 30; uh = np.zeros(N); uh[0] = c/2\nuh[1:] = [uhat(k) for k in range(1, N)]"
  },
  {
    "objectID": "pdes.html#implementation-2-output",
    "href": "pdes.html#implementation-2-output",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation",
    "text": "Implementation"
  },
  {
    "objectID": "pdes.html#more-about-neumann-boundary-conditions",
    "href": "pdes.html#more-about-neumann-boundary-conditions",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "More about Neumann boundary conditions",
    "text": "More about Neumann boundary conditions\nWe have used basis functions that satisfied\n\\[\n\\psi^{'}_j(\\pm 1) = 0\n\\]\nHowever, this was not strictly necessary! Neumann boundary conditions are often called natural conditions and we can implement them directly in the variational form:\n\\[\n(\\psi^{''}_j, \\psi_i) = -(\\psi^{'}_j, \\psi{'}_i) + [\\psi^{'}_j \\psi_i]_{-1}^{1}\n\\]\n\nEnforce boundary conditions weakly using \\(\\psi^{'}_j(-1)=a, \\psi^{'}_j(1)=b\\):\n\\[\n(\\psi^{''}_j, \\psi_i) = -(\\psi^{'}_j, \\psi{'}_i) + b \\psi_i(1) - a \\psi_i(-1)\n\\]\nHomogeneous Neumann (\\(a=b=0\\)):\n\\[\n(\\psi^{''}_j, \\psi_i) = -(\\psi^{'}_j, \\psi{'}_i)\n\\]"
  },
  {
    "objectID": "pdes.html#implementation-3",
    "href": "pdes.html#implementation-3",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation",
    "text": "Implementation\nUsing basis function \\(\\psi_j(x) = P_j(x)\\) that have \\(\\psi^{'}_j(\\pm 1) \\ne 0\\)\n\npsi = lambda j: Leg.basis(j)\ndef uf(xj, j): return psi(j)(xj) * fj(xj)\ndef uv(xj, i, j): return -psi(i).deriv(1)(xj) * psi(j).deriv(1)(xj)\nfhat = lambda j: quad(uf, -1, 1, args=(j,))[0]\nN = 20\n# Compute the stiffness matrix (not diagonal)\nS = np.zeros((N, N))\nfor i in range(1, N):\n    for j in range(i, N):\n        S[i, j] = quad(uv, -1, 1, args=(i, j))[0]\n        S[j, i] = S[i, j]\nS[0, 0] = 1 # To fix constraint uh[0] = c/2\nfh = np.zeros(N); fh[0] = c/2\nfh[1:] = [fhat(k) for k in range(1, N)]\nfh = np.array(fh, dtype=float)\nuh = np.linalg.solve(S, fh)"
  },
  {
    "objectID": "pdes.html#collocation",
    "href": "pdes.html#collocation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Collocation",
    "text": "Collocation\nConsider the Dirichlet problem\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu(-1) = a \\quad &\\text{and} \\quad u(1) = b\n\\end{align}\n\\]\nTo solve this problem with collocation we use a mesh \\(\\boldsymbol{x}=\\{x_i\\}_{i=0}^N\\), where \\(x_0=-1\\) and \\(x_N=1\\). The solution, using Lagrange polynomials, is\n\\[\nu_N(x) = \\sum_{i=0}^N \\hat{u}_i \\ell_i(x)\n\\]\nWe then require that the following \\(N+1\\) equations are satisfied\n\\[\n\\begin{align}\n\\mathcal{R}_N(x_i) &= 0, \\quad i=1, 2, \\ldots, N-1 \\\\\nu(x_0) = a \\quad &\\text{and} \\quad u(x_N) = b\n\\end{align}\n\\]\nwhere \\(\\mathcal{R}_N(x) = u^{''}_N(x)-f(x)\\)."
  },
  {
    "objectID": "pdes.html#solve-by-inserting-for-u_n-in-mathcalr_n",
    "href": "pdes.html#solve-by-inserting-for-u_n-in-mathcalr_n",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Solve by inserting for \\(u_N\\) in \\(\\mathcal{R}_N\\)",
    "text": "Solve by inserting for \\(u_N\\) in \\(\\mathcal{R}_N\\)\nWe get the \\(N-1\\) equations for \\(\\{\\hat{u}_j\\}_{j=1}^{N-1}\\)\n\\[\n\\sum_{j=0}^N \\hat{u}_j \\ell''_j(x_i)  = f(x_i), \\quad i = 1, 2, \\ldots, N-1\n\\]\nin addition to the boundary conditions: \\(\\hat{u}_0=u_N(x_0)=a\\) and \\(\\hat{u}_N=u_N(x_N)=b\\).\nThe matrix \\(D^{(2)} = (d^{(2)}_{ij})=(\\ell^{''}_j(x_i))_{i,j=0}^N\\) is dense. How do we compute it?\n\n\n\n\n\n\nNote\n\n\nUsing the Sympy Lagrange functions is numerically unstable"
  },
  {
    "objectID": "pdes.html#another-useful-form-of-the-lagrange-polynomials",
    "href": "pdes.html#another-useful-form-of-the-lagrange-polynomials",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Another useful form of the Lagrange polynomials",
    "text": "Another useful form of the Lagrange polynomials\n\\[\n\\ell_j(x) = \\prod_{\\substack{0 \\le m \\le N \\\\ m \\ne j}} \\frac{x-x_m}{x_j-x_m}\n\\]\nA small rearrangement leads to\n\\[\n\\ell_j(x) = \\ell(x) \\frac{w_j}{x-x_j},\n\\]\nwhere\n\\[\n\\ell(x) = \\prod^N_{\\substack{i=0}}(x-x_i) \\quad \\text{and} \\quad\nw_j = \\frac{1}{\\ell'(x_j)}= \\frac{1}{\\prod^N_{\\substack{i=0 \\\\ i \\neq j}} (x_j - x_i)}\n\\]\nHere \\((w_j)_{j=0}^N\\) are the barycentric weights. Scipy will give you these weights: from scipy.interpolate import BarycentricInterpolator"
  },
  {
    "objectID": "pdes.html#the-main-advantage-of-the-barycentric-approach-is-numerical-stability",
    "href": "pdes.html#the-main-advantage-of-the-barycentric-approach-is-numerical-stability",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "The main advantage of the Barycentric approach is numerical stability",
    "text": "The main advantage of the Barycentric approach is numerical stability\nAnd we can obtain the derivative matrix \\(d_{ij} = \\ell'_j(x_i)\\) as\n\\[\n\\begin{align*}\nd_{ij} &= \\frac{w_j}{w_i(x_i-x_j)}, \\quad i \\, \\neq j, \\\\\nd_{ii} &= -\\sum_{\\substack{j=0 \\\\ j \\ne i}}^N d_{ij}.\n\\end{align*}\n\\]\n\n\n\nfrom scipy.interpolate import BarycentricInterpolator\ndef Derivative(xj):\n    w = BarycentricInterpolator(xj).wi\n    W = w[None, :] / w[:, None]\n    X = xj[:, None]-xj[None, :]\n    np.fill_diagonal(X, 1)\n    D = W / X\n    np.fill_diagonal(D, 0)\n    np.fill_diagonal(D, -np.sum(D, axis=1))\n    return D\n\n\n\n\n\n\n\n\nNumpy broadcasting!\n\n\nW is the matrix with items \\(w_j / w_i\\). \\(w_i\\) varies along the first axis and is thus w[:, None]. \\(w_j\\) varies along the second axis and is w[None, :]. Likewise \\(x_i\\) is xj[:, None] and \\(x_j\\) is xj[None, :]"
  },
  {
    "objectID": "pdes.html#higher-order-derivatives",
    "href": "pdes.html#higher-order-derivatives",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Higher order derivatives",
    "text": "Higher order derivatives\n\\[\n\\begin{align*}\nd^{(n)}_{ij} &= \\frac{n}{x_i-x_j}\\left(\\frac{w_j}{w_i} d^{(n-1)}_{ii} - d^{(n-1)}_{ij} \\right), \\\\\nd^{(n)}_{ii} &= -\\sum_{\\substack{j=0 \\\\ j \\ne i}}^N d^{(n)}_{ij}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pdes.html#higher-order-derivatives-can-be-computed-recursively",
    "href": "pdes.html#higher-order-derivatives-can-be-computed-recursively",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Higher order derivatives can be computed recursively",
    "text": "Higher order derivatives can be computed recursively\n\\[\n\\begin{align*}\nd^{(n)}_{ij} &= \\frac{n}{x_i-x_j}\\left(\\frac{w_j}{w_i} d^{(n-1)}_{ii} - d^{(n-1)}_{ij} \\right), \\\\\nd^{(n)}_{ii} &= -\\sum_{\\substack{j=0 \\\\ j \\ne i}}^N d^{(n)}_{ij}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "pdes.html#higher-order-derivative-matrices-dn_ij-elln_jx_i-can-be-computed-recursively",
    "href": "pdes.html#higher-order-derivative-matrices-dn_ij-elln_jx_i-can-be-computed-recursively",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Higher order derivative matrices \\(d^{n}_{ij} = \\ell^{(n)}_j(x_i)\\) can be computed recursively",
    "text": "Higher order derivative matrices \\(d^{n}_{ij} = \\ell^{(n)}_j(x_i)\\) can be computed recursively\n\\[\n\\begin{align*}\nd^{(n)}_{ij} &= \\frac{n}{x_i-x_j}\\left(\\frac{w_j}{w_i} d^{(n-1)}_{ii} - d^{(n-1)}_{ij} \\right) \\\\\nd^{(n)}_{ii} &= -\\sum_{\\substack{j=0 \\\\ j \\ne i}}^N d^{(n)}_{ij}\n\\end{align*}\n\\]\n\ndef PolyDerivative(xj, m):\n    w = BarycentricInterpolator(xj).wi * (2*(len(xj)-1))\n    W = w[None, :] / w[:, None]\n    X = xj[:, None]-xj[None, :]\n    np.fill_diagonal(X, 1)\n    D = W / X\n    np.fill_diagonal(D, 0)\n    np.fill_diagonal(D, -np.sum(D, axis=1))\n    if m == 1: return D\n    D2 = np.zeros_like(D)\n    for k in range(2, m+1):\n        D2[:] = k / X * (W * D.diagonal()[:, None] - D)\n        np.fill_diagonal(D2, 0)\n        np.fill_diagonal(D2, -np.sum(D2, axis=1))\n        D[:] = D2\n    return D2"
  },
  {
    "objectID": "pdes.html#and-then-we-solve-any-equation-by-replacing-the-ordinary-derivatives-with-derivative-matrices",
    "href": "pdes.html#and-then-we-solve-any-equation-by-replacing-the-ordinary-derivatives-with-derivative-matrices",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "And then we solve any equation by replacing the ordinary derivatives with derivative matrices",
    "text": "And then we solve any equation by replacing the ordinary derivatives with derivative matrices\n\\[\n\\begin{align}\nu''(x) &= f(x), \\quad x \\in (-1, 1) \\\\\nu(-1) = a \\quad &\\text{and} \\quad u(1) = b\n\\end{align}\n\\]\nLet \\(d^{(2)}_{ij} = \\ell^{''}_j(x_i)\\) for all \\(i=1, \\ldots, N-1\\), ident the first and last rows of \\(D^{(2)}\\) and set \\(f_0=a\\) and \\(f_N=b\\). Solve\n\\[\n\\sum_{j=0}^N d^{(2)}_{ij} \\hat{u}_j = f_i, \\quad i=0, 1, \\ldots, N\n\\]\nMatrix form using \\(\\boldsymbol{\\hat{u}} = (\\hat{u}_j)_{j=0}^N\\) and \\(\\boldsymbol{f} = (f_j)_{j=0}^N\\)\n\\[\nD^{(2)} \\boldsymbol{\\hat{u}} = \\boldsymbol{f}\n\\]\n\\[\n\\boldsymbol{\\hat{u}} = (D^{(2)})^{-1} \\boldsymbol{f}\n\\]"
  },
  {
    "objectID": "pdes.html#use-chebyshev-points-for-spectral-accuracy",
    "href": "pdes.html#use-chebyshev-points-for-spectral-accuracy",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Use Chebyshev points for spectral accuracy",
    "text": "Use Chebyshev points for spectral accuracy\n\\[\nx_i = \\cos(i \\pi / N)\n\\]\nThe barycentric weights are then simply\n\\[\nw_i = (-1)^{i} c_i, \\quad c_i = \\begin{cases} 0.5 \\quad i=0 \\text{ or } i = N \\\\\n1 \\quad \\text{ otherwise}\n\\end{cases} \\tag{1}\n\\]\n\nN = 8 \nxj = np.cos(np.arange(N+1)*np.pi/N)\nw = BarycentricInterpolator(xj).wi * 2*N\nw\n\narray([ 0.5, -1. ,  1. , -1. ,  1. , -1. ,  1. , -1. ,  0.5])\n\n\n\n\n\n\n\n\nNote\n\n\nThe weights are only relative, so we have here scaled by \\(2N\\) to get (1)"
  },
  {
    "objectID": "pdes.html#implementation-for-poissons-equation",
    "href": "pdes.html#implementation-for-poissons-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Implementation for Poisson’s equation",
    "text": "Implementation for Poisson’s equation\n\ndef poisson_coll(N, f, bc=(0, 0)):\n    xj = np.cos(np.arange(N+1)*np.pi/N)[::-1]\n    D2 = PolyDerivative(xj, 2)      # Get second derivative matrix\n    D2[0, 0] = 1; D2[0, 1:] = 0      # ident first row\n    D2[-1, -1] = 1; D2[-1, :-1] = 0  # ident last row\n    fh = np.zeros(N+1)\n    fh[1:-1] = sp.lambdify(x, f)(xj[1:-1])\n    fh[0], fh[-1] = bc             # Fix boundary conditions\n    uh = np.linalg.solve(D2, fh)\n    return uh, D2\n\ndef l2_error(uh, ue):\n    uj = sp.lambdify(x, ue)\n    N = len(uh)-1\n    xj = np.cos(np.arange(N+1)*np.pi/N)[::-1]\n    L = BarycentricInterpolator(np.cos(np.arange(N+1)*np.pi/N)[::-1], yi=uh)\n    N = 4*len(uh) # Use denser mesh to compute L2-error\n    xj = np.linspace(-1, 1, N+1)\n    return np.sqrt(np.trapz((uj(xj)-L(xj).astype(float))**2, dx=2./N)) \n\n\n\nue = sp.exp(sp.cos(x-0.5))\nf = ue.diff(x, 2)\nbc = ue.subs(x, -1), ue.subs(x, 1)\nerr = []\nfor N in range(2, 46, 2):\n    uh, D = poisson_coll(N, f, bc=bc)\n    err.append(l2_error(uh, ue))\nfig = plt.figure(figsize=(6, 2.5))\nplt.loglog(np.arange(2, 46, 2), err, '*')\nplt.title(\"L2-error Posson's equation\");"
  },
  {
    "objectID": "pdes.html#chebyshev-basis-functions",
    "href": "pdes.html#chebyshev-basis-functions",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Chebyshev basis functions",
    "text": "Chebyshev basis functions\n\nExactly the same approach as for Legendre, only with a weighted inner product \\[\n(u, v)_{\\omega} = \\int_{-1}^1 \\frac{u v}{\\sqrt{1-x^2}}dx\n\\]\n\n\n\nFor Dirichlet boundary conditions: Find \\(u_N \\in V_N = \\text{span}\\{T_i-T_{i+2}\\}_{i=0}^N\\) such that \\[\n(\\mathcal{R}_N, v)_{\\omega} = 0, \\quad \\forall \\, v \\in V_N\n\\] The basis functions \\(\\psi_i=T_i-T_{i+2}\\) satisfy \\(\\psi_i(\\pm 1)=0\\).\nFor Neumann boundary conditions, the basis functions are slightly different since \\[\nT'_k(-1) = (-1)^{k+1} k^2 \\quad \\text{and} \\quad T'_{k}(1) = k^2\n\\] The basis functions \\(\\phi_k = T_{k} - \\left(\\frac{k}{k+2}\\right)^2 T_{k+2}\\) satisfy \\(\\phi'_k(\\pm 1) = 0\\).\nInhomogeneous boundary conditions are handled like for Legendre with the same boundary function \\(B(x)\\)."
  },
  {
    "objectID": "pdes.html#recap",
    "href": "pdes.html#recap",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Recap",
    "text": "Recap\nWe have until now considered the approximation of a function \\(u(x)\\) using a function space \\(V_N = \\text{span}\\{\\psi_j\\}_{j=0}^N\\)\n\\[\nu_N(x) \\approx u(x), \\quad u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x)\n\\]\nIn order to find the unknowns (degrees of freedom) \\(\\{\\hat{u}_j\\}_{j=0}^N\\) we have considered the error \\(e=u_N-u\\) through various methods\n\nVariational methods\n\nGalerkin\nLeast squares\n\nCollocation\n\nLagrange polynomials"
  },
  {
    "objectID": "pdes.html#we-will-now-learn-to-approximate-small-ux-by-small-u_nx-such-that-it-satisfies-a-linear-equation",
    "href": "pdes.html#we-will-now-learn-to-approximate-small-ux-by-small-u_nx-such-that-it-satisfies-a-linear-equation",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "We will now learn to approximate \\(\\small u(x)\\) by \\(\\small u_N(x)\\) such that it satisfies a linear equation",
    "text": "We will now learn to approximate \\(\\small u(x)\\) by \\(\\small u_N(x)\\) such that it satisfies a linear equation\n\\[\n\\mathcal{L}(u) = f\n\\]\nwhere the generic operator \\(\\mathcal{L}(u)\\) can represent anything, like\n\\[\n\\begin{align*}\nu  &= f \\\\\nu' &= f \\\\\nu'' &= f \\\\\nu'' +\\alpha u' + \\lambda u  &= f \\\\\n\\frac{d}{dx}\\left(\\alpha \\frac{d u}{dx}\\right) &= f\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\nFunction approximation is then simply the case \\(u=f\\), such that \\(u_N \\approx f\\)."
  },
  {
    "objectID": "pdes.html#define-a-new-error-measure-the-residual",
    "href": "pdes.html#define-a-new-error-measure-the-residual",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Define a new error measure (the residual)",
    "text": "Define a new error measure (the residual)\nDefine a residual that we ultimately want to be zero\n\\[\n\\mathcal{R} = \\mathcal{L}(u)-f\n\\]\nand create a “numerical” residual\n\\[\n\\mathcal{R}_N = \\mathcal{L}(u_N)-f\n\\]\nwhere the unknowns are still \\(\\{\\hat{u}_j\\}_{j=0}^N\\)."
  },
  {
    "objectID": "pdes.html#define-a-new-error-measure-the-residual-that-we-ultimately-want-to-be-zero",
    "href": "pdes.html#define-a-new-error-measure-the-residual-that-we-ultimately-want-to-be-zero",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Define a new error measure (the residual) that we ultimately want to be zero",
    "text": "Define a new error measure (the residual) that we ultimately want to be zero\n\\[\n\\mathcal{R} = \\mathcal{L}(u)-f\n\\]\nand create a “numerical” residual by inserting for \\(u = u_N\\)\n\\[\n\\mathcal{R}_N = \\mathcal{L}(u_N)-f\n\\]\nThe task now is to minmize \\(\\mathcal{R}_N\\) in order to find the unknowns that are still \\(\\{\\hat{u}_j\\}_{j=0}^N\\).\n\n\n\n\n\n\nNote\n\n\nFor function approximation \\(\\mathcal{R}_N = e = u_N - f = u_N - u\\)."
  },
  {
    "objectID": "pdes.html#composite-legendre-neumann-basis",
    "href": "pdes.html#composite-legendre-neumann-basis",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Composite Legendre Neumann basis",
    "text": "Composite Legendre Neumann basis\n\\[\n\\psi_j = P_j + b(j)P_{j+1} + a(j) P_{j+2}\n\\]\n\\(\\text{Using boundary conditions:}  \\quad P'_j(-1) = \\frac{j(j+1)}{2}(-1)^j \\quad \\text{and} \\quad P'_j(1) = \\frac{j(j+1)}{2}\\)\n\nWe have two conditions and two unknowns\n\\[\\small\n\\begin{align}\n\\psi'_j(-1) &= P'_j(-1) + b(j)P'_{j+1}(-1) + a(j) P'_{j+2}(-1) \\\\\n&= \\left(\\frac{j(j+1)}{2}-b(j)\\frac{(j+1)(j+2)}{2} +a(j)\\frac{(j+2)(j+3)}{2}\\right)(-1)^j = 0\n\\end{align}\n\\]\n\\[ \\small\n\\psi'_j(1) = \\left(\\frac{j(j+1)}{2} + b(j) \\frac{(j+1)(j+2)}{2}+a(j)\\frac{(j+2)(j+3)}{2}\\right) = 0\n\\]\nSolve the two equations to find \\(a(j), b(j)\\) and thus the Neumann basis function \\(\\psi_j\\):\n\\[\nb(j)=0, \\, a(j) = - \\frac{j(j+1)}{(j+2)(j+3)} \\longrightarrow \\boxed{\\psi_j = P_j - \\frac{j(j+1)}{(j+2)(j+3)} P_{j+2}}\n\\]"
  },
  {
    "objectID": "pdes.html#dense-stiffness-matrix",
    "href": "pdes.html#dense-stiffness-matrix",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Dense stiffness matrix",
    "text": "Dense stiffness matrix\nUsing basis function \\(\\psi_j = P_j\\) leads to a dense stiffness matrix\n\nplt.spy(S)"
  },
  {
    "objectID": "pdes.html#dense-stiffness-matrix-psi_j-psi_i",
    "href": "pdes.html#dense-stiffness-matrix-psi_j-psi_i",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Dense stiffness matrix \\((\\psi'_j, \\psi'_i)\\)",
    "text": "Dense stiffness matrix \\((\\psi'_j, \\psi'_i)\\)\nUsing basis function \\(\\psi_j = P_j\\) leads to a dense stiffness matrix\n\nplt.spy(S)\n\n\n\n\n\n\n\n\nand thus the need for the linear algebra solve \\(\\boldsymbol{\\hat{u}} = S^{-1} \\boldsymbol{f}\\)\nuh = np.linalg.solve(S, fh)"
  },
  {
    "objectID": "pdes.html#another-alternating-oddeven-basis-can-be-created-from-legendre-polynomials-p_jx",
    "href": "pdes.html#another-alternating-oddeven-basis-can-be-created-from-legendre-polynomials-p_jx",
    "title": "Solving PDEs with the method of weighted residuals",
    "section": "Another alternating odd/even basis can be created from Legendre polynomials \\(P_j(x)\\)",
    "text": "Another alternating odd/even basis can be created from Legendre polynomials \\(P_j(x)\\)\n\n\n\\[\n\\begin{align}\n\\psi_j(x) &= P_j(x) - P_{j+2}(x)\\\\\n\\psi_j(\\pm 1) &= 0\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that\n\\[\nP_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1) = 1.\n\\]\nHence for any \\(j\\) all basis functions are zero\n\\[\n\\begin{align}\nP_j(-1) - P_{j+2}(-1) &= (-1)^j - (-1)^{j+2} = 0 \\\\\nP_j(1)-P_{j+2}(1) & =1-1=0\n\\end{align}\n\\]"
  }
]