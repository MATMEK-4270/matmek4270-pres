---
title: "Algorithms and implementations for exponential decay models"
subtitle: "MATMEK-4270"
author: "Prof. Mikael Mortensen, University of Oslo"

format:
  revealjs: 
    slide-number: true
    preview-links: auto
    css: styles.css
    embed-resources: false
pandoc:
  to: revealjs
  standalone: false
  wrap: none
  default-image-extension: png
  html-math-method:
    method: mathjax
    url: >-
      https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
  slide-level: 2

---


## Hans Petter Langtangen 1962-2016 {.smaller}

::: {.hidden}
\newcommand{\half}{\scriptstyle\frac{1}{2}}
\newcommand{\halfi}{{1/2}}
\newcommand{\uex}{u_{e}}
:::

::: columns 
::: {.column width=50%}
![](./figures/SIM135194_Simula_Hans_Petter_004-1024x681.jpg)

:::

::: {.column width=3%}
:::

::: {.column width=47%}

 * 2011-2015 Editor-In-Chief SIAM J of Scientific Computing
 * Author of 13 published books on scientific computing
 * Professor of Mechanics, University of Oslo 1998
 * Developed INF5620 (which became IN5270 and now MAT-MEK4270)
 * [Memorial page](http://hpl-memorial.simula.no)

:::
:::

## A little bit about myself {.smaller}

![](https://raw.githack.com/spectralDNS/spectralutilities/master/movies/RB_256_512_movie_jet.png){.absolute bottom=0 right=50 width="600" height="200"}
![](./figures/chalmers.png){.absolute top=200 right=0}
![](./figures/FFI_logo_eng.png){.absolute bottom=0 left=0}

  * Professor of mechanics (2019-)
  * PhD (Chalmers University of Technology) in mathematical modelling of turbulent combustion
  * Norwegian Defence Research Establishment (2007-2012)
  * Computational Fluid Dynamics
  * High Performance Computing
  * Spectral methods

## Principal developer of Shenfun {.smaller}

High performance computing platform for solving PDEs by the spectral Galerkin method. Written in Python (Cython). [https://github.com/spectralDNS/shenfun](https://github.com/spectralDNS/shenfun)

::: {layout-ncol="3" height="300"}
![](https://raw.githack.com/spectralDNS/spectralutilities/master/figures/strong_scaling_pencil_col.png)

![](https://raw.githack.com/spectralDNS/spectralutilities/master/movies/Kuramato_movie_128.gif)

![](https://raw.githack.com/spectralDNS/spectralutilities/master/movies/KHmovie_3.gif)
:::
::: {layout-ncol="3"}
![](https://raw.githack.com/spectralDNS/spectralutilities/master/movies/isotropic300_12.gif)

![](https://cdn.jsdelivr.net/gh/spectralDNS/spectralutilities@master/figures/moebius8_trans.png)

![](https://raw.githack.com/spectralDNS/spectralutilities/master/figures/torus2.png)
:::

## MAT-MEK4270 in a nutshell {.smaller}

 * Numerical methods for partial differential equations (PDEs)
 * How to solve the equations, not why
 * How do we solve a PDE in practice?
 * How do we trust the answer?
 * Is the numerical scheme stable? accurate? consistent?
 * Focus on programming (github, python, testing code) 
 * IN5670 -> IN5270 -> MAT-MEK4270 - Lots of old material

## Syllabus {.smaller}

::: {.callout-important appearance="default"}
## Important stuff
   * [Lecture notes](https://matmek-4270.github.io/matmek4270-book/intro.html)
   * [Presentations (including this one)](https://matmek-4270.github.io/matmek4270-pres)
   * [Github organization MATMEK-4270](https://github.com/MATMEK-4270)
:::

::: {.callout-warning appearance="simple"}
## Also important stuff, but less so as I will try to put all really important stuff in the lecture notes 

  * [Langtangen, Finite Difference Computing with exponential decay](https://link.springer.com/book/10.1007/978-3-319-29439-1) - Chapters 1 and 2.
  * [Langtangen and Linge, Finite Difference Computing with PDEs](https://link.springer.com/book/10.1007/978-3-319-55456-3)  - Parts of chapters 1 and 2.
  * [Langtangen and Mardal, Introduction to Numerical Methods for Variational Problems](https://link.springer.com/book/10.1007/978-3-030-23788-2)
:::

## Two major approaches {.smaller}

::: columns 

::: {.column width=50%}
### Finite differences

$$
\frac{du(t)}{dt} \approx \frac{u(t+\Delta t) - u(t)}{\Delta t}
$$

* Approximate in points
* Uniform grid
* Taylor expansions

:::

::: {.column width=50%}

### Variational methods

$$
\int_{\Omega} u'' v d\Omega = -\int_{\Omega} u' v' d\Omega + \int_{\Gamma} u'v d\Gamma 
$$

* Approximate weakly 
* Finite element method
* Least squares method
* Galerkin method
:::
:::

We will use both approaches to first consider **function** approximations and then the approximation of **equations**.

## Required software skills {.smaller}

 * Our software platform: Python, Jupyter notebooks
 * Important Python packages: `numpy`, `scipy`, `matplotlib`,
   `sympy`, `shenfun`, ...
 * Anaconda Python, conda environments

## Assumed/ideal background {.smaller}

 * IN1900: Python programming, solution of ODEs
 * Some experience with finite difference methods
 * Some analytical and numerical knowledge of PDEs
 * Much experience with calculus and linear algebra
 * Much experience with programming of mathematical problems
 * Experience with mathematical modeling with PDEs
   (from physics, mechanics, geophysics, or ...)

# Start-up example - exponential decay

## Exponential decay model 

::: {.callout-note} 
## ODE problem
$$
u'=-au,\quad u(0)=I,\ t\in (0,T]
$$

where $a>0$ is a constant and $u(t)$ is the time-dependent solution.
:::

* We study first a simple 1D ODE, because this will lead us to the building blocks that we need for solving PDEs! 
* We can more easily study the concepts of stability, accuracy, convergence and consistency.

::: footer
See [Langtangen, Finite Difference Computing - Chapter 1](https://link.springer.com/book/10.1007/978-3-319-29439-1)
:::

## What to learn in the start-up example {.smaller}

 * How to think when constructing finite difference methods, with special focus
   on the **Forward Euler**, **Backward Euler**, and **Crank-Nicolson** (midpoint) schemes
 * How to formulate a computational algorithm and translate it into
   Python code
 * How to optimize the code for computational speed
 * How to plot the solutions
 * How to compute numerical errors and convergence rates
 * How to analyse the numerical solution


## What to learn in the start-up example; programming topics {.smaller visibility="hidden"} 

 * How to verify an implementation and automate verification
   through tests in Python
 * How to structure code in terms of functions, classes, and modules
 * How to work with Python concepts such as arrays, lists, dictionaries,
   lambda functions, functions in functions (closures),
   unit tests, command-line interfaces
 * How to perform array computing and understand the difference from
   scalar computing. Vectorization.

## What to learn in the start-up example; mathematical analysis {.smaller visibility="hidden"}

 * How to uncover numerical artifacts in the computed solution
 * How to analyze the numerical schemes mathematically to understand
   why artifacts occur
 * How to derive mathematical expressions for various measures of
   the error in numerical methods, frequently by using the `sympy` software
   for symbolic computation
 * Introduce concepts such as finite difference operators,
   mesh (grid), mesh functions,
   stability, truncation error, consistency, and convergence

## Finite difference methods {.smaller}

 * The finite difference method is the simplest method
   for solving differential equations
 * Satisfy the equations in discrete points, not continuously
 * Fast to learn, derive, and implement
 * A very useful tool to know, even if you aim at using the finite element or the finite volume method

```{python}
import matplotlib.pyplot as plt
import numpy as np

N = 100
T = 1
t = np.linspace(0, T, N)
fig = plt.figure(figsize=(8, 4))
f = lambda t: t + np.sin(4*t*np.pi) + np.sin(t*np.pi)
plt.plot(t, f(t), 'k--')
t = np.linspace(0, T, 7)
rnd = np.array([0, 0.02, 0.05, 0.1, 0.02, 0.1, 0])
plt.plot(t, f(t)+rnd, 'ko')
plt.legend(['Exact', 'FD']);
```

## Topics in the first intro to the finite difference method {.smaller visibility="hidden"}

  * How to think about finite difference discretization
  * Key concepts:
    - mesh
    - mesh function
    - finite difference approximations
  * The Forward Euler, Backward Euler, and Crank-Nicolson methods
  * Finite difference operator notation
  * How to derive an algorithm and implement it in Python
  * How to test the implementation


## The steps in the finite difference method {.smaller}

Solving a differential equation by a finite difference method
consists of four steps:

 1. discretizing the domain,
 2. fulfilling the equation at discrete time points,
 3. replacing derivatives by finite differences,
 4. solve the discretized problem. (Often with a recursive algorithm in 1D)

## Step 1: Discretizing the domain {.smaller}

The time domain $[0,T]$ is represented by a *mesh*: a finite number of $N_t+1$ points

$$
0 = t_0 < t_1 < t_2 < \cdots < t_{N_t-1} < t_{N_t} = T
$$

![](./figures/line.png){fig-align="center"}

 * We seek the solution $u$ at the mesh points: $u(t_n)$, $n=1,2,\ldots,N_t$.
 * Note: $u^0$ is known as $I$.
 * Notational short-form for the numerical approximation to $u(t_n)$: $u^n$
 * In the differential equation: $u(t)$ is the exact solution
 * In the numerical method and implementation: $u^n$ is the numerical approximation

## Step 1: Discretizing the domain {.smaller}

$u^n$ is a **mesh function**, defined at the mesh points $t_n$, $n=0,\ldots,N_t$ only.

```{python}
#| fig-align: center
N = 100
t = np.linspace(0, T, N)
fig, ax = plt.subplots(1, 1) 
ax.plot(t, f(t), 'k--')
t = np.linspace(0, T, 7)
ax.plot(t, f(t)+rnd, 'ko')
for i, ti in enumerate(t):
    ax.text(ti+0.01, f(ti)+0.05, f'$u^{i}$')

ax.set_xticklabels([f'$t_{i}$' for i in range(7)]);
ax.set_yticks([]);
```

## What about a mesh function between the mesh points? {.smaller}

Can extend the mesh function to yield values between mesh points
by *linear interpolation*:

$$
\begin{equation}
u(t) \approx u^n + \frac{u^{n+1}-u^n}{t_{n+1}-t_n}(t - t_n)
\end{equation}
$$

```{python}
#| fig-align: center
fig, ax = plt.subplots(1, 1, figsize=(8, 4))
t = np.linspace(0, T, N)
ax.plot(t, f(t), 'k--')
t = np.linspace(0, T, 7)
ax.plot(t, f(t)+rnd, 'ko-')
for i, ti in enumerate(t):
    ax.text(ti+0.01, f(ti)+0.05, f'$u^{i}$')
ax.legend(['Exact', 'FD-linear interpolation']);
ax.set_xticklabels([f'$t_{i}$' for i in range(7)]);
ax.set_yticks([]);
```

## Step 2: Fulfilling the equation at discrete time points {.smaller}

 * The ODE holds for all $t\in (0,T]$ (infinite no of points)
 * Idea: let the ODE be valid at the mesh points only (finite no of points)

$$
u'(t_n) = -au(t_n),\quad n=1,\ldots,N_t
$$

## Step 3: Replacing derivatives by finite differences {.smaller}

Now it is time for the **finite difference** approximations of
derivatives:

$$
u'(t_n) \approx \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n}
$$

![](./figures/fd_forward.png){.absolute top=300 left=250 width=550}

## Step 3: Replacing derivatives by finite differences {.smaller}

Inserting the finite difference approximation in

$$
 u'(t_n) = -au(t_n)
$$

gives

$$
\begin{equation}
\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -au^{n},\quad n=0,1,\ldots,N_t-1
\end{equation}
$$

(Known as *discrete equation*, or *discrete problem*,
or *finite difference method/scheme*)

## Step 4: Formulating a recursive algorithm {.smaller}

How can we actually compute the $u^n$ values?

  * given $u^0=I$
  * compute $u^1$ from $u^0$
  * compute $u^2$ from $u^1$
  * compute $u^3$ from $u^2$ (and so forth)

In general: we have $u^n$ and seek $u^{n+1}$

::: {.callout-note} 
## The Forward Euler scheme
Solve wrt $u^{n+1}$ to get the computational formula:
$$
u^{n+1} = u^n - a(t_{n+1} -t_n)u^n
$$
:::

## Let us apply the scheme by hand {.smaller}

Assume constant time spacing: $\Delta t = t_{n+1}-t_n=\mbox{const}$ such that $u^{n+1} = u^n (1- a \Delta t)$

$$
\begin{align*}
u^0 &= I,\\
u^1 & = I(1-a\Delta t),\\
u^2 & = I(1-a\Delta t)^2,\\
&\vdots\\
u^{N_t} &= I(1-a\Delta t)^{N_t}
\end{align*}
$$

Ooops - we can find the numerical solution by hand (in this simple example)! No need for a computer (yet)...

## A backward difference {.smaller}

Here is another finite difference approximation to the
derivative (backward difference):

$$
u'(t_n) \approx \frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}}
$$

![](./figures/fd_backward.png){.absolute top=280 left=250 width=550}

## The Backward Euler scheme {.smaller}

Inserting the finite difference approximation in $u'(t_n)=-au(t_n)$ yields the Backward Euler (BE) scheme:

$$
\frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}} = -a u^n
$$

Solve with respect to the unknown $u^{n+1}$:

$$
u^{n+1} = \frac{1}{1+ a(t_{n+1}-t_n)} u^n
$$

::: {.callout-note}
We use $u^{n+1}$ as unknown and rename $u^n \longrightarrow u^{n+1}$ and $u^{n-1} \longrightarrow u^{n}$
:::

## A centered difference {.smaller}

Centered differences are better approximations than forward or
backward differences.

![](./figures/fd_centered_CN){.absolute top=200 left=250 width=550}

## The Crank-Nicolson scheme; ideas {.smaller}

Idea 1: let the ODE hold at $t_{n+\half}$. With $N_t+1$ points, that is $N_t$ equations for $n=0, 1, \ldots N_t-1$

$$
 u'(t_{n+\half}) = -au(t_{n+\half})
$$

Idea 2: approximate $u'(t_{n+\half})$ by a **centered difference**

$$
u'(t_{n+\half}) \approx \frac{u^{n+1}-u^n}{t_{n+1}-t_n}
$$

**Problem:** $u(t_{n+\half})$ is not defined, only $u^n=u(t_n)$ and $u^{n+1}=u(t_{n+1})$

::: {.fragment}

Solution (linear interpolation):

$$
u(t_{n+\half}) \approx \frac{1}{2} (u^n + u^{n+1})
$$

:::

## The Crank-Nicolson scheme; result  {.smaller}

Result:

$$
\frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -a\frac{1}{2} (u^n + u^{n+1})
$$

Solve wrt to $u^{n+1}$:

$$
u^{n+1} = \frac{1-\frac{1}{2} a(t_{n+1}-t_n)}{1 + \frac{1}{2} a(t_{n+1}-t_n)}u^n
$$
This is a Crank-Nicolson (CN) scheme or a midpoint or centered scheme.


## The unifying $\theta$-rule {.smaller}

The Forward Euler, Backward Euler, and Crank-Nicolson schemes can be formulated as one scheme with a varying parameter $\theta$:

$$
\frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -a (\theta u^{n+1} + (1-\theta) u^{n})
$$

 * $\theta =0$: Forward Euler
 * $\theta =1$: Backward Euler
 * $\theta =1/2$: Crank-Nicolson
 * We may alternatively choose any $\theta\in [0,1]$.

$u^n$ is known, solve for $u^{n+1}$:

$$
u^{n+1} = \frac{1 - (1-\theta) a(t_{n+1}-t_n)}{1 + \theta a(t_{n+1}-t_n)} u^n
$$

## Constant time step {.smaller}

Very common assumption (not important, but exclusively used for
simplicity hereafter): constant time step $t_{n+1}-t_n\equiv\Delta t$


Summary of schemes for constant time step
$$
\begin{align}
u^{n+1} &= (1 - a\Delta t )u^n  \quad (\hbox{FE}) \\
u^{n+1} &= \frac{1}{1+ a\Delta t} u^n  \quad (\hbox{BE}) \\
u^{n+1} &= \frac{1-\frac{1}{2} a\Delta t}{1 + \frac{1}{2} a\Delta t} u^n \quad (\hbox{CN})\\
u^{n+1} &= \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n \quad (\theta-\hbox{rule})
\end{align}
$$

# Implementation

## Implementation {.smaller}

Model:

$$
u'(t) = -au(t),\quad t\in (0,T], \quad u(0)=I
$$

Numerical method:

$$
u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
$$

for $\theta\in [0,1]$. Note

 * $\theta=0$ gives Forward Euler
 * $\theta=1$ gives Backward Euler
 * $\theta=1/2$ gives Crank-Nicolson

## Requirements of a program

  * Compute the numerical solution $u^n$, $n=1,2,\ldots,N_t$
  * Display the numerical and exact solution $\uex(t)=e^{-at}$
  * Bring evidence to a correct implementation (**verification**)
  * Compare the numerical and the exact solution in a plot
  * Quantify the error $\uex (t_n) - u^n$ using **norms**
  * Compute the **convergence rate** of the numerical scheme
  * (Optimize for speed)

## Python - pros and cons {chalkboard-buttons="true" visibility="hidden"}


## Algorithm

 * Store $u^n$, $n=0,1,\ldots,N_t$ in an array $\boldsymbol{u}$.
 * Algorithm:
   - initialize $u^0$
   - for $n=1, 2, \ldots, N_t$: compute $u^n$ using the $\theta$-rule formula

## In Python {.smaller}

```{.python}
import numpy as np
def solver(I, a, T, dt, theta):
    """Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt."""
    Nt = int(T/dt)            # no of time intervals
    T = Nt*dt                 # adjust T to fit time step dt
    u = np.zeros(Nt+1)           # array of u[n] values
    t = np.linspace(0, T, Nt+1)  # time mesh
    u[0] = I                  # assign initial condition
    for n in range(0, Nt):    # n=0,1,...,Nt-1
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
    return u, t

u, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)
# Write out a table of t and u values:
for i in range(len(t)):
    print(f't={t[i]:6.3f} u={u[i]:g}')

```

## In Python {.smaller}

```{python}
#| echo: true
import numpy as np
def solver(I, a, T, dt, theta):
    """Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt."""
    Nt = int(T/dt)            # no of time intervals
    T = Nt*dt                 # adjust T to fit time step dt
    u = np.zeros(Nt+1)           # array of u[n] values
    t = np.linspace(0, T, Nt+1)  # time mesh
    u[0] = I                  # assign initial condition
    for n in range(0, Nt):    # n=0,1,...,Nt-1
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
    return u, t

u, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)
# Write out a table of t and u values:
for i in range(len(t)):
    print(f't={t[i]:6.3f} u={u[i]:g}')

```

## Plot the solution {.smaller}

We will also learn about plotting. It is very important to present data in a clear and consise manner. It is very easy to generate a *naked* plot

```{python}
#| echo : true
import matplotlib.pyplot as plt 
I, a, T, dt, theta = 1, 2, 8, 0.8, 1
u, t = solver(I, a, T, dt, theta)
fig = plt.figure(figsize=(6, 4))
ax = fig.gca()
ax.plot(t, u)
```

## Plot the solution {.smaller}

But you should always add legends, titles, exact solution, etc. Make the plot nice:-)

```{python}
#| echo : true
u_exact = lambda t, I, a: I*np.exp(-a*t)
u, t = solver(I=I, a=a, T=T, dt=0.8, theta=1)
te = np.linspace(0, T, 1000)
ue = u_exact(te, I, a)
fig = plt.figure(figsize=(6, 4))
plt.plot(t, u, 'bs-', te, ue, 'r')
plt.title('Decay')
plt.legend(['numerical', 'exact'])
plt.xlabel('Time'), plt.ylabel('u(t)');
```

## [Plotly](https://plotly.com/) is a very good alternative

```{python}
#| echo : true
import plotly.express as px
pfig = px.line(x=t, y=u, labels={'x': 'Time', 'y': 'u(t)'}, 
               width=600, height=400, title='Decay',
               template="simple_white")
pfig.show()
```

## Verifying the implementation

 * Verification = bring evidence that the program works
 * Find suitable test problems
 * Make function for each test problem
 * Later: put the verification tests in a professional testing framework
    - pytest
    - github actions


## Comparison with exact numerical solution {.smaller}

::: {.callout-note}
## What is exact?

There is a difference between exact numerical solution and exact solution!
:::

Repeated use of the $\theta$-rule gives exact numerical solution:
$$
\begin{align*}
u^0 &= I,\\
u^1 &= Au^0 = AI\\
u^n &= A^nu^{n-1} = A^nI
\end{align*}
$$

Exact solution on the other hand: 

$$
u(t) = \exp(-a t), \quad u(t_n) = \exp(-a t_n)
$$


## Making a test based on an exact numerical solution {.smaller}

The exact discrete solution is

$$
u^n = IA^n
$$

Test if your solver gives

$$ 
\max_n |u^n - IA^n| < \epsilon\sim 10^{-15}
$$

for a few precalculated steps.

::: {.callout-tip}
Make sure you understand what $n$ in $u^n$ and in $A^n$ means! $n$ is not used as a power in $u^n$, but it is a power in $A^n$!
:::

## Run a few numerical steps by hand {.smaller}

Use a calculator ($I=0.1$, $\theta=0.8$, $\Delta t =0.8$):

$$ 
A\equiv \frac{1 - (1-\theta) a\Delta t}{1 + \theta a \Delta t} = 0.298245614035
$$

$$
\begin{align*}
u^1 &= AI=0.0298245614035,\\
u^2 &= Au^1= 0.00889504462912,\\
u^3 &=Au^2= 0.00265290804728
\end{align*}
$$

## The test based on exact numerical solution {.smaller}

```{python}
#| echo : true
def test_solver_three_steps(solver):
    """Compare three steps with known manual computations."""
    theta = 0.8
    a = 2
    I = 0.1
    dt = 0.8
    u_by_hand = np.array([I,
                          0.0298245614035,
                          0.00889504462912,
                          0.00265290804728])

    Nt = 3  # number of time steps
    u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)
    tol = 1E-14  # tolerance for comparing floats
    diff = abs(u - u_by_hand).max()
    success = diff < tol
    assert success, diff

test_solver_three_steps(solver)
```

::: {.callout-note}
We do not use the **exact** solution because the numerical solution will not equal the exact! 
:::

## Quantifying the error {.smaller}

### Computing the norm of the error 

 * $e^n = u^n - u_e(t_n)$ is a **mesh function**
 * Usually we want one number for the error
 * Use a norm of $e^n$

Norms of a function $f(t)$:

$$
\begin{align}
||f||_{L^2} &= \left( \int_0^T f(t)^2 dt\right)^{1/2} \\
||f||_{L^1} &= \int_0^T |f(t)| dt \\
||f||_{L^\infty} &= \max_{t\in [0,T]}|f(t)|
\end{align}
$$

## Norms of mesh functions {.smaller}

 * Problem: $f^n =f(t_n)$ is a **mesh function** and hence not defined for all $t$. How to integrate $f^n$?
 * Idea: Apply a numerical integration rule, using only
   the mesh points of the mesh function.

The Trapezoidal rule:

$$
||f^n|| = \left(\Delta t\left(\half(f^0)^2 + \half(f^{N_t})^2
+ \sum_{n=1}^{N_t-1} (f^n)^2\right)\right)^{1/2}
$$

Common simplification yields the $\ell^2$ norm of a mesh function:

$$
||f^n||_{\ell^2} = \left(\Delta t\sum_{n=0}^{N_t} (f^n)^2\right)^{1/2}
$$

## Norms - notice!

 * The *continuous* norms use capital $L^2, L^1, L^\infty{}$
 * The *discrete* norm uses lowercase $\ell^2, \ell^1, \ell^{\infty}$

## Implementation of the error norm

$$ 
E = ||e^n||_{\ell^2}  = \sqrt{\Delta t\sum_{n=0}^{N_t} (e^n)^2}
$$

Python code for the norm:

```{python}
#| echo : true
u_exact = lambda t, I, a: I*np.exp(-a*t)
I, a, T, dt, theta = 1., 2., 8., 0.2, 1
u, t = solver(I, a, T, dt, theta)
en = u_exact(t, I, a) - u
E = np.sqrt(dt*np.sum(en**2))
print(f'Errornorm = {E}')
```

## How about computational speed? {.smaller}

The code is naive and not very efficient. It is not vectorized!

::: {.callout-tip}
## Vectorization

**Vectorization** refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.

:::

::: {.fragment}
For example, you have three arrays

$$
\boldsymbol{u} = (u_i)_{i=0}^N, \boldsymbol{v} = (v_i)_{i=0}^N, \boldsymbol{w} = (w_i)_{i=0}^N
$$

Now compute

$$
w_i = u_i \cdot v_i, \quad \forall \, i=0, 1, \ldots, N
$$

:::

## How about computational speed? {.smaller}

The code is naive and not very efficient. It is not vectorized!

::: {.callout-tip}
## Vectorization

**Vectorization** refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.

::: 

Regular (scalar) implementation:

```{python}
#| echo : true
N = 1000
u = np.random.random(N)
v = np.random.random(N)
w = np.zeros(N)

for i in range(N):
    w[i] = u[i] * v[i]
```

Vectorized:

```{python}
#| echo : true
w[:] = u * v
```

Numpy is heavily vectorized! So much so that mult, add, div, etc are vectorized by default!

## How about computational speed? {.smaller}

The code is naive and not very efficient. It is not vectorized!

::: {.callout-tip}
## Vectorization

**Vectorization** refers to the process of converting iterative operations on individual elements of an array (or other data structures) into batch operations on entire arrays.

::: 

::: {.callout-warning}
## Vectorization warning

Pretty much all the code you will see and get access to in this course will be vectorized!

:::

## Vectorizing the decay solver {.smaller}

Get rid of the for-loop!

```{.python}
u[0] = I                  # assign initial condition
for n in range(0, Nt):    # n=0,1,...,Nt-1
    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
```

::: {.fragment}

How? Difficult because it is a **recursive** update and not regular **elementwise** multiplication. But remember

$$
A = (1 - (1- \theta) a  \Delta t)/(1 + \theta \Delta t a)
$$

$$
\begin{align*}
u^1 & = A u^0,\\
u^2 & = A u^1,\\
&\vdots\\
u^{N_t} &= A u^{N_t-1} 
\end{align*}
$$

:::

## Vectorized code {.smaller}

```{.python}
u[0] = I                  # assign initial condition
for n in range(0, Nt):    # n=0,1,...,Nt-1
    u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
```
Can be implemented as

::: {.fragment}
```{.python}
u[0] = I                  # assign initial condition
u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
u[:] = np.cumprod(u)     
```
:::

::: {.fragment .fade-in-then-out}
because

$$
u^n = A^n u^0, \quad \text{since }
\begin{cases}
u^1 & = A u^0,\\
u^2 & = A u^1 = A^2 u^0,\\
&\vdots\\
u^{N_t} &= A u^{N_t-1} = A^{N_t} u^0 
\end{cases}
$$

```{python}
#| echo: true
np.cumprod([1, 2, 2, 2])
```

:::

## Why vectorization? {.smaller}

* Python for-loops are slow!
* Python for-loops usually requires more lines of code.

```{python}
#| echo : true
def f0(u, I, theta, a, dt):
    u[0] = I                  
    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
    u[:] = np.cumprod(u)
    return u

def f1(u,  I, theta, a, dt):
    u[0] = I                 
    for n in range(0, len(u)-1):  
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
    return u

I, a, T, dt, theta = 1, 2, 8, 0.8, 1
u, t = solver(I, a, T, dt, theta)

assert np.allclose(f0(u.copy(), I, theta, a, dt), 
                   f1(u.copy(), I, theta, a, dt))
```

::: {.fragment}

Lets try some timings!

:::


## Why vectorization? Timings {.smaller}

```{python}
#| echo : true
def f0(u, I, theta, a, dt):
    u[0] = I                  
    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
    u[:] = np.cumprod(u)

def f1(u,  I, theta, a, dt):
    u[0] = I                 
    for n in range(0, len(u)-1):  
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
```

Lets try some timings:

```{python}
#| echo : true
%timeit -q -o -n 1000 f0(u, I, theta, a, dt)
```

```{python}
#| echo : true
%timeit -q -o -n 1000 f1(u, I, theta, a, dt)
```

::: {.fragment}

Hmm. Not really what's expected. Why? Because the array `u` is really short! Lets try a longer array

```{python}
#| echo : true
print(f"Length of u = {u.shape[0]}") 
```

:::

## Longer array timings {.smaller}

```{python}
#| echo : true
dt = dt/10
u, t = solver(I, a, T, dt, theta) 
print(f"Length of u = {u.shape[0]}")
```

```{python}
#| echo : true
%timeit -q -o -n 100 f0(u, I, theta, a, dt)
```
```{python}
#| echo : true
%timeit -q -o -n 100 f1(u, I, theta, a, dt)
```

::: {.fragment}

Even longer array:

```{python}
#| echo : true
dt = dt/10
u, t = solver(I, a, T, dt, theta) 
print(f"Length of u = {u.shape[0]}")
```

```{python}
#| echo : true
%timeit -q -o -n 100 f0(u, I, theta, a, dt)
```
```{python}
#| echo : true
%timeit -q -o -n 100 f1(u, I, theta, a, dt)
```

Vectorized code takes the same time! Only overhead costs, not the actual computation.
:::

## What else is there? [Numba](https://numba.pydata.org/) {.smaller}

```{python}
#| echo : true 
import numba as nb
@nb.jit
def f2(u,  I, theta, a, dt):
    u[0] = I                 
    for n in range(0, len(u)-1):  
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
```

Time it once
```{python}
#| echo : true
%timeit -q -o -n 100 f2(u, I, theta, a, dt)
```

::: {.fragment}
Slow because the code needs to be compiled. Try again
```{python}
#| echo : true
%timeit -q -o -n 100 f2(u, I, theta, a, dt)
```

That is even faster than the vectorized code! 

:::

## What else? [Cython](https://cython.org/) {.smaller}

```{python}
#| echo : true
%load_ext cython
```

```{python}
#| echo : true
%%cython -a
#cython: boundscheck=False, wraparound=False, cdivision=True
cpdef void f3(double[::1] u, int  I, double theta, double a, double dt):
    cdef int n
    cdef int N = u.shape[0]
    u[0] = I                 
    for n in range(0, N-1):  
        u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
    return
```

## Cython timing {.smaller}

```{python}
#| echo : true
%timeit -q -o -n 100 f3(u, I, theta, a, dt)
```

::: {.callout-note}
## Cython and Numba are both fast!

Cython and Numba are both as fast as pure C. Either one can be used to speed up critical routines with very little additional effort!

:::

::: {.callout-note}
Cython is very easy to use in notebooks, but requires [some additional steps](https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html) to be compiled used as extension modules with regular python programs.
:::
