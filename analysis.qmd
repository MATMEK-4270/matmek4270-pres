---
title: "Analysis of exponential decay models"
subtitle: "MATMEK-4270"
author: "Prof. Mikael Mortensen, University of Oslo"

format:
  revealjs: 
    slide-number: true
    preview-links: auto
    css: styles.css
    embed-resources: false
pandoc:
  to: revealjs
  standalone: false
  wrap: none
  default-image-extension: png
  html-math-method:
    method: mathjax
    url: >-
      https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
  slide-level: 2
---


## Recap - Finite differencing of exponential decay {.smaller}

::: {.hidden}
\newcommand{\half}{\scriptstyle\frac{1}{2}}
\newcommand{\halfi}{{1/2}}
\newcommand{\uex}{u_{e}}
:::

::: {.callout-note}
## The ordinary differential equation
$$
u'(t) = -au(t),\quad u(0)=I, \quad y \in (0, T]
$$
where $a>0$ is a constant.

:::

Solve the ODE by finite difference methods:

  * Discretize in time:

    $$0 = t_0 < t_1 < t_2 < \cdots < t_{N_t-1} < t_{N_t} = T$$

  * Satisfy the ODE at $N_t$ discrete time steps:

    $$
    \begin{align}
    u'(t_n) &= -a u(t_n), \quad &n\in [1, \ldots, N_t], \text{ or} \\
    u'(t_{n+\half}) &= -a u(t_{n+\half}), \quad &n\in [0, \ldots, N_t-1]
    \end{align}
    $$ 

## Finite difference algorithms {.smaller}

  * Discretization by a generic $\theta$-rule

$$\frac{u^n-u^{n-1}}{\triangle t} = -(1-\theta)au^{n-1} - \theta u^n$$

$$\begin{cases}
  \theta = 0 \quad &\text{Forward Euler} \\
  \theta = 1 \quad &\text{Backward Euler} \\
  \theta = 1/2 \quad &\text{Crank-Nicolson}
  \end{cases}
$$

Note $u^n = u(t_n)$

  * Solve recursively: Set $u^0 = I$ and then

  $$u^n = \frac{1-(1-\theta)a \triangle t}{1+\theta a \triangle t}u^{n-1} \quad \text{for } n>0$$

## Analysis of finite difference equations 

Model:
$$
u'(t) = -au(t),\quad u(0)=I
$$

Method:

$$
u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
$$

::: {.callout-note}
## Problem setting
How good is this method? Is it safe to use it?

:::

## Encouraging numerical solutions - Backwards Euler

$I=1$, $a=2$, $\theta =1$, $\Delta t=1.25, 0.75, 0.5, 0.1$.

```{python}
import matplotlib.pyplot as plt
import numpy as np
def solver(I, a, T, dt, theta):
    """Solve u'=-a*u, u(0)=I, for t in (0, T] with steps of dt."""
    Nt = int(T/dt)            # no of time intervals
    T = Nt*dt                 # adjust T to fit time step dt
    u = np.zeros(Nt+1)           # array of u[n] values
    t = np.linspace(0, T, Nt+1)  # time mesh
    u[0] = I                  # assign initial condition
    u[1:] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)
    u[:] = np.cumprod(u)
    return u, t

u_exact = lambda I, a, t: I*np.exp(-a*t)
I, a, T, theta = 1, 2, 8, 1
dt = np.array([1.25, 0.75, 0.5, 0.1])
fig, axs = plt.subplots(2, 2)
for i in range(2):
    for j in range(2):
        u0, t0 = solver(I, a, T, dt[i*2+j], theta)
        axs[i, j].plot(t0, u0, 'b', t0, u_exact(I, a, t0), 'r--')
        axs[i, j].legend(['numerical', 'exact'])
        axs[i, j].set_title(f'$\Delta t = {dt[i*2+j]}$')
        axs[i, j].label_outer()
```

## Discouraging numerical solutions - Crank-Nicolson

$I=1$, $a=2$, $\theta=0.5$, $\Delta t=1.25, 0.75, 0.5, 0.1$.

```{python}
I, a, T, theta = 1, 2, 8, 0.5
fig, axs = plt.subplots(2, 2)
for i in range(2):
    for j in range(2):
        u0, t0 = solver(I, a, T, dt[i*2+j], theta)
        axs[i, j].plot(t0, u0, 'b', t0, u_exact(I, a, t0), 'r--')
        axs[i, j].legend(['numerical', 'exact'])
        axs[i, j].set_title(f'$\Delta t = {dt[i*2+j]}$')
        axs[i, j].label_outer()
```

## Discouraging numerical solutions - Forward Euler

$I=1$, $a=2$, $\theta=0$, $\Delta t=1.25, 0.75, 0.5, 0.1$.

```{python}
I, a, T, theta = 1, 2, 8, 0
fig, axs = plt.subplots(2, 2)
for i in range(2):
    for j in range(2):
        u0, t0 = solver(I, a, T, dt[i*2+j], theta)
        axs[i, j].plot(t0, u0, 'b', t0, u_exact(I, a, t0), 'r--')
        axs[i, j].legend(['numerical', 'exact'])
        axs[i, j].set_title(f'$\Delta t = {dt[i*2+j]}$')
        axs[i, j].label_outer()
```

## Summary of observations {.smaller}

The characteristics of the displayed curves can be summarized as follows:

  * The Backward Euler scheme *always* gives a monotone solution, lying above the exact solution.
  * The Crank-Nicolson scheme gives the most accurate results, but for $\Delta t=1.25$ the solution oscillates.
  * The Forward Euler scheme gives a growing, oscillating solution for $\Delta t=1.25$; a decaying, oscillating solution for $\Delta t=0.75$; a strange solution $u^n=0$ for $n\geq 1$ when $\Delta t=0.5$; and a solution seemingly as accurate as the one by the Backward Euler scheme for $\Delta t = 0.1$, but the curve lies *below* the exact solution.
  * Small enough $\Delta t$ gives stable and accurate solution for all methods!

## Problem setting 

::: {.callout-note}
## We ask the question

 * Under what circumstances, i.e., values of the input data $I$, $a$, and $\Delta t$ will the Forward Euler and Crank-Nicolson schemes result in undesired oscillatory solutions?


Techniques of investigation:

 * Numerical experiments
 * Mathematical analysis

Another question to be raised is

 * How does $\Delta t$ impact the error in the numerical solution?

:::

## Exact numerical solution {.smaller}

For the simple exponential decay problem we are lucky enough to have an exact numerical solution

$$
u^{n} = IA^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}
$$

Such a formula for the exact discrete solution is unusual to obtain in practice, but very handy for our analysis here.

::: {.callout-note}

An exact dicrete
solution fulfills a discrete equation (without round-off
errors), whereas an exact solution fulfills the original mathematical equation.

:::

## Stability {.smaller}

Since $u^n=I A^n$,

 * $A < 0$ gives a factor $(-1)^n$ and oscillatory solutions
 * $|A|>1$ gives growing solutions
 * Recall: the exact solution is *monotone* and *decaying*
 * If these qualitative properties are not met, we say that the
   numerical solution is *unstable*

::: {.fragment}
For stability we need

$$
A > 0 \quad \text{ and } \quad |A| \le 1
$$

:::

## Computation of stability in this problem {.smaller}

$A < 0$ if

$$
\frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t} < 0
$$

To avoid oscillatory solutions we must have $A> 0$

::: {.fragment}
$$
\Delta t < \frac{1}{(1-\theta)a}, \theta < 1
$$

 * Always fulfilled for Backward Euler ($\theta=1 \rightarrow 1 < 1+a \Delta t$ always true)
 * $\Delta t \leq 1/a$ for Forward Euler ($\theta=0$)
 * $\Delta t \leq 2/a$ for Crank-Nicolson ($\theta = 0.5$)

:::

## Computation of stability in this problem {.smaller}

$|A|\leq 1$ means $-1\leq A\leq 1$

$$
-1\leq\frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t} \leq 1
$$

$-1$ is the critical limit (because $A\le 1$ is always satisfied):

Always fulfilled for Backward Euler ($\theta=0$) and Crank-Nicolson ($\theta=0.5$). For forward Euler or simply $\theta < 0.5$ we have
 
$$
\Delta t \leq \frac{2}{(1-2\theta)a},\quad
$$

 and thus $\Delta t \leq 2/a$ for stability of the Forward Euler ($\theta=0$) method

## Explanation of problems with Forward Euler  {.smaller}

:::: {.columns}

::: {.column width="55%"}

```{python}
I, a, T, theta = 1, 2, 8, 0
fig, axs = plt.subplots(2, 2)
ab = {(0, 0): 'a)', (0, 1): 'b)', (1, 0): 'c)', (1, 1): 'd)'}
for i in range(2):
    for j in range(2):
        u0, t0 = solver(I, a, T, dt[i*2+j], theta)
        axs[i, j].plot(t0, u0, 'b', t0, u_exact(I, a, t0), 'r--')
        axs[i, j].legend(['numerical', 'exact'])
        axs[i, j].set_title(f'$\Delta t = {dt[i*2+j]}$')
        axs[i, j].text(3.2, u0.max()*0.85, f'{ab[(i, j)]}', size=20)
        axs[i, j].label_outer()
```
:::

::: {.column width="45%"}
 a. $a\Delta t= 2\cdot 1.25=2.5$ and $A=-1.5$: oscillations and growth
 b. $a\Delta t = 2\cdot 0.75=1.5$ and $A=-0.5$: oscillations and decay
 c. $\Delta t=0.5$ and $A=0$: $u^n=0$ for $n>0$
 d. Smaller $\Delta t$: qualitatively correct solution
:::

::::


## Explanation of problems with Crank-Nicolson {.smaller}

:::: {.columns}

::: {.column width="55%"}

```{python}
I, a, T, theta = 1, 2, 8, 0.5
fig, axs = plt.subplots(2, 2)
for i in range(2):
    for j in range(2):
        u0, t0 = solver(I, a, T, dt[i*2+j], theta)
        axs[i, j].plot(t0, u0, 'b', t0, u_exact(I, a, t0), 'r--')
        axs[i, j].legend(['numerical', 'exact'])
        axs[i, j].set_title(f'$\Delta t = {dt[i*2+j]}$')
        axs[i, j].text(3.2, u0.max()*0.85, f'{ab[(i, j)]}', size=20)
        axs[i, j].label_outer()
```

:::

::: {.column width="45%"}
 a. $\Delta t=1.25$ and $A=-0.25$: oscillatory solution
 
Never any growing solution
:::

::::

## Summary of stability {.smaller}

 * Forward Euler is *conditionally stable*
   - $\Delta t < 2/a$ for avoiding growth
   - $\Delta t\leq 1/a$ for avoiding oscillations

 * The Crank-Nicolson is *unconditionally stable* wrt growth
   and conditionally stable wrt oscillations
   - $\Delta t < 2/a$ for avoiding oscillations

 * Backward Euler is unconditionally stable


## Comparing amplification factors {.smaller}

$u^{n+1}$ is an amplification $A$ of $u^n$:

$$
u^{n+1} = Au^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}
$$

The exact solution is also an amplification:

$$
\begin{align}
u(t_{n+1}) &= e^{-a(t_n+\Delta t)} \\
u(t_{n+1}) &= e^{-a \Delta t} e^{-a t_n} \\
u(t_{n+1} &= A_e u(t_n), \quad A_e = e^{-a\Delta t}
\end{align}
$$

A possible measure of accuracy: $A_e - A$
